<!doctype html>



  


<html class="theme-next pisces use-motion">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  
  
  <link href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/vendors/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.0.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="That's The Story Of Life." />





  <link rel="alternate" href="/atom.xml" title="StarrySky's Blog" type="application/atom+xml" />




  <link rel="shortcut icon" type="image/x-icon" href="/images/myfavicon.ico?v=5.0.1" />






<meta property="og:type" content="website">
<meta property="og:title" content="StarrySky's Blog">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="StarrySky's Blog">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="StarrySky's Blog">



<script type="text/javascript" id="hexo.configuration">
  var NexT = window.NexT || {};
  var CONFIG = {
    scheme: 'Pisces',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: 0,
      author: 'Author'
    }
  };
</script>

  <title> StarrySky's Blog </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="">

  










  
  
    
  

  <div class="container one-collumn sidebar-position-left 
   page-home 
 ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta custom-logo">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">StarrySky's Blog</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle"></p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-主页">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br />
            
            menu.主页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-存档">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br />
            
            menu.存档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-标签">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br />
            
            menu.标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-分类">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br />
            
            menu.分类
          </a>
        </li>
      

      
    </ul>
  

  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/05/23/svm/" itemprop="url">
                  (转)支持向量机
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Veröffentlicht am</span>
            <time itemprop="dateCreated" datetime="2016-05-23T19:01:31+08:00" content="2016-05-23">
              2016-05-23
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <blockquote>
<p>全文引自<a href="http://blog.pluskid.org/?p=702" target="_blank" rel="external">Free Mind</a>博客,如果要引用请注明Free Mind的原文地址。</p>
</blockquote>
<p>##<strong>Maximum Margin Classifier</strong> </p>
<center><img src="http://blog.pluskid.org/wp-content/uploads/2010/09/svm.png" alt=""></center>

<p>支持向量机即 Support Vector Machine，简称 SVM 。我最开始听说这头机器的名号的时候，一种神秘感就油然而生，似乎把 Support 这么一个具体的动作和 Vector 这么一个抽象的概念拼到一起，然后再做成一个 Machine ，一听就很玄了！</p>
<p>不过后来我才知道，原来 SVM 它并不是一头机器，而是一种算法，或者，确切地说，是一类算法，当然，这样抠字眼的话就没完没了了，比如，我说 SVM 实际上是一个分类器 (Classifier) ，但是其实也是有用 SVM 来做回归 (Regression) 的。所以，这种字眼就先不管了，还是从分类器说起吧。</p>
<p>SVM 一直被认为是效果最好的现成可用的分类算法之一（其实有很多人都相信，“之一”是可以去掉的）。这里“现成可用”其实是很重要的，因为一直以来学术界和工业界甚至只是学术界里做理论的和做应用的之间，都有一种“鸿沟”，有些很 fancy 或者很复杂的算法，在抽象出来的模型里很完美，然而在实际问题上却显得很脆弱，效果很差甚至完全 fail 。而 SVM 则正好是一个特例——在两边都混得开。</p>
<p>好了，由于 SVM 的故事本身就很长，所以废话就先只说这么多了，直接入题吧。当然，说是入贴，但是也不能一上来就是 SVM ，而是必须要从线性分类器开始讲。这里我们考虑的是一个两类的分类问题，数据点用 $x$ 来表示，这是一个 $n$ 维向量，而类别用 $y$ 来表示，可以取 1 或者 -1 ，分别代表两个不同的类（有些地方会选 0 和 1 ，当然其实分类问题选什么都无所谓，只要是两个不同的数字即可，不过这里选择 +1 和 -1 是为了方便 SVM 的推导，后面就会明了了）。一个线性分类器就是要在 $n$ 维的数据空间中找到一个超平面，其方程可以表示为</p>
<p>$$<br>w^Tx + b = 0<br>$$</p>
<p>一个超平面，在二维空间中的例子就是一条直线。我们希望的是，通过这个超平面可以把两类数据分隔开来，比如，在超平面一边的数据点所对应的 $y$ 全是 -1 ，而在另一边全是 1 。具体来说，我们令 $f(x)=w^Tx + b$ ，显然，如果 $f(x)=0$ ，那么 $x$ 是位于超平面上的点。我们不妨要求对于所有满足 $f(x)<0$ 的点，其对应的="" $y$="" 等于-1，而="" $f(x)="">0$ 则对应 $y=1$ 的数据点。当然，有些时候（或者说大部分时候）数据并不是线性可分的，这个时候满足这样条件的超平面就根本不存在，不过关于如何处理这样的问题我们后面会讲，这里先从最简单的情形开始推导，就假设数据都是线性可分的，亦即这样的超平面是存在的。</0$></p>
<center><img src="http://blog.pluskid.org/wp-content/uploads/2010/09/Hyper-Plane.png" alt=""></center>

<p>如上图所示，两种颜色的点分别代表两个类别，红颜色的线表示一个可行的超平面。在进行分类的时候，我们将数据点 $x$ 代入 $f(x)$ 中，如果得到的结果小于 0 ，则赋予其类别 -1 ，如果大于 0 则赋予类别 1 。如果 $f(x)=0$，则很难办了，分到哪一类都不是。事实上，对于 $f(x)$ 的绝对值很小的情况，我们都很难处理，因为细微的变动（比如超平面稍微转一个小角度）就有可能导致结果类别的改变。理想情况下，我们希望 $f(x)$ 的值都是很大的正数或者很小的负数，这样我们就能更加确信它是属于其中某一类别的。</p>
<p>从几何直观上来说，由于超平面是用于分隔两类数据的，越接近超平面的点越“难”分隔，因为如果超平面稍微转动一下，它们就有可能跑到另一边去。反之，如果是距离超平面很远的点，例如图中的右上角或者左下角的点，则很容易分辩出其类别。</p>
<center><img src="http://blog.pluskid.org/wp-content/uploads/2010/09/geometric_margin.png" alt=""></center>

<p>实际上这两个 Criteria 是互通的，我们定义 functional margin 为 $\hat{\gamma}=y(w^Tx+b)=yf(x)$，注意前面乘上类别 $y$ 之后可以保证这个 margin 的非负性（因为 $f(x)&lt;0$ 对应于 $y=-1$ 的那些点），而点到超平面的距离定义为 geometrical margin 。不妨来看看二者之间的关系。如图所示，对于一个点 $x$ ，令其垂直投影到超平面上的对应的为 $x_0$ ，由于 $w$ 是垂直于超平面的一个向量（请自行验证），我们有 $$ x=x_0+\gamma\frac{w}{|w|} $$ 又由于 $x_0$ 是超平面上的点，满足 $f(x_0)=0$ ，代入超平面的方程即可算出 $$ \gamma = \frac{w^Tx+b}{|w|}=\frac{f(x)}{|w|} $$ 不过，这里的 $\gamma$ 是带符号的，我们需要的只是它的绝对值，因此类似地，也乘上对应的类别 $y$ 即可，因此实际上我们定义 geometrical margin 为： $$ \tilde{\gamma} = y\gamma = \frac{\hat{\gamma}}{|w|} $$ 显然，functional margin 和 geometrical margin 相差一个 $|w|$ 的缩放因子。按照我们前面的分析，对一个数据点进行分类，当它的 margin 越大的时候，分类的 confidence 越大。对于一个包含 $n$ 个点的数据集，我们可以很自然地定义它的 margin 为所有这 $n$ 个点的 margin 值中最小的那个。于是，为了使得分类的 confidence 高，我们希望所选择的 hyper plane 能够最大化这个 margin 值。 不过这里我们有两个 margin 可以选，不过 functional margin 明显是不太适合用来最大化的一个量，因为在 hyper plane 固定以后，我们可以等比例地缩放 $w$ 的长度和 $b$ 的值，这样可以使得 $f(x)=w^Tx+b$ 的值任意大，亦即 functional margin $\hat{\gamma}$ 可以在 hyper plane 保持不变的情况下被取得任意大，而 geometrical margin 则没有这个问题，因为除上了 $|w|$ 这个分母，所以缩放 $w$ 和 $b$ 的时候 $\tilde{\gamma}$ 的值是不会改变的，它只随着 hyper plane 的变动而变动，因此，这是更加合适的一个 margin 。这样一来，我们的 maximum margin classifier 的目标函数即定义为 $$ \max \tilde{\gamma} $$ 当然，还需要满足一些条件，根据 margin 的定义，我们有 $$ y_i(w^Tx_i+b) = \hat{\gamma}_i \geq \hat{\gamma}, \quad i=1,\ldots,n $$ 其中 $\hat{\gamma}=\tilde{\gamma}|w|$ ，根据我们刚才的讨论，即使在超平面固定的情况下，$\hat{\gamma}$ 的值也可以随着 $|w|$ 的变化而变化。由于我们的目标就是要确定超平面，因此可以把这个无关的变量固定下来，固定的方式有两种：一是固定 $|w|$ ，当我们找到最优的 $\tilde{\gamma}$ 时 $\hat{\gamma}$ 也就可以随之而固定；二是反过来固定 $\hat{\gamma}$ ，此时 $|w|$ 也可以根据最优的 $\tilde{\gamma}$ 得到。处于方便推导和优化的目的，我们选择第二种，令 $\hat{\gamma}=1$ ，则我们的目标函数化为： $$ \max \frac{1}{|w|}, \quad s.t., y_i(w^Tx_i+b)\geq 1, i=1,\ldots,n $$ 通过求解这个问题，我们就可以找到一个 margin 最大的 classifier ，如下图所示，中间的红色线条是 Optimal Hyper Plane ，另外两条线到红线的距离都是等于 $\tilde{\gamma}$ 的： </p>
<center><img src="http://blog.pluskid.org/wp-content/uploads/2010/09/Optimal-Hyper-Plane.png" alt=""></center>

<p>到此为止，算是完成了 Maximum Margin Classifier 的介绍，通过最大化 margin ，我们使得该分类器对数据进行分类时具有了最大的 confidence （实际上，根据我们说给的一个数据集的 margin 的定义，准确的说，应该是“对最不 confidence 的数据具有了最大的 confidence”——虽然有点拗口）。不过，到现在似乎还没有一点点 Support Vector Machine 的影子。很遗憾的是，这个要等到下一次再说了，不过可以先小小地剧透一下，如上图所示，我们可以看到 hyper plane 两边的那个 gap 分别对应的两条平行的线（在高维空间中也应该是两个 hyper plane）上有一些点，显然两个 hyper plane 上都会有点存在，否则我们就可以进一步扩大 gap ，也就是增大 $\tilde{\gamma}$ 的值了。这些点呢，就叫做 support vector ，嗯，先说这么多了。</p>
<p>##<strong>Support Vector</strong><br>上一次介绍支持向量机，结果说到 Maximum Margin Classifier ，到最后都没有说“支持向量”到底是什么东西。不妨回忆一下上次最后一张图：</p>
<center><img src="http://blog.pluskid.org/wp-content/uploads/2010/09/Optimal-Hyper-Plane.png" alt=""></center>

<p>可以看到两个支撑着中间的 gap 的超平面，它们到中间的 separating hyper plane 的距离相等（想想看：为什么一定是相等的？），即我们所能得到的最大的 geometrical margin $\tilde{\gamma}$ 。而“支撑”这两个超平面的必定会有一些点，试想，如果某超平面没有碰到任意一个点的话，那么我就可以进一步地扩充中间的 gap ，于是这个就不是最大的 margin 了。由于在 $n$ 维向量空间里一个点实际上是和以原点为起点，该点为终点的一个向量是等价的，所以这些“支撑”的点便叫做支持向量。</p>
<p>很显然，由于这些 supporting vector 刚好在边界上，所以它们是满足 $y(w^Tx+b)=1$ （还记得我们把 functional margin 定为 1 了吗？），而对于所有不是支持向量的点，也就是在“阵地后方”的点，则显然有 $y(w^Tx+b) &gt; 1$ 。事实上，当最优的超平面确定下来之后，这些后方的点就完全成了路人甲了，它们可以在自己的边界后方随便飘来飘去都不会对超平面产生任何影响。这样的特性在实际中有一个最直接的好处就在于存储和计算上的优越性，例如，如果使用 100 万个点求出一个最优的超平面，其中是 supporting vector 的有 100 个，那么我只需要记住这 100 个点的信息即可，对于后续分类也只需要利用这 100 个点而不是全部 100 万个点来做计算。（当然，通常除了 K-Nearest Neighbor 之类的 Memory-based Learning 算法，通常算法也都不会直接把所有的点记忆下来，并全部用来做后续 inference 中的计算。不过，如果算法使用了 Kernel 方法进行非线性化推广的话，就会遇到这个问题了。Kernel 方法在下一次会介绍。）</p>
<p>当然，除了从几何直观上之外，支持向量的概念也会从其优化过程的推导中得到。其实上一次还偷偷卖了另一个关子就是虽然给出了目标函数，却没有讲怎么来求解。现在就让我们来处理这个问题。回忆一下之前得到的目标函数：</p>
<p>$$<br>\max \frac{1}{|w|}\quad s.t., y_i(w^Tx_i+b)\geq 1, i=1,\ldots,n<br>$$</p>
<p>这个问题等价于（为了方便求解，我在这里加上了平方，还有一个系数，显然这两个问题是等价的，因为我们关心的并不是最优情况下目标函数的具体数值）：</p>
<p>$$<br>\min \frac{1}{2}|w|^2\quad s.t., y_i(w^Tx_i+b)\geq 1, i=1,\ldots,n<br>$$</p>
<p>到这个形式以后，就可以很明显地看出来，它是一个凸优化问题，或者更具体地说，它是一个二次优化问题——目标函数是二次的，约束条件是线性的。这个问题可以用任何现成的 QP (Quadratic Programming) 的优化包进行求解。所以，我们的问题到此为止就算全部解决了，于是我睡午觉去了~ :)</p>
<p>啊？呃，有人说我偷懒不负责任了？好吧，嗯，其实呢，虽然这个问题确实是一个标准的 QP 问题，但是它也有它的特殊结构，通过 Lagrange Duality 变换到对偶变量 (dual variable) 的优化问题之后，可以找到一种更加有效的方法来进行求解——这也是 SVM 盛行的一大原因，通常情况下这种方法比直接使用通用的 QP 优化包进行优化要高效得多。此外，在推导过程中，许多有趣的特征也会被揭露出来，包括刚才提到的 supporting vector 的问题。</p>
<p>关于 Lagrange duality 我没有办法在这里细讲了，可以参考 Wikipedia 。简单地来说，通过给每一个约束条件加上一个 Lagrange multiplier，我们可以将它们融和到目标函数里去</p>
<p>$$<br>\mathcal{L}(w,b,\alpha)=\frac{1}{2}|w|^2-\sum_{i=1}^n\alpha_i \left(y_i(w^Tx_i+b)-1\right)<br>$$</p>
<p>然后我们令</p>
<p>$$<br>\theta(w) = \max_{\alpha_i\geq 0}\mathcal{L}(w,b,\alpha)<br>$$</p>
<p>容易验证，当某个约束条件不满足时，例如 $y_i(w^Tx_i+b) &lt; 1$，那么我们显然有 $\theta(w)=\infty$ （只要令 $\alpha_i=\infty$ 即可）。而当所有约束条件都满足时，则有 $\theta(w)=\frac{1}{2}|w|^2$ ，亦即我们最初要最小化的量。因此，在要求约束条件得到满足的情况下最小化 $\frac{1}{2}|w|^2$ 实际上等价于直接最小化 $\theta(w)$ （当然，这里也有约束条件，就是 $\alpha_i \geq 0,i=1,\cdots,n$），因为如果约束条件没有得到满足，$\theta(w)$ 会等于无穷大，自然不会是我们所要求的最小值。具体写出来，我们现在的目标函数变成了：</p>
<p>$$<br>\min<em>{w,b};\theta(w) = \min</em>{w,b};<br>\max_{\alpha_i\geq 0};<br>\mathcal{L}(w,b,\alpha) = p^*<br>$$ </p>
<p>这里用 $p^* $ 表示这个问题的最优值，这个问题和我们最初的问题是等价的。不过，现在我们来把最小和最大的位置交换一下： </p>
<p>$$<br>\max_{\alpha<em>i \geq 0};<br>\min</em>{w,b};<br>\mathcal{L}(w,b,\alpha) = d^*<br>$$ </p>
<p>当然，交换以后的问题不再等价于原问题，这个新问题的最优值用 $d^<em> $ 来表示。并，我们有 $d^</em> \leq p^<em> $ ，这在直观上也不难理解，最大值中最小的一个总也比最小值中最大的一个要大吧！ :) 总之，第二个问题的最优值 $d^</em> $ 在这里提供了一个第一个问题的最优值 $p^*  $ 的一个下界，在满足某些条件的情况下，这两者相等，这个时候我们就可以通过求解第二个问题来间接地求解第一个问题。具体来说，就是要满足 KKT 条件，这里暂且先略过不说，直接给结论：我们这里的问题是满足 KKT 条件的，因此现在我们便转化为求解第二个问题。</p>
<p>首先要让 $\mathcal{L}$ 关于 $w$ 和 $b$ 最小化，我们分别令 $\partial \mathcal{L}/\partial w$ 和 $\partial \mathcal{L}/\partial b$ 等于零：</p>
<p>$$<br>\begin{aligned}<br>\frac{\partial \mathcal{L}}{\partial w}=0 &amp;\Rightarrow w=\sum_{i=1}^n \alpha_i y_i x<em>i \<br>\frac{\partial \mathcal{L}}{\partial b} = 0 &amp;\Rightarrow \sum</em>{i=1}^n \alpha_i y_i = 0<br>\end{aligned}<br>$$</p>
<p>带回 $\mathcal{L}$ 得到：</p>
<p>$$<br>\begin{aligned}<br>\mathcal{L}(w,b,\alpha) &amp;= \frac{1}{2}\sum_{i,j=1}^n\alpha_i\alpha_jy_iy_jx_i^Tx<em>j-\sum</em>{i,j=1}^n\alpha_i\alpha_jy_iy_jx_i^Tx<em>j – b\sum</em>{i=1}^n\alpha_iy<em>i + \sum</em>{i=1}^n\alpha<em>i \<br>&amp;= \sum</em>{i=1}^n\alpha<em>i – \frac{1}{2}\sum</em>{i,j=1}^n\alpha_i\alpha_jy_iy_jx_i^Tx_j<br>\end{aligned}<br>$$</p>
<p>此时我们得到关于 dual variable $\alpha$ 的优化问题：</p>
<p>$$<br>\begin{aligned}<br>\max<em>\alpha &amp;\sum</em>{i=1}^n\alpha<em>i – \frac{1}{2}\sum</em>{i,j=1}^n\alpha_i\alpha_jy_iy_jx_i^Tx_j \<br>s.t., &amp;\alpha<em>i\geq 0, i=1,\ldots,n \<br>&amp;\sum</em>{i=1}^n\alpha_iy_i = 0<br>\end{aligned}<br>$$</p>
<p>如前面所说，这个问题有更加高效的优化算法，不过具体方法在这里先不介绍，让我们先来看看推导过程中得到的一些有趣的形式。首先就是关于我们的 hyper plane ，对于一个数据点 $x$ 进行分类，实际上是通过把 $x$ 带入到 $f(x)=w^Tx+b$ 算出结果然后根据其正负号来进行类别划分的。而前面的推导中我们得到 $w=\sum_{i=1}^n \alpha_i y_i x_i$ ，因此</p>
<p>$$<br>\begin{aligned}<br>f(x)&amp;=\left(\sum_{i=1}^n\alpha_i y_i x<em>i\right)^Tx+b \<br>&amp;= \sum</em>{i=1}^n\alpha_i y_i \langle x_i, x\rangle + b<br>\end{aligned}<br>$$</p>
<p>这里的形式的有趣之处在于，对于新点 $x$ 的预测，只需要计算它与训练数据点的内积即可（这里 $\langle \cdot, \cdot\rangle$ 表示向量内积），这一点至关重要，是之后使用 Kernel 进行非线性推广的基本前提。此外，所谓 Supporting Vector 也在这里显示出来——事实上，所有非 Supporting Vector 所对应的系数 $\alpha$ 都是等于零的，因此对于新点的内积计算实际上只要针对少量的“支持向量”而不是所有的训练数据即可。</p>
<p>为什么非支持向量对应的 $\alpha$ 等于零呢？直观上来理解的话，就是这些“后方”的点——正如我们之前分析过的一样，对超平面是没有影响的，由于分类完全有超平面决定，所以这些无关的点并不会参与分类问题的计算，因而也就不会产生任何影响了。这个结论也可由刚才的推导中得出，回忆一下我们刚才通过 Lagrange multiplier 得到的目标函数：</p>
<p>$$<br>\max_{\alpha<em>i\geq 0}\;\mathcal{L}(w,b,\alpha)=\max</em>{\alpha<em>i\geq 0}\;\frac{1}{2}|w|^2-\sum</em>{i=1}^n\alpha_i \color{red}{\left(y_i(w^Tx_i+b)-1\right)}<br>$$</p>
<p>注意到如果 $x_i$ 是支持向量的话，上式中红颜色的部分是等于 0 的（因为支持向量的 functional margin 等于 1 ），而对于非支持向量来说，functional margin 会大于 1 ，因此红颜色部分是大于零的，而 $\alpha_i$ 又是非负的，为了满足最大化，$\alpha_i$ 必须等于 0 。这也就是这些非 Supporting Vector 的点的悲惨命运了。 :p</p>
<p>嗯，于是呢，把所有的这些东西整合起来，得到的一个 maximum margin hyper plane classifier 就是支持向量机（Support Vector Machine），经过直观的感觉和数学上的推导，为什么叫“支持向量”，应该也就明了了吧？当然，到目前为止，我们的 SVM 还比较弱，只能处理线性的情况，不过，在得到了 dual 形式之后，通过 Kernel 推广到非线性的情况就变成了一件非常容易的事情了。不过，具体细节，还要留到下一次再细说了。 :)</p>
<p>##<strong>Kernel</strong> </p>
<p>前面我们介绍了线性情况下的支持向量机，它通过寻找一个线性的超平面来达到对数据进行分类的目的。不过，由于是线性方法，所以对非线性的数据就没有办法处理了。例如图中的两类数据，分别分布为两个圆圈的形状，不论是任何高级的分类器，只要它是线性的，就没法处理，SVM 也不行。因为这样的数据本身就是线性不可分的。</p>
<center><img src="http://blog.pluskid.org/wp-content/uploads/2010/09/two_circles.png" alt=""></center>

<p>对于这个数据集，我可以悄悄透露一下：我生成它的时候就是用两个半径不同的圆圈加上了少量的噪音得到的，所以，一个理想的分界应该是一个“圆圈”而不是一条线（超平面）。如果用 $X_1$ 和 $X_2$ 来表示这个二维平面的两个坐标的话，我们知道一条二次曲线（圆圈是二次曲线的一种特殊情况）的方程可以写作这样的形式：</p>
<p>$$<br>a_1X_1 + a_2X_1^2 + a_3 X_2 + a_4 X_2^2 + a_5 X_1X_2 + a_6 = 0<br>$$</p>
<p>注意上面的形式，如果我们构造另外一个五维的空间，其中五个坐标的值分别为 $Z_1 = X_1$, $Z_2=X_1^2$, $Z_3=X_2$, $Z_4=X_2^2$, $Z_5=X_1X_2$，那么显然，上面的方程在新的坐标系下可以写作：</p>
<p>$$<br>\sum_{i=1}^5a_iZ_i + a_6 = 0<br>$$</p>
<p>关于新的坐标 $Z$ ，这正是一个 hyper plane 的方程！也就是说，如果我们做一个映射 $\phi:\mathbb{R}^2\rightarrow\mathbb{R}^5$ ，将 $X$ 按照上面的规则映射为 $Z$ ，那么在新的空间中原来的数据将变成线性可分的，从而使用之前我们推导的线性分类算法就可以进行处理了。这正是 Kernel 方法处理非线性问题的基本思想。</p>
<p>再进一步描述 Kernel 的细节之前，不妨再来看看这个例子映射过后的直观例子。当然，我没有办法把 5 维空间画出来，不过由于我这里生成数据的时候就是用了特殊的情形，具体来说，我这里的超平面实际的方程是这个样子（圆心在 $X_2$ 轴上的一个正圆）：</p>
<p>$$<br>a_1X_1^2 + a_2(X_2-c)^2 + a_3 = 0<br>$$</p>
<p>因此我只需要把它映射到 $Z_1 = X_1^2$, $Z_2=X_2^2$, $Z_3=X_2$ 这样一个三维空间中即可，下图（这是一个 gif 动画）即是映射之后的结果，将坐标轴经过适当的旋转，就可以很明显地看出，数据是可以通过一个平面来分开的：</p>
<center><img src="http://blog.pluskid.org/wp-content/uploads/2010/09/rotate.gif" alt=""></center>

<p>现在让我们再回到 SVM 的情形，假设原始的数据时非线性的，我们通过一个映射 $\phi(\cdot)$ 将其映射到一个高维空间中，数据变得线性可分了，这个时候，我们就可以使用原来的推导来进行计算，只是所有的推导现在是在新的空间，而不是原始空间中进行。当然，推导过程也并不是可以简单地直接类比的，例如，原本我们要求超平面的法向量 $w$ ，但是如果映射之后得到的新空间的维度是无穷维的（确实会出现这样的情况，比如后面会提到的 Gaussian Kernel ），要表示一个无穷维的向量描述起来就比较麻烦。于是我们不妨先忽略过这些细节，直接从最终的结论来分析，回忆一下，我们上一次得到的最终的分类函数是这样的：</p>
<p>$$<br>f(x) = \sum_{i=1}^n\alpha_i y_i \langle x_i, x\rangle + b<br>$$</p>
<p>现在则是在映射过后的空间，即：</p>
<p>$$<br>f(x) = \sum_{i=1}^n\alpha_i y_i \langle \phi(x_i), \phi(x)\rangle + b<br>$$</p>
<p>而其中的 $\alpha$ 也是通过求解如下 dual 问题而得到的：</p>
<p>$$<br>\begin{aligned}<br>\max<em>\alpha &amp;\sum</em>{i=1}^n\alpha<em>i – \frac{1}{2}\sum</em>{i,j=1}^n\alpha_i\alpha_jy_iy_j\langle \phi(x_i),\phi(x_j)\rangle \<br>s.t., &amp;\alpha<em>i\geq 0, i=1,\ldots,n \<br>&amp;\sum</em>{i=1}^n\alpha_iy_i = 0<br>\end{aligned}<br>$$</p>
<p>这样一来问题就解决了吗？似乎是的：拿到非线性数据，就找一个映射 $\phi(\cdot)$ ，然后一股脑把原来的数据映射到新空间中，再做线性 SVM 即可。不过若真是这么简单，我这篇文章的标题也就白写了——说了这么多，其实还没到正题呐！其实刚才的方法稍想一下就会发现有问题：在最初的例子里，我们对一个二维空间做映射，选择的新空间是原始空间的所有一阶和二阶的组合，得到了五个维度；如果原始空间是三维，那么我们会得到 19 维的新空间（验算一下？），这个数目是呈爆炸性增长的，这给 $\phi(\cdot)$ 的计算带来了非常大的困难，而且如果遇到无穷维的情况，就根本无从计算了。所以就需要 Kernel 出马了。</p>
<p>不妨还是从最开始的简单例子出发，设两个向量 $x_1 = (\eta_1,\eta_2)^T$ 和 $x_2=(\xi_1,\xi_2)^T$ ，而 $\phi(\cdot)$ 即是到前面说的五维空间的映射，因此映射过后的内积为：</p>
<p>$$<br>\langle \phi(x_1),\phi(x_2)\rangle = \eta_1\xi_1 + \eta_1^2\xi_1^2 + \eta_2\xi_2 + \eta_2^2\xi_2^2+\eta_1\eta_2\xi_1\xi_2<br>$$</p>
<p>另外，我们又注意到：</p>
<p>$$<br>\left(\langle x_1, x_2\rangle + 1\right)^2 = 2\eta_1\xi_1 + \eta_1^2\xi_1^2 + 2\eta_2\xi_2 + \eta_2^2\xi_2^2 + 2\eta_1\eta_2\xi_1\xi_2 + 1<br>$$</p>
<p>二者有很多相似的地方，实际上，我们只要把某几个维度线性缩放一下，然后再加上一个常数维度，具体来说，上面这个式子的计算结果实际上和映射</p>
<p>$$<br>\varphi(X_1,X_2)=(\sqrt{2}X_1,X_1^2,\sqrt{2}X_2,X_2^2,\sqrt{2}X_1X_2,1)^T<br>$$</p>
<p>之后的内积 $\langle \varphi(x_1),\varphi(x_2)\rangle$ 的结果是相等的（自己验算一下）。区别在于什么地方呢？一个是映射到高维空间中，然后再根据内积的公式进行计算；而另一个则直接在原来的低维空间中进行计算，而不需要显式地写出映射后的结果。回忆刚才提到的映射的维度爆炸，在前一种方法已经无法计算的情况下，后一种方法却依旧能从容处理，甚至是无穷维度的情况也没有问题。</p>
<p>我们把这里的计算两个向量在映射过后的空间中的内积的函数叫做核函数 (Kernel Function) ，例如，在刚才的例子中，我们的核函数为：</p>
<p>$$<br>\kappa(x_1,x_2)=\left(\langle x_1, x_2\rangle + 1\right)^2<br>$$</p>
<p>核函数能简化映射空间中的内积运算——刚好“碰巧”的是，在我们的 SVM 里需要计算的地方数据向量总是以内积的形式出现的。对比刚才我们写出来的式子，现在我们的分类函数为：</p>
<p>$$<br>\sum_{i=1}^n\alpha_i y_i \color{red}{\kappa(x_i,x)} + b<br>$$</p>
<p>其中 $\alpha$ 由如下 dual 问题计算而得：</p>
<p>$$<br>\begin{aligned}<br>\max<em>\alpha &amp;\sum</em>{i=1}^n\alpha<em>i – \frac{1}{2}\sum</em>{i,j=1}^n\alpha_i\alpha_jy_iy_j \color{red}{\kappa(x_i, x_j)} \<br>s.t., &amp;\alpha<em>i\geq 0, i=1,\ldots,n \<br>&amp;\sum</em>{i=1}^n\alpha_iy_i = 0<br>\end{aligned}<br>$$</p>
<p>这样一来计算的问题就算解决了，避开了直接在高维空间中进行计算，而结果却是等价的，实在是一件非常美妙的事情！当然，因为我们这里的例子非常简单，所以我可以手工构造出对应于 $\varphi(\cdot)$ 的核函数出来，如果对于任意一个映射，想要构造出对应的核函数就很困难了。</p>
<p>最理想的情况下，我们希望知道数据的具体形状和分布，从而得到一个刚好可以将数据映射成线性可分的 $\phi(\cdot)$ ，然后通过这个 $\phi(\cdot)$ 得出对应的 $\kappa(\cdot,\cdot)$ 进行内积计算。然而，第二步通常是非常困难甚至完全没法做的。不过，由于第一步也是几乎无法做到，因为对于任意的数据分析其形状找到合适的映射本身就不是什么容易的事情，所以，人们通常都是“胡乱”选择映射的，所以，根本没有必要精确地找出对应于映射的那个核函数，而只需要“胡乱”选择一个核函数即可——我们知道它对应了某个映射，虽然我们不知道这个映射具体是什么。由于我们的计算只需要核函数即可，所以我们也并不关心也没有必要求出所对应的映射的具体形式。 :D</p>
<p>当然，说是“胡乱”选择，其实是夸张的说法，因为并不是任意的二元函数都可以作为核函数，所以除非某些特殊的应用中可能会构造一些特殊的核（例如用于文本分析的文本核，注意其实使用了 Kernel 进行计算之后，其实完全可以去掉原始空间是一个向量空间的假设了，只要核函数支持，原始数据可以是任意的“对象”——比如文本字符串），通常人们会从一些常用的核函数中选择（根据问题和数据的不同，选择不同的参数，实际上就是得到了不同的核函数），例如：</p>
<p>多项式核 $\kappa(x_1,x_2) = \left(\langle x_1,x_2\rangle + R\right)^d$ ，显然刚才我们举的例子是这里多项式核的一个特例（$R=1,d=2$）。虽然比较麻烦，而且没有必要，不过这个核所对应的映射实际上是可以写出来的，该空间的维度是 $\binom{m+d}{d}$ ，其中 $m$ 是原始空间的维度。<br>高斯核 $\kappa(x_1,x_2) = \exp\left(-\frac{|x_1-x_2|^2}{2\sigma^2}\right)$ ，这个核就是最开始提到过的会将原始空间映射为无穷维空间的那个家伙。不过，如果 $\sigma$ 选得很大的话，高次特征上的权重实际上衰减得非常快，所以实际上（数值上近似一下）相当于一个低维的子空间；反过来，如果 $\sigma$ 选得很小，则可以将任意的数据映射为线性可分——当然，这并不一定是好事，因为随之而来的可能是非常严重的过拟合问题。不过，总的来说，通过调控参数 $\sigma$ ，高斯核实际上具有相当高的灵活性，也是使用最广泛的核函数之一。<br>线性核 $\kappa(x_1,x_2) = \langle x_1,x_2\rangle$ ，这实际上就是原始空间中的内积。这个核存在的主要目的是使得“映射后空间中的问题”和“映射前空间中的问题”两者在形式上统一起来了。<br>最后，总结一下：对于非线性的情况，SVM 的处理方法是选择一个核函数 $\kappa(\cdot,\cdot)$ ，通过将数据映射到高维空间，来解决在原始空间中线性不可分的问题。由于核函数的优良品质，这样的非线性扩展在计算量上并没有比原来复杂多少，这一点是非常难得的。当然，这要归功于核方法——除了 SVM 之外，任何将计算表示为数据点的内积的方法，都可以使用核方法进行非线性扩展。</p>
<p>此外，略微提一下，也有不少工作试图自动构造专门针对特定数据的分布结构的核函数，感兴趣的同学可以参考，比如 NIPS 2003 的 Cluster Kernels for Semi-Supervised Learning 和 ICML 2005 的 Beyond the point cloud: from transductive to semi-supervised learning 等。</p>
<p>##<strong>Outliers</strong> </p>
<p>在最开始讨论支持向量机的时候，我们就假定，数据是线性可分的，亦即我们可以找到一个可行的超平面将数据完全分开。后来为了处理非线性数据，使用 Kernel 方法对原来的线性 SVM 进行了推广，使得非线性的的情况也能处理。虽然通过映射 $\phi(\cdot)$ 将原始数据映射到高维空间之后，能够线性分隔的概率大大增加，但是对于某些情况还是很难处理。例如可能并不是因为数据本身是非线性结构的，而只是因为数据有噪音。对于这种偏离正常位置很远的数据点，我们称之为 outlier ，在我们原来的 SVM 模型里，outlier 的存在有可能造成很大的影响，因为超平面本身就是只有少数几个 support vector 组成的，如果这些 support vector 里又存在 outlier 的话，其影响就很大了。例如下图：</p>
<center><img src="http://blog.pluskid.org/wp-content/uploads/2010/09/Optimal-Hyper-Plane-2.png" alt=""></center>

<p>用黑圈圈起来的那个蓝点是一个 outlier ，它偏离了自己原本所应该在的那个半空间，如果直接忽略掉它的话，原来的分隔超平面还是挺好的，但是由于这个 outlier 的出现，导致分隔超平面不得不被挤歪了，变成途中黑色虚线所示（这只是一个示意图，并没有严格计算精确坐标），同时 margin 也相应变小了。当然，更严重的情况是，如果这个 outlier 再往右上移动一些距离的话，我们将无法构造出能将数据分开的超平面来。</p>
<p>为了处理这种情况，SVM 允许数据点在一定程度上偏离一下超平面。例如上图中，黑色实线所对应的距离，就是该 outlier 偏离的距离，如果把它移动回来，就刚好落在原来的超平面上，而不会使得超平面发生变形了。具体来说，原来的约束条件</p>
<p>$$<br>y_i(w^Tx_i+b)\geq 1, \quad i=1,\ldots,n<br>$$</p>
<p>现在变成</p>
<p>$$<br>y_i(w^Tx_i+b)\geq 1\color{red}{-\xi_i}, \quad i=1,\ldots,n<br>$$</p>
<p>其中 $\xi_i\geq 0$ 称为松弛变量 (slack variable) ，对应数据点 $x_i$ 允许偏离的 functional margin 的量。当然，如果我们运行 $\xi_i$ 任意大的话，那任意的超平面都是符合条件的了。所以，我们在原来的目标函数后面加上一项，使得这些 $\xi_i$ 的总和也要最小：</p>
<p>$$<br>\min \frac{1}{2}|w|^2\color{red}{+C\sum_{i=1}^n \xi_i}<br>$$</p>
<p>其中 $C$ 是一个参数，用于控制目标函数中两项（“寻找 margin 最大的超平面”和“保证数据点偏差量最小”）之间的权重。注意，其中 $\xi$ 是需要优化的变量（之一），而 $C$ 是一个事先确定好的常量。完整地写出来是这个样子：</p>
<p>$$<br>\begin{aligned}<br>\min &amp; \frac{1}{2}|w|^2 + C\sum_{i=1}^n\xi_i \<br>s.t., &amp; y_i(w^Tx_i+b)\geq 1-\xi_i, i=1,\ldots,n \<br>&amp; \xi_i \geq 0, i=1,\ldots,n<br>\end{aligned}<br>$$</p>
<p>用之前的方法将限制加入到目标函数中，得到如下问题：</p>
<p>$$<br>\mathcal{L}(w,b,\xi,\alpha,r)=\frac{1}{2}|w|^2 + C\sum_{i=1}^n\xi<em>i – \sum</em>{i=1}^n\alpha_i \left(y_i(w^Tx_i+b)-1+\xi<em>i\right) – \sum</em>{i=1}^n r_i\xi_i<br>$$</p>
<p>分析方法和前面一样，转换为另一个问题之后，我们先让 $\mathcal{L}$ 针对 $w$、$b$ 和 $\xi$ 最小化：</p>
<p>$$<br>\begin{aligned}<br>\frac{\partial \mathcal{L}}{\partial w}=0 &amp;\Rightarrow w=\sum_{i=1}^n \alpha_i y_i x<em>i \<br>\frac{\partial \mathcal{L}}{\partial b} = 0 &amp;\Rightarrow \sum</em>{i=1}^n \alpha_i y_i = 0 \<br>\frac{\partial \mathcal{L}}{\partial \xi_i} = 0 &amp;\Rightarrow C-\alpha_i-r_i=0, \quad i=1,\ldots,n<br>\end{aligned}<br>$$</p>
<p>将 $w$ 带回 $\mathcal{L}$ 并化简，得到和原来一样的目标函数：</p>
<p>$$<br>\max<em>\alpha \sum</em>{i=1}^n\alpha<em>i – \frac{1}{2}\sum</em>{i,j=1}^n\alpha_i\alpha_jy_iy_j\langle x_i,x_j\rangle<br>$$</p>
<p>不过，由于我们得到 $C-\alpha_i-r_i=0$ ，而又有 $r_i\geq 0$ （作为 Lagrange multiplier 的条件），因此有 $\alpha_i\leq C$ ，所以整个 dual 问题现在写作：</p>
<p>$$<br>\begin{aligned}<br>\max<em>\alpha &amp;\sum</em>{i=1}^n\alpha<em>i – \frac{1}{2}\sum</em>{i,j=1}^n\alpha_i\alpha_jy_iy_j\langle x_i,x_j\rangle \<br>s.t., &amp;0\leq \alpha<em>i\leq C, i=1,\ldots,n \<br>&amp;\sum</em>{i=1}^n\alpha_iy_i = 0<br>\end{aligned}<br>$$</p>
<p>和之前的结果对比一下，可以看到唯一的区别就是现在 dual variable $\alpha$ 多了一个上限 $C$ 。而 Kernel 化的非线性形式也是一样的，只要把 $\langle x_i,x_j \rangle$ 换成 $\kappa(x_i,x_j)$ 即可。这样一来，一个完整的，可以处理线性和非线性并能容忍噪音和 outliers 的支持向量机才终于介绍完毕了。 :)</p>
<p>##<strong>Numerical Optimization</strong> </p>
<p>作为支持向量机系列的基本篇的最后一篇文章，我在这里打算简单地介绍一下用于优化 dual 问题的 Sequential Minimal Optimization (SMO) 方法。确确实实只是简单介绍一下，原因主要有两个：第一这类优化算法，特别是牵涉到实现细节的时候，干巴巴地讲算法不太好玩，有时候讲出来每个人实现得结果还不一样，提一下方法，再结合实际的实现代码的话，应该会更加明了，而且也能看出理论和实践之间的差别；另外（其实这个是主要原因）我自己对这一块也确实不太懂。 :p</p>
<p>先回忆一下我们之前得出的要求解的 dual 问题：</p>
<p>$$<br>\begin{aligned}<br>\max<em>\alpha &amp;\sum</em>{i=1}^n\alpha<em>i – \frac{1}{2}\sum</em>{i,j=1}^n\alpha_i\alpha_jy_iy_j\kappa( x_i,x_j) \<br>s.t., &amp;0\leq \alpha<em>i\leq C, i=1,\ldots,n \<br>&amp;\sum</em>{i=1}^n\alpha_iy_i = 0<br>\end{aligned}<br>$$</p>
<p>对于变量 $\alpha$ 来说，这是一个 quadratic 函数。通常对于优化问题，我们没有办法的时候就会想到最笨的办法——Gradient Descent ，也就是梯度下降。注意我们这里的问题是要求最大值，只要在前面加上一个负号就可以转化为求最小值，所以 Gradient Descent 和 Gradient Ascend 并没有什么本质的区别，其基本思想直观上来说就是：梯度是函数值增幅最大的方向，因此只要沿着梯度的反方向走，就能使得函数值减小得越大，从而期望迅速达到最小值。当然普通的 Gradient Descent 并不能保证达到最小值，因为很有可能陷入一个局部极小值。不过对于 quadratic 问题，极值只有一个，所以是没有局部极值的问题。</p>
<p>另外还有一种叫做 Coordinate Descend 的变种，它每次只选择一个维度，例如 $\alpha=(\alpha_1,\ldots,\alpha_n)$ ，它每次选取 $\alpha_i$ 为变量，而将 $\alpha<em>1,\ldots,\alpha</em>{i-1},\alpha_{i+1},\ldots,\alpha_n$ 都看成是常量，从而原始的问题在这一步变成一个一元函数，然后针对这个一元函数求最小值，如此反复轮换不同的维度进行迭代。Coordinate Descend 的主要用处在于那些原本很复杂，但是如果只限制在一维的情况下则变得很简单甚至可以直接求极值的情况，例如我们这里的问题，暂且不管约束条件，如果只看目标函数的话，当 $\alpha$ 只有一个分量是变量的时候，这就是一个普通的一元二次函数的极值问题，初中生也会做，带入公式即可。</p>
<p>然而这里还有一个问题就是约束条件的存在，其实如果没有约束条件的话，本身就是一个多元的 quadratic 问题，也是很好求解的。但是有了约束条件，结果让 Coordinate Descend 变得很尴尬了：比如我们假设 $\alpha_1$ 是变量，而 $\alpha_2,\ldots,\alpha<em>n$ 是固定值的话，那么其实没有什么好优化的了，直接根据第二个约束条件 $\sum</em>{i=1}^n\alpha_iy_i = 0$ ，$\alpha_1$ 的值立即就可以定下来——事实上，迭代每个坐标维度，最后发现优化根本进行不下去，因为迭代了一轮之后会发现根本没有任何进展，一切都停留在初始值。</p>
<p>所以 Sequential Minimal Optimization (SMO) 一次选取了两个坐标维度来进行优化。例如（不失一般性），我们假设现在选取 $\alpha_1$ 和 $\alpha_2$ 为变量，其余为常量，则根据约束条件我们有：</p>
<p>$$<br>\sum_{i=1}^n\alpha_iy_i = 0 \Rightarrow \alpha_2=\frac{1}{y<em>2}\left(\sum</em>{i=3}^n\alpha_iy_i-\alpha_1y_1\right) \triangleq y_2\left(K-\alpha_1y_1\right)<br>$$</p>
<p>其中那个从 3 到 n 的作和由于都是常量，我们统一记作 $K$ ，然后由于 $y\in{-1,+1}$ ，所以 $y_2$ 和 $1/y_2$ 是完全一样的，所以可以拿到分子上来。将这个式子带入原来的目标函数中，可以消去 $\alpha_2$ ，从而变成一个一元二次函数，具体展开的形式我就不写了，总之现在变成了一个非常简单的问题：带区间约束的一元二次函数极值问题——这个也是初中就学过求解方法的。唯一需要注意一点的就是这里的约束条件，一个就是 $\alpha_1$ 本身需要满足 $0\leq\alpha_1\leq C$ ，然后由于 $\alpha_2$ 也要满足同样的约束，即：</p>
<p>$$<br>0\leq y_2 (K-\alpha_1y_1) \leq C<br>$$</p>
<p>也可以得到 $\alpha_1$ 的一个可行区间，同 $[0,C]$ 交集即可得到最终的可行区间。这个问题可以从图中得到一个直观的感觉。原本关于 $\alpha_1$ 和 $\alpha_2$ 的区间限制构成途中绿色的的方块，而另一个约束条件 $y_1\alpha_1 + y_2\alpha_2 = K$ 实际上表示一条直线，两个集合的交集即是途中红颜色的线段，投影到 $\alpha_1$ 轴上所对应的区间即是 $\alpha_1$ 的取值范围，在这个区间内求二次函数的最大值即可完成 SMO 的一步迭代。</p>
<center><img src="http://blog.pluskid.org/wp-content/uploads/2010/09/interval.png" alt=""></center>

<p>同 Coordinate Descent 一样，SMO 也会选取不同的两个 coordinate 维度进行优化，可以看出由于每一个迭代步骤实际上是一个可以直接求解的一元二次函数极值问题，所以求解非常高效。此外，SMO 也并不是依次或者随机地选取两个坐标维度，而是有一些启发式的策略来选取最优的两个坐标维度，具体的选取方法（和其他的一些细节），可以参见 John C. Platt 的那篇论文 Fast Training of Support Vector Machines Using Sequential Minimal Optimization 。关于 SMO ，我就不再多说了。如果你对研究实际的代码比较感兴趣，可以去看 LibSVM 的实现，当然，它那个也许已经不是原来版本的 SMO 了，因为本来 SVM 的优化就是一个有许多研究工作的领域，在那些主要的优化方法之上，也有各种改进的办法或者全新的算法提出来。</p>
<p>除了 LibSVM 之外，另外一个流行的实现 SVMlight 似乎是用了另一种优化方法，具体可以参考一下它相关的论文 Making large-Scale SVM Learning Practical 。</p>
<p>此外，虽然我们从 dual 问题的推导中得出了许多 SVM 的优良性质，但是 SVM 的数值优化（即使是非线性的版本）其实并不一定需要转化为 dual 问题来完成的，具体做法我并不清楚，不过这方面的文章也不少，比如 2007 年 Neural Computation 的一篇 Training a support vector machine in the primal 。如果感兴趣可以参考一下。 :)</p>
<p>#<strong>Duality</strong><br>简单来说，对于任意一个带约束的优化都可以写成这样的形式：</p>
<p>$$<br>\begin{aligned}<br>\min&amp;f_0(x) \<br>s.t. &amp;f_i(x)\leq 0, \quad i=1,\ldots,m\<br>&amp;h_i(x)=0, \quad i=1,\ldots,p<br>\end{aligned}<br>$$</p>
<p>形式统一能够简化推导过程中不必要的复杂性。其他的形式都可以归约到这样的标准形式，例如一个 $\max f(x)$ 可以转化为 $\min -f(x)$ 等。假如 $f_0,f_1,\ldots,f_m$ 全都是凸函数，并且 $h_1,\ldots,h_p$ 全都是仿射函数（就是形如 $Ax+b$ 的形式），那么这个问题就叫做凸优化（Convex Optimization）问题。凸优化问题有许多优良的性质，例如它的极值是唯一的。不过，这里我们并没有假定需要处理的优化问题是一个凸优化问题。</p>
<p>虽然约束条件能够帮助我们减小搜索空间，但是如果约束条件本身就是比较复杂的形式的话，其实是一件很让人头痛的问题，为此我们希望把带约束的优化问题转化为无约束的优化问题。为此，我们定义 Lagrangian 如下：</p>
<p>$$<br>L(x,\lambda,\nu)=f<em>0(x)+\sum</em>{i=1}^m\lambda_if<em>i(x)+\sum</em>{i=1}^p\nu_ih_i(x)<br>$$</p>
<p>它通过一些系数把约束条件和目标函数结合在了一起。当然 Lagrangian 本身并不好玩，现在让我们来让他针对 $\lambda$ 和 $\nu$ 最大化，令：</p>
<p>$$<br>z(x)=\max_{\lambda\succeq 0, \nu}L(x,\lambda,\nu)<br>$$</p>
<p>这里 $\lambda\succeq 0$ 理解为向量 $\lambda$ 的每一个元素都非负即可。这个函数 $z(x)$ 对于满足原始问题约束条件的那些 $x$ 来说，其值等于 $f_0(x)$ ，这很容易验证，因为满足约束条件的 $x$ 会使得 $h_i(x)=0$ ，因此最后一项消掉了，而 $f_i(x)\leq 0$ ，并且我们要求了 $\lambda \succeq 0$ ，因此 $\lambda_if_i(x)\leq 0$ ，所以最大值只能在它们都取零的时候得到，这个时候就只剩下 $f_0(x)$ 了。因此，对于满足约束条件的那些 $x$ 来说，$f_0(x)=z(x)$ 。这样一来，原始的带约束的优化问题其实等价于如下的无约束优化问题：</p>
<p>$$<br>\min_x z(x)<br>$$</p>
<p>因为如果原始问题有最优值，那么肯定是在满足约束条件的某个 $x^* $ 取得，而对于所有满足约束条件的 $x$ ，$z(x)$ 和 $f_0(x)$ 都是相等的。至于那些不满足约束条件的 $x$ ，原始问题是无法取到的，否则极值问题无解。很容易验证对于这些不满足约束条件的 $x$ 有 $z(x)=\infty$，这也和原始问题是一致的，因为求最小值得到无穷大可以和“无解”看作是相容的。</p>
<p>到这里，我们成功把带约束问题转化为了无约束问题，不过这其实只是一个形式上的重写，并没有什么本质上的改变。我们只是把原来的问题通过 Lagrangian 写作了如下形式：</p>
<p>$$<br>\min<em>x\ \max</em>{\lambda\succeq 0, \nu} L(x, \lambda, \nu)<br>$$</p>
<p>这个问题（或者说原始的带约束的形式）称作 primal problem 。相对应的还有一个 dual problem ，其形式非常类似，只是把 $\min$ 和 $\max$ 交换了一下：</p>
<p>$$<br>\max_{\lambda\succeq 0, \nu}\ \min_x L(x, \lambda, \nu)<br>$$</p>
<p>交换之后的 dual problem 和原来的 primal problem 并不相等，直观地，我们可以这样来理解：胖子中最瘦的那个都比瘦骨精中最胖的那个要胖。当然这是很不严格的说法，而且扣字眼的话可以纠缠不休，所以我们还是来看严格数学描述。和刚才的 $z(x)$ 类似，我们也用一个记号来表示内层的这个函数，记：</p>
<p>$$<br>g(\lambda,\nu) = \min_x L(x, \lambda, \nu)<br>$$</p>
<p>并称 $g(\lambda,\nu)$ 为 Lagrange dual function （不要和 $L$ 的 Lagrangian 混淆了）。$g$ 有一个很好的性质就是它是 primal problem 的一个下界。换句话说，如果 primal problem 的最小值记为 $p^* $ ，那么对于所有的 $\lambda \succeq 0$ 和 $\nu$ ，我们有：</p>
<p>$$<br>g(\lambda,\nu)\leq p^*<br>$$</p>
<p>因为对于极值点（实际上包括所有满足约束条件的点）$x^* $，注意到 $\lambda\succeq 0$ ，我们总是有</p>
<p>$$<br>\sum_{i=1}^m\lambda_if<em>i(x^* )+\sum</em>{i=1}^p\nu_ih_i(x^* )\leq 0<br>$$</p>
<p>因此</p>
<p>$$<br>L(x^<em> ,\lambda,\nu)=f_0(x^</em> )+\sum_{i=1}^m\lambda_if<em>i(x^* )+\sum</em>{i=1}^p\nu_ih_i(x^<em> )\leq f_0(x^</em> )<br>$$</p>
<p>于是</p>
<p>$$<br>g(\lambda,\nu)=\min_x L(x,\lambda,\nu)\leq L(x^<em> ,\lambda,\nu)\leq f_0(x^</em> )=p^*<br>$$</p>
<p>这样一来就确定了 $g$ 的下界性质，于是</p>
<p>$$<br>\max_{\lambda\succeq 0,\nu}g(\lambda,\nu)<br>$$</p>
<p>实际上就是最大的下界。这是很自然的，因为得到下界之后，我们自然地就希望得到最好的下界，也就是最大的那一个——因为它离我们要逼近的值最近呀。记 dual problem 的最优值为 $d^* $ 的话，根据上面的推导，我们就得到了如下性质：</p>
<p>$$<br>d^<em> \leq p^</em><br>$$</p>
<p>这个性质叫做 weak duality ，对于所有的优化问题都成立。其中 $p^<em> -d^</em> $ 被称作 duality gap 。需要注意的是，无论 primal problem 是什么形式，dual problem 总是一个 convex optimization 的问题——它的极值是唯一的（如果存在的话），并且有现成的软件包可以对凸优化问题进行求解（虽然求解 general 的 convex optimization 实际上是很慢并且只能求解规模较小的问题的）。这样一来，对于那些难以求解的 primal problem （比如，甚至可以是 NP 问题），我们可以通过找出它的 dual problem ，通过优化这个 dual problem 来得到原始问题的一个下界估计。或者说我们甚至都不用去优化这个 dual problem ，而是（通过某些方法，例如随机）选取一些 $\lambda\succeq 0$ 和 $\nu$ ，带到 $g(\lambda,\nu)$ 中，这样也会得到一些下界（只不过不一定是最大的那个下界而已）。当然要选 $\lambda$ 和 $\nu$ 也并不是总是“随机选”那么容易，根据具体问题，有时候选出来的 $\lambda$ 和 $\nu$ 带入 $g$ 会得到 $-\infty$ ，这虽然是一个完全合法的下界，然而却并没有给我们带来任何有用的信息。</p>
<p>故事到这里还没有结束，既然有 weak duality ，显然就会有 strong duality 。所谓 strong duality ，就是</p>
<p>$$<br>d^<em> =p^</em><br>$$</p>
<p>这是一个很好的性质，strong duality 成立的情况下，我们可以通过求解 dual problem 来优化 primal problem ，在 SVM 中我们就是这样做的。当然并不是所有的问题都能满足 strong duality ，在讲 SVM 的时候我们直接假定了 strong duality 的成立，这里我们就来提一下 strong duality 成立的条件。不过，这个问题如果要讲清楚，估计写一本书都不够，应该也有不少专门做优化方面的人在研究这相关的问题吧，我没有兴趣（当然也没有精力和能力）来做一个完整的介绍，相信大家也没有兴趣来看这样的东西——否则你肯定是专门研究优化方面的问题的了，此时你肯定比我懂得更多，也就不用看我写的介绍啦。 :p</p>
<p>所以，这里我们就简要地介绍一下 Slater 条件和 KKT 条件。Slater 条件是指存在严格满足约束条件的点 $x$ ，这里的“严格”是指 $f_i(x)\leq 0$ 中的“小于或等于号”要严格取到“小于号”，亦即，存在 $x$ 满足</p>
<p>$$<br>\begin{aligned}<br>f_i(x)&lt;0&amp;\quad i=1,\ldots,m\ h_i(x)=0&amp;\quad i=1,\ldots,p \end{aligned} $$ 我们有：如果原始问题是 Convex 的并且满足 Slater 条件的话，那么 strong duality 成立。需要注意的是，这里只是指出了 strong duality 成立的一种情况，而并不是唯一情况。例如，对于某些非 convex optimization 的问题，strong duality 也成立。这里我们不妨回顾一下 SVM 的 primal problem ，那是一个 convex optimization 问题（QP 是凸优化问题的一种特殊情况），而 Slater 条件实际上在这里就等价于是存在这样的一个超平面将数据分隔开来，亦即是“数据是可分的”。当数据不可分是，strong duality 不能成立，不过，这个时候我们寻找分隔平面这个问题本身也就是没有意义的了，至于我们如何通过把数据映射到特征空间中来解决不可分的问题，这个当时已经介绍过了，这里就不多说了。</p>
<p>让我们回到 duality 的话题。来看看 strong duality 成立的时候的一些性质。假设 $x^<em>  $ 和 $(\lambda^</em>  ,\nu^<em>  )$ 分别是 primal problem 和 dual problem 的极值点，相应的极值为 $p^</em>  $ 和 $d^<em>  $ ，首先 $p^</em>  =d^*  $ ，此时我们可以得到</p>
<p>$$<br>\begin{aligned}<br>f_0(x^<em>  )&amp;=g(\lambda^</em>  ,\nu^<em> )\<br>&amp;=\min_x\left(f<em>0(x)+\sum</em>{i=1}^m\lambda_i^</em> f<em>i(x)+\sum</em>{i=1}^p\nu_i^<em> h_i(x)\right)\<br>&amp;\leq f_0(x^</em>  )+\sum_{i=1}^m\lambda_i^<em>  f_i(x^</em>  )+\sum_{i=1}^p\nu_i^<em>  h_i(x^</em>  ) \<br>&amp;\leq f_0(x^*  )<br>\end{aligned}<br>$$</p>
<p>由于两头是相等的，所以这一系列的式子里的不等号全部都可以换成等号。根据第一个不等号我们可以得到 $x^<em>  $ 是 $L(x,\lambda^</em>  ,\nu^<em>  )$ 的一个极值点，由此可以知道 $L(x,\lambda^</em>  ,\nu^<em>  )$ 在 $x^</em> $ 处的梯度应该等于 0 ，亦即：</p>
<p>$$<br>\nabla f<em>0(x^* )+\sum</em>{i=1}^m\lambda_i^<em>  \nabla f_i(x^</em> )+\sum_{i=1}^p\nu_i^<em>  \nabla h_i(x^</em> )=0<br>$$</p>
<p>此外，由第二个不等式，又显然 $\lambda_i^<em> f_i(x^</em> )$ 都是非正的，因此我们可以得到</p>
<p>$$<br>\lambda_i^<em> f_i(x^</em> )=0,\quad i=1,\ldots,m<br>$$</p>
<p>这个条件叫做 complementary slackness 。显然，如果 $\lambda_i^<em> &gt;0$，那么必定有 $f_i(x^</em> )=0$ ；反过来，如果 $f_i(x^<em> )&lt;0$ 那么可以得到 $\lambda_i^</em> =0$ 。这个条件正是我们在介绍支持向量的文章末尾时用来证明那些非支持向量（对应于 $f_i(x^* )&lt;0$）所对应的系数 $\alpha_i$ （在本文里对应 $\lambda_i$）是为零的。 :) 再将其他一些显而易见的条件写到一起，就是传说中的KKT(Karush-Kuhn-Tucker) 条件： </p>
<p>$$<br>\begin{aligned}<br>&amp;f_i(x^<em> )\leq 0,\quad i=1,\ldots,m\<br>&amp;h_i(x^</em> )=0,\quad i=1,\ldots,p\<br>&amp;\lambda_i^<em> \geq 0,\quad i=1,\ldots,m\<br>&amp;\lambda_i^</em> f_i(x^<em> )=0,\quad i=1,\ldots,m\<br>\end{aligned} \<br>\textstyle\nabla f_0(x^</em> )+\sum_{i=1}^m\lambda_i^<em> \nabla f_i(x^</em> )+\sum_{i=1}^p\nu_i^<em>  \nabla h_i(x^</em> )=0<br>$$</p>
<p>任何满足 strong duality （不一定要求是通过 Slater 条件得到，也不一定要求是凸优化问题）的问题都满足 KKT 条件，换句话说，这是 strong duality 的一个必要条件。不过，当原始问题是凸优化问题的时候（当然还要求一应函数是可微的，否则 KKT 条件的最后一个式子就没有意义了），KKT 就可以升级为充要条件。换句话说，如果 primal problem 是一个凸优化问题，且存在 $\tilde{x}$ 和 $(\tilde{\lambda},\tilde{\nu})$ 满足 KKT 条件，那么它们分别是 primal problem 和 dual problem 的极值点并且 strong duality 成立。 其证明也比较简单，首先 primal problem 是凸优化问题的话，$g(\lambda,\nu)=\min_x L(x,\lambda,\nu)$ 的求解对每一组固定的 $(\lambda,\nu)$ 来说也是一个凸优化问题，由 KKT 条件的最后一个式子，知道 $\tilde{x}$ 是 $\min_x L(x,\tilde{\lambda},\tilde{\nu})$ 的极值点（如果不是凸优化问题，则不一定能推出来），亦即： $$ \begin{aligned} g(\tilde{\lambda},\tilde{\nu}) &amp;= \min_x L(x,\tilde{\lambda},\tilde{\nu}) \ &amp;= L(\tilde{x},\tilde{\lambda},\tilde{\nu}) \ &amp; = f<em>0(\tilde{x})+\sum</em>{i=1}^m\tilde{\lambda}_i^<em> f<em>i(\tilde{x})+\sum</em>{i=1}^p\tilde{\nu_i}^</em> h_i(\tilde{x}) \ &amp;= f_0(\tilde{x}) \end{aligned} $$ 最后一个式子是根据 KKT 条件的第二和第四个条件得到。由于 $g$ 是 $f_0$ 的下界，这样一来，就证明了 duality gap 为零，也就是说，strong duality 成立。 到此为止，做一下总结。我们简要地介绍了 duality 的概念，基本上没有给什么具体的例子。不过由于内容比较多，为了避免文章超长，就挑了一些重点讲了一下。总的来说，一个优化问题，通过求出它的 dual problem ，在只有 weak duality 成立的情况下，我们至少可以得到原始问题的一个下界。而如果 strong duality 成立，则可以直接求解 dual problem 来解决原始问题，就如同经典的 SVM 的求解过程一样。有可能 dual problem 比 primal problem 更容易求解，或者 dual problem 有一些优良的结构（例如 SVM 中通过 dual problem 我们可以将问题表示成数据的内积形式从而使得 kernel trick 的应用成为可能）。此外，还有一些情况会同时求解 dual 和 primal problem ，比如在迭代求解的过程中，通过判断 duality gap 的大小，可以得出一个有效的迭代停止条件。</p>
<p>##<strong>Kernel II</strong><br>在之前我们介绍了如何用 Kernel 方法来将线性 SVM 进行推广以使其能够处理非线性的情况，那里用到的方法就是通过一个非线性映射 $\phi(\cdot)$ 将原始数据进行映射，使得原来的非线性问题在映射之后的空间中变成线性的问题。然后我们利用核函数来简化计算，使得这样的方法在实际中变得可行。不过，从线性到非线性的推广我们并没有把 SVM 的式子从头推导一遍，而只是直接把最终得到的分类函数</p>
<p>$$<br>f(x) = \sum_{i=1}^n\alpha_i y_i \langle x_i, x\rangle + b<br>$$</p>
<center><img src="http://blog.pluskid.org/wp-content/uploads/2011/01/infinity.png" alt=""></center>

<p>中的内积换成了映射后的空间中的内积，并进一步带入了核函数进行计算。如果映射过后的空间是有限维的，那么这样的做法是可行的，因为之前的推导过程会一模一样，只是特征空间的维度变化了而已，相当于做了一些预处理。但是如果映射后的空间是无限维的，还能不能这么做呢？答案当然是能，因为我们已经在这么做了嘛！ :) 但是理由却并不是理所当然的，从有限到无限的推广许多地方都可以“直观地”类比，但是这样的直观性仍然需要严格的数学背景来支持，否则就会在一些微妙的地方出现一些奇怪的“悖论”（例如比较经典的芝诺的那些悖论）。当然这是一个很大的坑，没法填，所以这次我们只是来浮光掠影地看一看核方法背后的故事。</p>
<p>回忆一下原来我们做的非线性映射 $\phi$ ，它将原始特征空间中的数据点映射到另一个高维空间中，之前我们没有提过，其实这个高维空间在这里有一个华丽的名字——“再生核希尔伯特空间 (Reproducing Kernel Hilbert Space, RKHS)”。“再生核”就是指的我们用于计算内积的核函数，再说“再生”之前，我们先来简单地介绍一下 Hilbert Space ，它其实是欧氏空间的一个推广。首先从基本的向量空间开始，空间中的点具有加法和数乘的操作，在这个向量空间上定义一个内积操作，于是空间将升级为内积空间。根据内积可以定义一个范数：</p>
<p>$$<br>|x|^2=\langle x, x\rangle<br>$$</p>
<p>从而成为一个赋范向量空间。范数可以用于定义一个度量</p>
<p>$$<br>d(x_1,x_2) = |x_1-x_2|<br>$$</p>
<p>从而成为一个度量空间。如果这样的空间在这个度量下是完备的，那么这个空间叫做 Hilbert Space 。简单地来说，Hilbert Space 就是完备的内积空间。最简单的例子就是欧氏空间 $\mathbb{R}^m$ ，这是一个 $m$ 维的 Hilbert Space ，无穷维的例子比如是区间 $[a,b]$ 上的连续函数所组成的空间，并使用如下的内积定义</p>
<p>$$<br>\langle f_1,f_2\rangle = \int_a^bf_1(t)f_2(t)dt<br>$$</p>
<p>我们这里的 RKHS 就是一个函数空间。实际上，在这里我们有一个很有用的性质，就是维度相同的 Hilbert Space 是互相同构的——也就是说空间的各种结构（包括内积、范数、度量和向量运算等）都可以在不同的空间之间转换的时候得到保持。有了这样的性质，就可以让我们不用去关心 RKHS 中的点到底是什么。</p>
<p>将映射记为 $\phi:\mathcal{X}\rightarrow \mathcal{H}$ ，这里 $\mathcal{H}$ 表示 RKHS，用 $f$ 表示里面的元素 ；而 $\mathcal{X}$ 是原始特征空间，这里我们甚至不需要要求原始空间必须要是一个欧氏空间或者向量空间（这也是核方法的优点之一），用 $x$ 表示里面的点。由于刚才说了 $\mathcal{H}$ 中点的本质是什么对于我们的计算不会产生影响，所以我们可以人为地认为这些点“是什么”——更确切地说，我们认为（或者说定义） $\mathcal{H}$ 中的点是定义在 $\mathcal{X}$ 上的函数，在一定的条件下（详见 N. Aronszajn, Theory of Reproducing Kernels），我们可以找到对应于这个 Hilbert Space 的一个（唯一的）再生核函数（Reproducing Kernel） $K: \mathcal{X}\times\mathcal{X}\rightarrow \mathbb{R}$ （这里只考虑实函数），满足如下两条性质：</p>
<p>对于任意固定的 $x_0\in\mathcal{X}$ ，$K(x,x_0)$ 作为 $x$ 的函数属于我们的函数空间 $\mathcal{H}$ 。<br>对于任意 $x\in\mathcal{X}$ 和 $f(\cdot)\in\mathcal{H}$ ，我们有 $f(x) = \langle f(\cdot),K(\cdot,x)\rangle$ 。<br>其中第二条性质就叫做 reproducing property ，也是“再生核”名字的来源。至于字面上为什么这么叫，我也不清楚。也许是说元素 $x$ 经过 kernel 映射之后，由内积一乘，又给冒出来了 -.-bb 。有了这个 kernel 之后，我们可以很自然地把映射 $\phi$ 定义为：</p>
<p>$$<br>\phi(x) = K(\cdot,x)<br>$$</p>
<p>由核的再生性质，我们之前的用于计算 $\mathcal{H}$ 中内积的 kernel trick 也自然成立了：</p>
<p>$$<br>\langle \phi(x_1),\phi(x_2)\rangle = \langle K(\cdot,x_1),K(\cdot,x_2)\rangle = K(x_1,x_2)<br>$$</p>
<p>再生核有很多很好的性质，比如正定性（在线性代数里这样的性质通常称为“半正定”），也就是说对任意 $x_1,\ldots,x_n\in\mathcal{X}$ 和 $\xi_1,\ldots,\xi_n\in\mathbb{R}$ ，都有</p>
<p>$$<br>\sum_{i,j=1}^nK(x_i,x_j)\xi_i\xi_j \geq 0<br>$$</p>
<p>这是很好证明的，按照核函数的再生性质写成刚才的内积形式，然后把系数拿到内积里面去，上面那个式子就等于 $|\sum_{i=1}^n\xi_iK(\cdot,x_i)|^2$ ，根据范数的性质，也就非负了。</p>
<p>到这里，铺垫已经够多了，于是让我们回到 SVM ，这次我们不是直接偷工减料在最终得到的分类函数上做手脚，而是回到线性 SVM 的最初推导。当然，第一步我们要用刚才定义的映射 $\phi$ 将数据从原始空间 $\mathcal{X}$ 映射到 RKHS $\mathcal{H}$ 中，简单起见，我们用 $f_i(\cdot)$ 来表示 $K(\cdot,x_i)$。</p>
<p>和以前一样，我们使用一个线性超平面来分隔两类不同的点，并且我们假设经过非线性映射到 $\mathcal{H}$ 中之后数据已经是线性可分的了。这个线性超平面由一个线性函数来表示。这里需要再明确一下线性函数的概念，简单的说，如果 $x_1$、$x_2$ 是向量，$\alpha_1$、$\alpha_2$ 是标量，那么线性函数应该满足</p>
<p>$$<br>\mathcal{F}(\alpha_1 x_1+\alpha_2 x_2) = \alpha_1 \mathcal{F}(x_1) + \alpha_2 \mathcal{F}(x_2)<br>$$</p>
<p>在这里，由于我们讨论的空间 $\mathcal{H}$ 中的元素本身就是函数，因此我们把 $\mathcal{H}$ 上的函数改称“泛函 (functional)”。根据 Riesz Representation 定理，Hilbert Space 中的任意一个线性泛函 $\mathcal{F}$ ，都有一个 $f_\mathcal{F}\in\mathcal{H}$ ，使得</p>
<p>$$<br>\mathcal{F}(f) = \langle f,f_\mathcal{F}\rangle,\quad \forall f\in\mathcal{H}<br>$$</p>
<p>换句话说，线性函数可以由向量内积表示，这和我们熟知的有限维欧氏空间中是一样的。只是要表示超平面还得再加上一个截距 $b$</p>
<p>$$<br>\mathcal{F}(f) = \langle f,g\rangle + b<br>$$</p>
<p>这个样子的函数（泛函）严格来说称作仿射函数（泛函）。同我们在第一篇中类似，我们可以定义 margin ，得到 geometrical margin 为</p>
<p>$$<br>\gamma = \frac{y(\langle f,g\rangle +b)}{|g|}<br>$$</p>
<p>类似于原来的推导，我们最终会得到一个如下的目标函数</p>
<p>$$<br>\min \frac{1}{2}|g|^2\quad s.t., y_i(\langle f_i,g\rangle+b)\geq 1, i=1,\ldots,n<br>$$</p>
<p>形式上和以前一样，只是把 $x_i$ 换成了 $f_i$ ，$w$ 换成了 $g$ ，但是现在我们要求的参数 $g$ 是在 Hilbert Space 中，特别当 $\mathcal{H}$ 是无穷维的时候，是没有办法直接使用数值方法来求解的。即使可以转到 dual 优化推导，但是里面涉及到对无穷维向量的求导之类的问题，我还不知道是不是能直接推广。不过幸运的是，我们在这里可以再把问题转化到有限维空间中。</p>
<p>这需要借助一个叫做 Representer Theorem 的定理，该定理说明，上面这个目标函数（还包括很大一类其他的目标函数）的最优解 $g^* $ 可以写成如下的形式：</p>
<p>$$<br>g^* =\sum_{i=1}^n a_i f_i<br>$$</p>
<p>换句话说，可以由这 $n$ 个训练数据（有限集）张成。定理的证明是很简单的，记 $\mathcal{H}_0$ 为 ${f_1,\ldots,f_n}$ 张成的子空间，其正交补记为 $\mathcal{H}_0^\bot$ ，则任意的 $f\in\mathcal{H}$ 都可以唯一地表示成 $f=f_0+f_0^\bot$ ，其中 $f_0\in\mathcal{H}_0$、$f_0^\bot \in\mathcal{H}_0^\bot$ ，因此</p>
<p>$$<br>g^<em>  = g^</em> _0 + g^{* \bot}_0<br>$$</p>
<p>由于 $g^{* \bot}_0$ 垂直于 $f_1,\ldots,f_n$ ，因此</p>
<p>$$<br>\langle f_i,g^<em> \rangle = \langle f_i,g_0^</em>  + g_0^{<em> \bot}\rangle = \langle f_i,g_0^</em> \rangle + 0<br>$$</p>
<p>因此，$g_0^{* \bot}$ 部分的取值对于目标函数中的约束条件并不产生影响，可以任意定。另一方面，考虑目标函数本身，我们有</p>
<p>$$<br>|g^<em> |^2 = |g_0^</em>  + g_0^{<em> \bot}|^2 = |g_0^</em> |^2 + |g_0^{* \bot}|^2<br>$$</p>
<p>最后一个等式是由于两者相互垂直而得到的（也就是勾股定理的推广啦），得到这个形式之后，再注意到我们是希望最小化 $|g^<em> |^2$ ，其中 $g_0^{</em> \bot}$ 是可以任意取值的，而范数 $|g_0^{<em> \bot}|^2$ 又是非负的，所以在最小值的时候我们必定有 $|g_0^{</em> \bot}|^2=0$ ，从而 $g_0^{<em> \bot}=0$ ，也就证明了 $g^</em> \in\mathcal{H}_0$ 。</p>
<p>这样一来，问题就从在一个无穷维的 Hilbert Space 中找一个最优的 $g^<em> $ 转化为了在一个 $n$ 维的欧氏空间中找一个最优的系数 $\mathbf{a}^</em> $ ，又回到了我们熟悉的问题，目标函数也变成了下面的样子：</p>
<p>$$<br>\begin{aligned}<br>\frac{1}{2}|g|^2 &amp;= \frac{1}{2}\left|\sum_{i=1}^na_if<em>i\right|^2 \<br>&amp;= \frac{1}{2}\sum</em>{i,j=1}^na_ia_jK(x_i,x_j) \<br>&amp;= \frac{1}{2}\mathbf{a}^TK\mathbf{a}<br>\end{aligned}<br>$$</p>
<p>这里矩阵 $(K)_{ij}=K(x_i,x_j)$ 就是 Kernel Gram Matrix 。而约束条件也可以相应地写成</p>
<p>$$<br>\begin{aligned}<br>1 &amp;\leq y_i(\langle f_i,g\rangle + b) \<br>&amp;= y_i(\langle f<em>i,\sum</em>{j=1}^na_jf_j\rangle +b) \<br>&amp;= y_i(\mathbf{a}^TK_i + b)<br>\end{aligned}<br>$$</p>
<p>这里 $i=1,\ldots,n$ ，而 $K_i$ 表示矩阵 $K$ 的第 $i$ 列。所以回到了最初的线性 SVM 的 Quadratic Programming 问题。当然，形式上有一些差别，另外，原来的线性 SVM 的问题的维度是原始数据空间 $\mathcal{X}$ 的维度，而这里的问题维度是等于数据点的个数 $n$ ，这是 RKHS $\mathcal{H}$ 的一个子空间 $\mathcal{H}_0$ 。此外，原来的线性 SVM 不能处理 $\mathcal{X}$ 中的非线性问题，但是现在经过非线性映射之后，（理想情况下）数据应该变得线性可分了。当然，即使不能完全线性可分，我们也可以使用之前说过的 slack variable 的方法来放松约束。而问题的数值求解，也和以前类似，一方面可以直接使用二次优化的包求解，另一方面则可以通过 dual 优化的方式来完成——得到的结果应该跟我们之前偷懒直接在最终结果上把内积进行替换所得的结果是一样的。</p>
<p>最后稍微补充一下：在刚才的介绍中我们看起来好像是先确定了 RKHS 之后再找出对应的再生核的，但是在实际中，通常是先设计出一个核函数（或者说通常都是直接使用几个常见的核函数），然后对应的 RKHS 就自然地确定下来了。关于 RKHS 还有许多的内容，但是没有办法全部讲了。在传统的 Kernel 方法应用中，通常只要注意到是否可以全部表示为内积运算就可以尝试使用 kernel 方法了，许多常见的算法（例如 Least-Square Regression 、PCA 等）都是可以用核方法来扩展的，在这里 Representer Theorem 将会是重要的一环。</p>
<p>除此之外，进来还有不少在 RKHS 里衡量统计独立性的工作，又不是只是像传统的 kernel trick 那么简单了，说明 RKHS 还是包含了不少有趣的话题的。 :)</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/04/22/sciencism-and-hunmanism/" itemprop="url">
                  科学与人文
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Veröffentlicht am</span>
            <time itemprop="dateCreated" datetime="2016-04-22T13:43:11+08:00" content="2016-04-22">
              2016-04-22
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <blockquote>
<p>本篇文章摘自人民网刊载的吴国盛老师的文章,<a href="http://www.people.com.cn/GB/14738/27939/27984/27985/1895929.html" target="_blank" rel="external">查看原文</a></p>
</blockquote>
<p><strong>提要：</strong>希腊－西方的人文理想是“自由”，人文形式是“科学”和“理性”，所以科学一开始就是西方的人文，是自由的学问。近代发展出来的“唯人主义”（人道主义）人文传统可能背离“自由”这个古典的人文理想。近代科学的笛卡尔传统和培根传统分别强调了内在理性和外在经验，但最终共同受制于技术理性。近代科学与人文的分裂在于过份分科的教育体制，但分科化正是技术筹划的必然后果。“唯人主义”和“技术理性”信守共同的时代精神。今天弘扬科学精神，不必在科学与人文相区别的层面上突出科学的特异性，而应该在科学与人文合一的层面上，检讨我们时代的通病，重审自由和理性。</p>
<p><strong>作者简介：</strong>吴国盛，男，1964年9月生于湖北广济，北京大学哲学系教授、博士生导师。</p>
<p>这个题目来自当前面临的一个突出的文化困境以及摆脱这种困境的需要。人们将这个困境称做“科学”与“人文”“两种文化”[1]之间的分裂和日趋紧张的对立局面。但是，关于这个困境的种种述说以及摆脱这种困境的种种方案设计倒是带来了更多的困惑：所谓的“两种文化”究竟在什么意义上是分裂的？我们不是都承认科学是人类的一种文化现象吗？那它就该在某种意义上属于人文；我们不是也使用“人文科学”这样的术语吗？那就表明人文也是一种科学。还有，“两种文化”在什么意义上是可以沟通和弥合的，如果它们真的能够的话？</p>
<p>为了缓解科学与人文之间的紧张和矛盾，将科学与人文沟通起来，必须先把科学与人文之间的区别和联系说清楚。借着这个区别，我们理解科学与人文之间的对立和冲突之所在；借着这个联系，我们寻找沟通它们的可能性。</p>
<p>通过考察科学与人文的二分对立，我们进入对更基本问题的分析，即重审由技术理性和唯人主义结成的现代性。</p>
<h4 id="一、人文与人文精神"><a href="#一、人文与人文精神" class="headerlink" title="一、人文与人文精神"></a>一、人文与人文精神</h4><p>无论是西方还是中国，“人文”一词里都包含着两方面的意思：一是“人”，一是“文”。一是关于理想的“人”、理想的“人性”的观念，一是为了培养这种理想的人（性）所设置的学科和课程。前一方面的意思往往与“人性”（Humanity）等同，后一方面的意思往往与“人文学科”（Humanities）等同。值得注意的是，这两方面的意思总是结合在一起的，有着内在的关联：学科意义上的人文总是服务于理想人性意义上的人文，或相辅相成。“教养和文化、智慧和德性、理解力和批判力这些一般认同的理想人性，总是与语言的理解和运用、古老文化传统的认同、以及审美能力和理性反思能力的培养联系在一起，语言、文学、艺术、逻辑、历史、哲学总是被看成是人文学科的基本学科。”[2]</p>
<p>英文的Humanities直接来源于拉丁文Humanitas，而拉丁文Humanitas继承了希腊文paideia的意思，即对理想人性的培育、优雅艺术的教育和训练。公元2世纪罗马作家格利乌斯（Aulus Gellius）的一段话成了Humanitas的经典定义：</p>
<p>那些说拉丁语以及正确使用这种语言的人，并没有赋予Humanitas一词以一般以为具有的含义，即希腊人所谓的philanthropia，一种一视同仁待人的友爱精神和善意。但是，他们赋予humanitas以希腊文paideia的意思，也就是我们所说的“eruditionem institutionemque in bonas artes”，或者“美优之艺的教育与训练”（education and training in the liberal arts）。热切地渴望和追求这一切的人们，具有最高的人性。因为在所有动物中，只有人才追求这种知识，接受这种训练，因此，它被称作“Humanitas”或“Humanity”（人性）。[3]</p>
<p>按照希腊人的想法，理想的人、真正的人，就是自由的人。所以，整个西方的人文传统自始至终贯穿着“自由”的理念，一些与“人文”相关的词组就是由“自由”的词根组成的，比如“人文教育”（liberal education）、文科（liberal art）等。</p>
<p>汉语的“人文”一词同样有这两方面的意思。最早出现“人文”一词的《易经·贲》中说：“观乎天文以察时变，观乎人文以化成天下。”这里的人文就是教化的意思。中国的人文教化同样一方面是强调人之为人的内修，另一方面是强调礼乐仪文等文化形式。那么人之为人最重要的是什么呢？一般认为，以儒学为代表的中国思想把理想人性规定为“仁”，在孔子那里，仁者人也，人者仁也，两者互训互通。仁通过什么方式可以获得呢？克已复礼为仁！礼是实现仁的教化方式。</p>
<p>“人文”中“人”的方面和“文”的方面有可能得到不同的强调，其作为“文”的方面、文科课程的方面得到更多的强调的时候，人文被等同于人文学科和人文教育，特别是文史哲教育、文科教育。但是，无论是西方还是中国，作为人文的第一方面的“人”的理念向来是更重要的、更基本的方面。</p>
<p>正是为了强调这个更重要的方面，才出现了“人文精神”的说法。人文精神这个词是个地道的中文词，很难有对应的西文词，它与当代中国特定的语境相关。它既不是指人文教育（Humanities），也不是指西方的人文主义（Humanism），虽然与它们相关。我的理解，当人们使用“人文精神”这个词的时候，或多或少是在诉求一种人的理念，特别是“自由”这个西方人文的核心理念。90年代的人文精神大讨论，实际上针对的是市场经济大潮下实利主义的泛滥、理想的泯灭而开展的，这里要求弘扬的并不单纯是文科教育，而是对人之为人的重新反思，其批判的矛头所指往往更多的是人文学者和文学艺术家，所抨击的往往是中国的人文学界和人文领域里人文精神的失落。所以，人文精神应该看成是一种建基于对人之为人的哲学反思之上的批判态度和批判精神。简而言之，人文精神就是一种自由的精神。</p>
<h4 id="二、科学作为希腊－欧洲人的人文理想：自由的学问"><a href="#二、科学作为希腊－欧洲人的人文理想：自由的学问" class="headerlink" title="二、科学作为希腊－欧洲人的人文理想：自由的学问"></a>二、科学作为希腊－欧洲人的人文理想：自由的学问</h4><p>人文是个本地词，而科学却是个外来词。毫无疑问中国人有自己的人文，但要说有自己的科学就不是那么容易，需要费半天口舌才行。</p>
<p>当代汉语的“科学”一词译自英文或法文的science。起初一直译为“格致”，后来受日本影响译为“科学”。1897年，康有为在其《日本书目志》中引进了这个词。日本人用这个词表示西方分科的学问与中国不分科的儒学相对应，这个理解被20世纪初年的中国知识界所接受。1915年，留美学生创办的科学刊物取名为《科学》，并产生了广泛的影响，从此，“科学”一词成了science的定译。[4]</p>
<p>英文的science一词基本上指natural science（自然科学），但science来自拉丁文scientia，而后者涵义更广泛，是一般意义上的“知识”。德文的wissenschaft（科学）与拉丁文的scientia类似，涵义较广，不仅指自然科学，也包括社会科学，以及人文学科。我们知道德国人喜欢在非常广泛的意义上使用“科学”这个词，比如黑格尔讲哲学科学、狄尔泰讲精神科学、李凯尔特讲文化科学等。这些词的历史性关联暗示了一个更深层更广泛的思想传统，狭义的自然“科学”只有在这个深广的思想传统之下才有可能出现和发展。</p>
<p>今天我们讲科学，首先当然是指近代科学，而近代科学首先又是指近代自然科学。但是我们必须注意到，近代科学并不是凭空生长出来的，而且诞生之后又处在发展之中。因此值得追问的是，它何以能够由自然科学向社会科学、人文科学扩展？它又是如何植根于希腊和中世纪的“学问”和“知识”传统的？这两个问题实际上有着内在的联系，那就是，近代科学的母体不仅孕育了近代科学，而且也保证了近代科学能够由自然领域向社会和人文领域延伸，这个母体就是希腊人所开创的“求知”的精神、“理论”的理性、“对象化－主体性”的思想方式，一句话，是科学（哲学）的传统。</p>
<p>这里所谓的科学传统就不是特别针对近代科学而言，而是用来刻划希腊－欧洲人的一般存在方式的。这个传统，就是海德格尔所谓的“哲学－形而上学”传统，也是胡塞尔要着力弘扬和重建的理性传统。哲学和科学在希腊时代是合二为一的，就是到了今天，西方哲学依然是广义西方科学的某种特定形态（胡塞尔的理想是把哲学建设成最严格的科学）。黑格尔之所以能够称哲学为哲学科学，是因为西方哲学本来就属于西方的科学传统。为了理解这个传统，我们需要从希腊－欧洲人的人文理想谈起。</p>
<p>前面我们已经指出，希腊－欧洲人的人文理想是“自由”，自由被他们看成是人之所以为人的根本。我们从希腊的哲学和文学戏剧华章中，到处可以见到对“自由”理想的赞颂和追求之情。欧里庇得斯说：“所谓奴隶，就是一个不能发表自己思想观点的人。”自由的人是能够发表“自己”的思想观点的人，如何才能发表“自己”的观点呢？希腊哲学家发现，只有理性才能够保证达成这样的“自由”。亚里士多德说：“我们应该尽一切可能，使自己升华到永生的境界，使自己无愧于我们身上所存在的最优秀的品质而生活。……对于人来说，这就是以理性为根据的生活，因为它才使人成为人。”自由的人是理性的人，而“理性”就体现在“科学”之中。</p>
<p>如果说，中国的儒家的“人－文”是由“仁－礼”构成的，那么古典希腊人与之相对应的“人－文”在我看来就是“自由－科学”。也就是说，对古典希腊人而言，能够保证人成为人的那些优雅之艺是“科学”，而对“自由”的追求是希腊伟大的科学理性传统的真正秘密之所在。[5]</p>
<p>希腊哲学是希腊科学传统的第一个样本，它其中的自然哲学正是近代自然科学的直接先驱。[6]希腊的哲学（philosophia）是爱(philo)智(sophia)的意思，爱智又意味着什么呢？爱智不是一般的学习知识，而是摆脱实际的需要、探求那种非功利的“超越”的知识，一句话，“爱智”就是与世界建立一种“自由”的关系。亚里士多德的《形而上学》中有大量关于科学作为一种自由的探求的论述。他提到“既不提供快乐、也不以满足必需为目的的科学”（981b25），提到“为知识自身而求取知识”(982b1)，提到“为了知而追求知识，并不以某种实用为目的”（982b22），最后他说：“显然，我们追求它并不是为了其他效用，正如我们把一个为自己、并不为他人而存在的人称为自由人一样，在各种科学中唯有这种科学才是自由的，只有它才仅是为了自身而存在。”（982b26－28）[7]</p>
<p>这里所说的当然是哲学，亚里士多德也把它看成是一切科学（知识）中最高级的，是最理想的科学形态。这种科学理想，不只在亚里士多德那里能够找到，在他以前的柏拉图、苏格拉底那里同样能够找到。这种科学理想，既体现在亚里士多德开创的第一哲学（形而上学）那里，也体现在希腊人特有的科学――数学那里。在《理想国》里，柏拉图借苏格拉底之口特别强调了数学的非功利性、它的纯粹性、它对于追求真理的必要性，因为算术和几何的学习不是为做买卖，而是“迫使灵魂使用纯粹理性通向真理本身”（526B），这门科学的真正目的是纯粹为了知识。希腊人开辟了演绎和推理的数学传统，这首先是由于他们把数学这门科学看成是培养“自由民”所必须的“自由”的学问，自由的学问是纯粹的学问，不受实利所制约，而演绎科学正好符合这一“自由”的原则。</p>
<p>哪些科目被古典希腊人认定为人文教育的必修科目呢？苏格拉底以来的雅典教育四大学科：算术、几何、音乐（和声学）、天文，均是广义的数学学科。对柏拉图而言，还有更高级的学科是辩证法（不只是辩论术，主要是善的科学）。此前，智者学派曾把“辩论术”做为一门重要的教育课程。</p>
<p>罗马上流社会只关心军事和政治，只关心有实用目的的知识。罗马政治家老加图（Cator Elder,前234－前149）在论儿童教育时，只提到了讲演、医学、农业、军事、法律等实用技术，而对希腊式的纯科学教育持反对态度，很类似斯巴达人。此后，罗马人逐渐把文法和修辞做为高等教育的基本学科。与老加图略晚的罗马人法罗（Varro, 前116－前27）在其《教育九卷》（Disciplinarum Libri novem）中讨论了文法、修辞、辩证法、几何、算术、天文、音乐、医学和建筑九大学科。从公元四世纪起，前七门学科被称作“七艺”，成了欧洲高等教育的标准课程。[8]七艺中的四艺是数学学科，其中的辩证法则越来越多的指逻辑。所以七艺中的五艺应属科学学科。</p>
<p>中世纪希腊理性精神的弘扬特别体现在经院哲学上。原始的基督教因信称义，强调信仰淡泊知识。12世纪之后，亚里士多德的著作开始重新流行起来，对逻辑和推理的崇尚逐渐改变了基督教神学的形态，出现了极为偏重推理和逻辑的经院哲学，我们应该恰当地把它称为一种科学形态的神学：它是以科学理性的方式为教义辩护，而不是单纯强调信仰。作为对比我们可以注意到，希腊的科学理性同样影响了阿拉伯文化，但却没有在伊斯兰教中产生类似的成熟的“经院哲学”，虽然12世纪的确有杰出的回教哲学家阿维罗伊（1126－1198）曾为此做过努力。当时的哈里发发表了一道有象征意味的布告说：上帝已命令为那些妄想单凭理性就能导致真理的人备好地狱的烈火。[9]</p>
<p>怀特海在追溯近代科学的起源时说：“在现代科学理论还没有发展以前人们就相信科学可能成立的信念，是不知不觉地从中世纪神学中导引出来的。”[10]因为经院哲学的逻辑把严格确定的思想习惯深深地种在欧洲人的心里，这种习惯即使在经院哲学被否定以后仍然流传下来，就是伽利略，“他那条理清晰和分析入微的头脑便是从亚里士多德那里学来的。”[11]</p>
<h4 id="三、近代西方的人文传统：人文主义与人道主义"><a href="#三、近代西方的人文传统：人文主义与人道主义" class="headerlink" title="三、近代西方的人文传统：人文主义与人道主义"></a>三、近代西方的人文传统：人文主义与人道主义</h4><p>今天我们使用“人文”一词更多的与“近代”西方特有的人文传统相关联，这就是在欧洲的文艺复兴中出现并在日后发展起来的Humanism（德文Humanismus）传统。这个词有两个相互联系但又有区别的所指，中文分别译成“人文主义”和“人道主义”。</p>
<p>Humanism这个词虽然直到19世纪早期才出现[12]，但主要用来概括文艺复兴时期的“人文主义者”（humanista）的一般思想倾向，因而直接来源于它。15世纪后期使用的Humanista（人文主义者）一词又来源于更古老的“人文学习与课程”（studia humanitatis）。[13]中世纪后期，随着大翻译运动出现了不少远远超出传统七艺的学科，如神学、罗马法和教会法、医学、天文学与占星术、形而上学和自然哲学等。早期的人文主义者强调通才教育，尽管他们的“人文学习与课程”着力于语法、修辞、诗歌、历史和道德哲学五科（实际上就是我们今天习称的文史哲），有些人文主义者可能还对五科之外的学问持抵制态度（如彼特拉克），但总的来说人文主义者带来了新的知识（通过翻译希腊和拉丁古典）和新的眼光（人文主义），促进了新兴学问与传统学问之间的融合。[14]我们完全可以说，人文主义者深化了得自希腊而被中世纪马虎对待的传统四艺（算术、几何、音乐、天文）。除了强调通才教育外，人文主义者认为人文教育的目的在于培养美德和教育青年热爱生活，所以把伦理学（道德哲学）放在最重要的位置，其他各科都服务于美德的增进。人的修养、人的自我培育、自我丰富和全面发展，是人文主义的思想核心。</p>
<p>鉴于Humanism的文艺复兴来源以及对人的全面发展、全面教养的强调，汉语将之译成“人文主义”，与中文的“人文教化”相衔接。</p>
<p>人文主义运动至少有两个后果。第一是确立了既有别于传统的神学又有别于新兴的自然哲学（自然科学）的学科体系，导致了今天人们所说的人文学科（Humanities）；第二，铸成了一个新的信念体系，即认为人本身是最高的价值，是一切事物的价值尺度，把人确立为价值原点。</p>
<p>文艺复兴时期的人文主义者，复兴的是一种与宗教神学不同的世俗的知识（希腊、拉丁学术），但同时包含着对“人”之地位的一种重新的审视和思考。米兰多拉（Pico della Mirandola, 1463－1494）在《论人的高贵的演说》（Oration on the Dignity of Man）中借神圣的创造者的嘴对人类说：“其他一切生物受制于我们为它们立的法，但是，你不受任何约束，你可以随心所欲地处置它们。我们已经把你置于世界的中心，因此从那里你可以轻而易举地环视其中的一切。”[15]这里，人开始被置于世界的中心。莎士比亚在《哈姆雷特》中进一步道出了人文主义的思想主题：“人是一件多么了不起的杰作！多么高贵的理性！多么伟大的力量！多么优美的仪表！多么文雅的举动！在行为上多么象一个天使！在智慧上多么象一个天神！宇宙的精华！万物的灵长！”[16]当然，把人确立为世界的中心的思想也有其希腊来源。智者普罗泰哥拉曾有句名言：“人是万物的尺度，是存在者如何存在的尺度，也是不存在者如何不存在的尺度”。但是总的看来，人的地位问题在希腊时代还没有以这种方式、摆到这样的高度来对待，因为从根本上说，以苏格拉底－柏拉图－亚里士多德为代表的希腊古典思想是与智者的人类中心主义相对立的。苏格拉底强调，只有神而不是人才是万物的尺度，因为只有神才是最完美的，而任何不完美的东西都不能成为万物的尺度。“善     而非“人”才是他们关注的中心问题。</p>
<p>原初复兴古典文化意义上的“人文主义”似乎不足以表达这个意思，于是Humanism又被译成“人道主义”。但这个译法也有问题，因为中文的人道主义已经有了一个比较流行的解释，即主张对一切人都要仁慈都要讲爱，因此也被称为博爱的人道主义，实际上是Humanitarianism的中译（来源于希腊文的philanthropia）。所以作为一种思想体系的Humanism译成“人道主义”极易招误解，极易看成一个道德规范，而忽视它的形而上学意义。有人译成“人类中心主义”[17]，这是对Humanism的学理上的解释，是把Humanism解释成anthropocentrism，但它字面上并没有“中心”的意思。最准确的似应译成“人的主义”，但汉语不太顺。王若水先生仿照唯物主义（materialism）的译法译成“唯人主义”[18]，是很不错的译法，但可惜没有流传开来。本文有时从俗译成人道主义，但更多的采用“唯人主义”的译法。</p>
<p>这样，Humanism一词就有三种意思：复兴古典学术和强调人的全面教养的人文主义；作为仁慈博爱伦理的人道主义；作为以人为价值中心价值原点最高价值的唯人主义（人道主义）。</p>
<p>值得注意的是唯人主义与欧洲人文理想的关系：唯人主义符合从而实现了欧洲自由人的理想吗？这是一个极富有挑战的问题。在当代中国特定的语境中，“人”的问题就如同“科学”的问题一样，面临着一个尴尬的局面。作为一个正在渴望现代化的弱势民族，中国人渴望弘扬唯“人”的精神和唯“科学”的精神，因为“唯人主义”和“唯科学主义”恰恰就是“现代性”的基本组成部分。我们还清楚的记得80年代唯人主义和唯科学主义是如何作为当时思想启蒙运动的主旋律，而且这场启蒙运动到了今天依然在许多方面保有它的意义。但是，我们对唯人主义和科学主义的检讨，与启蒙精神本身拥有相同的思想来源从而享有同样的正当性，那是因为在这一切的背后是自由的精神。用海德格尔的话说，启蒙是开启一个明亮的场地，是去蔽，但任何去蔽都同时带来新的遮蔽，对启蒙的批判就是去掉这新的遮蔽，同样是去蔽。</p>
<p>人是一种自由的存在者，意思是说它“让”一切存在者成其所是。当然，首先，人们把这种自由理解成“让”人这种存在者成其所是，即人是人自身的创造者。它强调人的本质是由人自己塑造、培育和发展的，强调这种自我塑造的无限可能性。从这个意义上讲，“自由”的理念引导了近代唯人主义的人文传统，因为这个传统抓住了“人的自我创造”这个主题。但是，人的自我塑造、人的本质化即获得其规定性的过程，本身也是人对自由的背离的过程：人的本质化是对人的无本质即无固定的规定性的背离。[19]人的本质化，必损害人与世界之间的自由的关系，因为这种自由的关系只有在人持守他的无本质时才有可能。</p>
<p>人与世界的关系根本上是一种自由的关系。自由的人既不是世界的创造者，也不是世界的利用者和消费者，而是一个听之任之的“看护者”和欣赏者，所以也有人把这种自由的关系说成是审美的关系。但是，近代唯人主义在将人本质化的过程中破坏了人与世界之间这种自由的关系。唯人主义首先把世界置于一个以人为原点的坐标系之中，把一切存在者都置于以人为阿基米德点的价值天平中，从而最终把世界变成利用和消费的对象。世界的对象化的结果是人同时被对象化即本质化，人与世界的关系成了一种既定的、给定的、固定的关系。</p>
<p>唯人主义把人置于某种中心的位置，按照人的要求来安排世界，表面上看是最大程度的实现了人的自由。但事实上，人与世界之自由关系的损害最终必然反过来损害人对自身的自由发展，因为自我创造的过程总是受制于人与世界的关系。消费和利用的关系一旦成型也就是本质化，无论以理性的名义还是以科学技术的名义来规定这种本质，人都会沦落为一个被动的角色，他只须按照所谓理性或科学的方式去反应。他在近代极度“自由地”展开的世界几乎遮蔽了所有其余的可能性。这是当代思想家纷纷质疑“现代性”的根本原因：唯人主义可能从根本上损害了自由。</p>
<h4 id="四、近代西方的科学概念：笛卡尔形象与培根形象"><a href="#四、近代西方的科学概念：笛卡尔形象与培根形象" class="headerlink" title="四、近代西方的科学概念：笛卡尔形象与培根形象"></a>四、近代西方的科学概念：笛卡尔形象与培根形象</h4><p>我们在第二节指出广义的科学指的是希腊文明传给欧洲的一份精神遗产，即把理性和知识作为人的基本存在方式，也是习得理想人性的基本方式。这种意义上的科学是服务于自由理想的。但是，我们今天使用“科学”一词更多指的是近代科学所造就的科学概念。所以我们还要搞清楚，近代科学在何种意义上继续保持其为“科学”，同时，它作为“近代”科学有哪些新的特征。</p>
<p>近代科学有两种形象：笛卡尔形象和培根形象。它们分别代表了古典的理性传统和新兴的功利传统，笛卡尔的“我思故我在”和培根的“知识就是力量”分别是这两种传统的宣言。以他们的名字来命名这两种科学的形象是恰当的。</p>
<p>近代科学是希腊科学传统的直接继承者，这种继承性体现在近代科学的理性形式和自由精神上。笛卡尔的“我思”突现的是那种理性奠基的精神，而作为近代科学之肇始的自然数学化运动应该正确的看成是理性奠基运动，即把作为自然科学之对象的自然界彻底理性化的运动。与16、17世纪自然的数学化同时的其他重要的理性化工作还有：培根归纳法和笛卡尔演绎法等方法论问题的提出以及被科学家群体的消化，从此科学以其方法论而区别于其他知识形式；科学社团和研究机构的建立和有序化以及研究范围的分门别类，从此专业化、分科化成了科学事业扩张的基本线索。</p>
<p>随着自然的数学化、研究的方法论化、科学建制的分科分层化，近代科学完成了其理性化过程，并构成日后科学发展的基本精神气质。著名科学社会学家默顿在其经典之作《科学社会学》（The Sociology of Science）中将之概括为四个：普遍性（Universalism）、公有性（Communism）、无私利性（Disinterestedness）和有条理的怀疑主义（Organized Scepticism）。这四条精神气质（ethos）是对希腊所倡导的科学理性精神的一个很好的注解：有条理的怀疑主义是自由的怀疑和批判精神；公有性是自由发表和自由的探索精神；无私利性是不计利害只求真理的精神；普遍性也就是普遍理性精神，把理性的能力和可能性做了最大限度的扩展，发挥到了希腊人远未达到的地步。</p>
<p>近代科学不光是希腊理性精神的正宗传人和光大者，作为现代工业社会的奠基者，科学还以其“效用”服务于意欲“控制”的人类权力意志（will to power）。这是近代科学的一个暂新的维度：力量化、控制化、预测化。美国著名科学史家科恩说：“新科学的一个革命性的特点是增加了一个实用的目的，即通过科学改善当时的日常生活。寻求科学真理的一个真正目的必然对人类的物质生活条件起作用。这种信念在16世纪和17世纪一直在发展，以后越来越强烈而广泛地传播，构成了新科学本身及其特点。”[20]弗兰西斯·培根是这一科学形象的代言人，他强烈的主张科学应该增进人类的物质福利，否则就是些空洞的论证和言词游戏。他因此批评希腊人的科学大部分只是些“无聊老人对无知青年的谈话”，“他们真是具有孩子的特征，敏于喋喋多言，不能有所制作；因为他们的智慧是丰足于文字而贫瘠于动作。这样看来，从现行哲学的源头和产地看到的一些迹象是并不好的。”[21]培根倡导经验论、归纳法，尽管并未为同时代的科学家们所运用。但他在《新大西岛》中设想的科学研究机构所罗门宫，成了后来英国皇家学会的建设兰图。</p>
<p>意欲对自然有所图谋的培根纲领之重视外在经验是顺理成章的，但这里的经验也应该是完全新型的经验，它是对自然有所行动之后看看自然会有什么样的反应，而不是被动的观看。在这一点上，培根本人提出的比较被动和静止观照的经验论并不能真正实现他的纲领。近代科学延着他的思路发展出来的经验论是操作主义的实验经验论。可严格控制可重复操作的实验，是近代科学理论得以发展的外部制约条件和启发性因素。</p>
<p>笛卡尔形象和培根形象有时也被概括成数学传统和实验传统[22]。不同的历史时期，科学发展的主流可以侧重不同的传统。但是总的看来，近代科学的形象是由这两种科学形象合成而来的。经验加理性、实验加数学，通常就被认为是近代科学的两大要素。值得注意的是，这两种科学形象之间并非没有矛盾和冲突，相反，也许正是它们之间的对立构成了近代科学发展的一种基本的张力。它们相互制约，维持微妙的平衡。</p>
<p>但是希腊的基因总在顽强的发挥作用。我们确实可以隐约从近代科学史中看出一条理性论的主线，这些主线上的科学巨匠们总是更多的偏爱数学理性的内在力量。他们中杰出的一位，爱因斯坦，虽然也同时强调了这两大要素，称它们是“内在的完备”和“外部的证实”[23]，但在他的内心，科学的基础是理性而不是经验数据，科学本质上是“人类理智的自由发明”[24]。据说当爱丁顿的日全食考察队证实了他的广义相对论的预言时，他不动声色的说：“我知道这个理论是正确的”，当一位学生问他假如他的预言没有得到证实该怎么办，他回答说：“那么我只好向亲爱的上帝道歉了――那个理论还是正确的。”[25]</p>
<p>这个故事显示了笛卡尔科学理想的顽固性。如果说“外部的证实”应该屈从于“内在的完备”，那么在一颗古典的科学心灵看来，科学真理本质也应该是超越功利的。但是这样的古典理想并不总是能够得到实现，特别在今天，由于越来越深地卷入工业和军事政治，科学家们不得不屈从于商业秘密和军事秘密的要求，而破坏自由探索和自由发表的公有原则；由于耗资越来越大，他们也不得不越来越取悦于拨款人的功利好恶，而破坏无私利性原则。当代生命科学和生物技术在这方面表现得尤其突出。</p>
<p>在笛卡尔形象与培根形象之间非常明显的冲突，往往使人产生如下的疑问：近代科学究竟是怎样协调这两大传统的？它们是如何并行不悖的引导近代科学的发展的？我的看法是，在某种更深的意义上，两大传统事实上合流了。它们共同的受着一种新的理性形式――我愿称之为“技术理性”――的支配和控制，而这种新的技术理性与希腊的理性已不可同日而语，尽管前者确实来源于后者，但已经渐行渐远。</p>
<p>技术理性来源于人的“权力意志”，是对希腊理性的一种无限扩张。服务于“力量”（power）的要求，允诺“无限”(infinite)的可能性，是技术理性的两大要素。希腊的理性服务于“善”的要求，而且只允诺有限的可能性。人因为有理性而趋向善、热爱神，人在这种追求善的过程中领悟到自己的有限性。在希腊理性中不包含“控制”和“统治”的内在要求，恰如其份的理解人与上帝、人与世界的关系，被认为是真正的理性行为，所以，“理解”(comprehension)而非“力量”(power)是希腊理性的要义。值得指出的是，许多近代科学的创建者们并没有一开始就接受培根的“力量”纲领，相反还是坚持古老的理想，即把科学的主要目的看成是理解人类的处境，特别是为理解人与上帝的关系服务。例如牛顿，他多次表白，他从事科学研究的目的在于教人相信上帝的存在。</p>
<p>由于服务于力量的控制和运用，近代科学必然要求预测的有效性，并把它作为一个根本的边界条件。培根本人也曾说过：“欲征服自然，必先服从自然”[26]服从自然现在被认为是服从自然的规律，而自然的规律不是别的，也就是自然界的可预测性。</p>
<p>对自然可预测性的要求最终是通过自然的数学化来实现的。希腊的演绎科学――数学是可预测性的典范，有着勿庸置疑的可靠性，但那时数学被认为是通往善的一个必经阶梯。[27]自然的数学化以及近代数学本身的迅速发展，事实上均来源于对有效预测的要求。这种要求使得近代科学创造了一个纯粹“量”的世界，发展出了一套“计算”的方法论。因此，尽管近代科学和希腊科学都使用数学，但数学对他们而言已经不具相同的意义了。近代数学已经受雇于预测和控制的要求，服务于“力量”的意志。</p>
<p>不仅如此，近代以数学化为核心的科学理性还因其对“无限”性的允诺，而区别于希腊科学。希腊数学基本上限定在有限性的范围之内，对无限“敬而远之”。欧几里德的《几何原本》并未给出一个均匀平直无限的三维欧氏空间：这个空间恰恰是19世纪才被最后规定出来的。[28]近代的无限性首先发端于哥白尼革命，起始于一个谨小慎微的天文学改革，结果却导致了“从封闭世界到无限宇宙”[29]的革命性变革。与宇宙论的无限化相伴随的是无限数学的出现。牛顿微积分的发明是一个极具象征性的事件，它是无限数学第一次服务于近代科学，并帮助安排了一个无限的宇宙模型。</p>
<p>近代无限理性最终表现在对理性之无限“力量”的肯定。技术理性使人们相信，科学技术可以解决一切问题，因为科学技术具有无限发展的可能性：如果问题还没有得到解决，那是科技还不够发达；如果出现了不良的结局和负面的影响，那消除这种结局和影响也还是得靠科技的进一步发展。科学发展的无限能力的信念首先表现为科学家的“无禁区”的自由探索。由于近代科学事实上的深刻的“功利化”“权力化”，任何超越功利的不计后果的“无禁区”探索，都可能事实上造成恶劣的后果。核物理学发展初期，匈牙利物理学家西拉德因为忧虑核能量会被纳粹所掌控，曾建议各国的核物理学家暂缓发表他们的研究成果。这个所谓的“自我出版检查制度”是空前的，“几个世纪以来，科学家们都是为自由交流思想而斗争，所以任何时候也不应该持有与此相反的原则。他们自己就是极端自由的忠实信徒，并且是军国主义的不妥协的敌人。但是，现在他们感觉到，国际舞台的形势是十分复杂的。”从西拉德的信中可以看出，“当时科学家们对科学进步所寄托的希望，竟然由于可能产生可怕的后果而变得害怕科学向前发展了。写信的人好象竟希望实验失败。”[30]这个案例充分显示了，由于科学成为一种力量（权力、能量）的象征，自由探索的精神遭受了怎样深刻的挑战，科学家们面临着一种怎样的二难处境。</p>
<h4 id="五、近代科学与人文的双重关系：分裂与合流"><a href="#五、近代科学与人文的双重关系：分裂与合流" class="headerlink" title="五、近代科学与人文的双重关系：分裂与合流"></a>五、近代科学与人文的双重关系：分裂与合流</h4><p>在回顾了近代西方的人文传统和科学传统之后，我们可以发现，近代科学与人文事实上存在着双重的关系：一方面随着专业化和学科分化愈演愈烈，人文学科的阵地激剧萎缩，在教育体制上人文教育与科学教育互相隔绝；另一方面，以唯人主义为标志的近代人文传统和以技术理性为标志的近代科学传统事实上紧密的结合在一起，共同构成“现代性”的基础。正是现代性所要求的专业分工和力量意志，导致了科学（自然科学和社会科学）与人文学科的分裂，以及人文学科的严重危机。</p>
<p>科学与人文的分裂表现在相互联系的四个方面：</p>
<p>第一，自然科学和技术愈演愈烈的学科分化和扩张，使人文学科的领地日见狭窄。我们首先需要明确的是，科学与人文的分裂在近代并不是两个旗鼓相当的阵营之间的分裂，而是作为传统知识主体的人文学科日渐缩小成一个小的学问分支。文理科的发展极度不对称，理工农医科的规模越来越大，而人文学科越来越小。不仅在学科规模方面人文地位越来越低，而且在教育思想方面，科学教育、专业教育、技术教育压倒了人文教育。</p>
<p>第二，学问普遍的科学化倾向和功利化，导致了社会科学的兴起，也使人文学科的地位进一步下降。近代以来，运用自然科学的方法来解决社会问题的学科即社会科学日渐兴起，它们进一步挤占了传统人文学科的地盘。人文学科甚至到了只有栖身在社会科学这个牌子才有生存机会的地步。社会“科学”的概念取代了“人文”的概念，“功利”的概念取代了“理想”的概念。在一个科学化的时代，为了争得在学术殿堂中的位置，人文学界也出现了“人文科学”的说法。这个词组的用意并不是想阐明科学本质上就是人文――就象我们在第一二节所阐明的那样――而是说，人文也是一种象近代科学那样的“力量型”的学问，借以在科学时代合法地谋得一席之地。</p>
<p>第三，重视培养专业人才的教育体制，人为的造成了科学与人文之间的疏远和隔绝。对于当代中国人来说，谈论科学与人文的分裂时心中想的，正是中国现行教育体制中严重的文理分科现象。这种分科现象在西方各国的教育史上或多或少的都存在，但都没有象当代中国这样突出。分科化、专门化和专家化与教育理念有关，凡重专才教育，则专科化倾向较严重；凡重通才教育者，则专科化倾向就比较淡化。在专才教育体制下成长起来的理工科学生缺乏基本的人文素养，对于社会进步和发展难以有一个宽阔的视野和深谋远虑的计划。正象许多教育家所指出的，如果高等教育培养出来的学生只会用自己学科的内部标准去判断事物，那就不是真正的高等教育。真正的高等教育应该培养学生既掌握内部标准，也能够用其他学科的原理和方法即“外部标准”来评判自己的学科，能够看出自己学科的优点和局限。[31]</p>
<p>第四，自然科学自许的道德中立，使得科学家们心安理得的拒绝人文关怀。与之相关的是近代哲学对事实与价值的二分，这种二分将科学置于澄清事实的范围，而不涉及价值问题。皇家学会的干事长胡克为学会草拟的章程时写道：“皇家学会的职责是：通过实验改进自然事物的知识，以及所有有用的技艺、制造业、实用机械、工程和发明的知识，同时不干预神学、形而上学、道德、政治、文法、修辞学或逻辑。”[32]科学家们也许并不反对博爱善行的人道主义，而且更深的认同唯人主义的“力量原则”，但当这种力量原则与特定的历史文化经验发生冲突时，他们有可能毫无犹豫的牺牲后者。科学与人文的分裂体现在科学对人文传统的轻视，特别是当这种人文传统不合技术理性的逻辑时。</p>
<p>科学与人文的分裂根源于知识体系的分科化、专业化，而知识的专科化又源于什么？这个问题把我们引向近代科学与人文合流的方面：技术理性与唯人主义的合流。很明显，知识的专科化来自技术理性，来自那种诉求“效率”和“力量”的科学的本质。所以，科学与人文的分裂是技术筹划的必然结果。力量型科学要求一种分工型的科学和教育体制，只有理解型的科学才要求一种综合的领悟力。今天对科学与人文之分裂的反省，最终应该导向对力量型本身的反省。</p>
<p>技术理性就在这个意义上与唯人主义合流。它们都是相信“力量”（power）的乐观主义。在唯人主义看来，一切问题都是可以解决的，因为人有理性这种无限的能力。为了解决人所面临的一切问题，技术理性广泛地行使它的威力。唯人主义得到弘扬的地方，也就是技术理性大展宏图的地方，因为它们相互确认。这种相互确认并不简单是人作为目的，技术作为手段，相反，手段和目的在这里混成一体，因为技术理性正是人之所以能把自己确立为价值中心的唯一根据和保证。许多技术批判主义者批评技术的发展违背了人性的目标，是对唯人主义的背离。其实，“技术违背了人性”，却不见得是对唯人主义的背离，因为正是唯人主义本身推动了技术的这种违背人性的倾向。唯人主义陷入一个自我拆台的怪圈之中。</p>
<p>这里的逻辑并不难理解。唯人主义因着技术理性而自命不凡，而把自己确立为价值原点和世界的中心，而觉得自己无所不能，可以对自然界为所欲为。这里，对人的自我崇拜就自然而然的转化为对技术的崇拜。如果我们要求自然屈从于技术，那么我们也在要求作为自然一部分的我们自己屈从于技术；如果我们认为技术产品优于自然的产品，那么同样，我们的创造物就会被认为优于作为自然产品的我们自身。由于我们人类注定是自然的一部分，因此唯人主义注定要遭受技术的异化：本来是用以确立人之地位的，最终却被用来贬低人类自己。技术发达了，人类却丧失了劳动的乐趣，甚至劳动的权利（所谓技术失业）；科学发展了，人类却越来越不知道生命和存在的意义。这是唯人主义深刻的困境。</p>
<h4 id="六、弘扬科学精神：两种思路"><a href="#六、弘扬科学精神：两种思路" class="headerlink" title="六、弘扬科学精神：两种思路"></a>六、弘扬科学精神：两种思路</h4><p>今天到处都能听到弘扬科学精神的呼声。但这个有待弘扬的东西究竟指的是什么？众说纷纭，莫衷一是。问题在于，为什么会有弘扬科学精神的要求，其背后的动机是什么。不同的动机和要求，想弘扬的东西就不会是一样的。</p>
<p>总的来看有两种思路。一个广泛持有的思路是，在科学与人文相区别的意义上，特别的张扬科学的优越性。这种思路分两部分：第一部分，基本上把科学精神等同于科学方法，因为科学之区别于非科学、科学之特别的有效用，就在于科学方法；第二部分，主张把在科学研究领域特别有效用的科学方法，不仅不折不扣的运用在实际的科学研究中，而且要运用到更广泛的日常社会生活领域中去。我把这样的主张定义成科学主义：主张在科学领域行之有效的科学方法可以而且应当在非科学领域普遍使用[33]。科学主义自然有强有弱，但科学方法的超（科学）范围运用是它的基本主张。按照这种定义，弘扬科学精神的第一种思路就可以概括成：科学精神就是科学方法，弘扬科学精神就是弘扬科学主义。</p>
<p>在这种思路下人们对“什么是科学精神”的回答，基本上都是关于“什么是科学方法”的总结概括。由于都是从各人的私人经验出发，得出的结论各各不同。但问题在于，有没有一个普遍适应的科学方法。“历史学派的科学哲学家都承认，像逻辑主义那样建立一个严格的超历史的形式方法论以符合科学发展实际上是不可能的，任何方法都是具体的、历史的。”[34]因此，一般的谈论科学方法可能是舍本逐木。对一个想尽快进入研究前沿尽快出产研究成果的科学研究者而言，多多浸淫于科学共同体的研究氛围之中，比到处打听科学方法论更有益处。熟能生巧，习惯成自然，科学方法的习得是一个实践问题而非理论问题。</p>
<p>对于一个非科学家，或者对于一个正处在日常生活中的科学家来说，有没有必要把科学的方法也同时运用起来呢？弘扬科学主义的人所希望的正是这个。“让科学成为我们的生活方式”，这句广告词彰显的就是这种科学主义的理想。然而，科学主义的实施是有条件的、有限度的，无条件的强的科学主义是不可取的，就算想取也是不可能的。</p>
<p>我可以举几个典型的例子来说明，为什么科学主义是不可取的。对所有的对象一视同仁，忽略掉它们的特质，从而用数学的方法对它们进行处理，这是典型的科学方法。但是，忽略掉个体的特质，肯定不是没有缺陷的，特别用在人与人交往的领域。教育学里有一个原则叫“因材施教”，如果培养学生像工业化的流水线生产一样，那肯定不是一个好的教育方法。</p>
<p>还有，对所有的对象，只注意它表现出来的东西，不关注它表现不出来的东西，这也是典型的科学方法，或称黑箱方法，因为科学要求一种可操作性，追求一种有效处理问题的能力。但是，如果承认每个人都有自己的“内心世界”的话，那人际之间的许多问题就不是能够用黑箱方法解决的。</p>
<p>还可以提出一种人们可能最不赞同的科学方法：为达目的不择手段。从效率的观点、从目标管理的观点、从操作的观点看，这确实是可接受的。而这些观点，又恰恰是科学方法所必须采纳的。但是，离开科学研究的比较单纯的境域（context）进入一个比较复杂的境域中，某种科学方法就变得太荒谬而不可取了。胡适当年评论科玄论战的“科”字方时一针见血的指出，他们都没有端出一个“科学的人生观”来，原因是，他们虽然抽象的承认科学可以解决人生问题，却不愿公然认同那具体的“纯物质、纯机械的人生观”。显然，没有人会认同“为了解决人口问题在人口稠密的地区放原子弹、做核试验”，虽然其方法是高效的。希特勒手下的科学家一直在研究如何高效的毒死犹太人和高效的焚化犹太人的尸体，他们确实很讲科学方法的。</p>
<p>科学的方法由于服务于对自然的支配和控制，所以其基本的逻辑是强权的逻辑、力量的逻辑。这种逻辑用于社会问题上，给出的必定是一个严密控制和高效统治的社会方案，它最好的使用对象是一个奴役的社会。这正是我们要对科学方法在社会领域的运用保持警惕的地方：科学主义的社会理论充分运用的地方有可能是一个极权社会。</p>
<p>科学主义的限度不仅在于它在非科学领域的实际运用有可能是非法的和无效的，也就是说，它实际上“不能够”无条件的运用，而且在于那种“应该无限扩张”的原始动机是可疑的。一些伟大的科学家们都深深地意识到科学的有限性，反对科学的“无限扩张”。爱因斯坦在对加州理工学院学生的讲话中说：“你们只懂得应用科学本身是不够的。关心人的本身，应当始终成为一切技术上奋斗的主要目标；关心怎样组织人的劳动和产品分配这样一些尚未解决的重大问题，用以保证我们科学思想的成果会造福于人类，而不致成为祸害。在你们埋头于图表和方程时，千万不要忘记这一点！”[35]在1949年8月20日的一封信中，爱因斯坦说：“我酷爱正义，并竭尽全力为改善人类境况而奋斗，但这些同我对科学的兴趣是互不相干的。”[36] 1931年10月19日祝贺大法官布兰代斯的信说：“人类真正的进步的取得，依赖于发明创造的并不多，而更多的是依赖于像布兰代斯这样的人的良知良能。”[37] 1937年9月的一封信中，爱因斯坦说：“我们切莫忘记，仅凭知识和技巧并不能给人类的生活带来幸福和尊严。人类完全有理由把高尚的道德标准和价值观的宣道士置于客观真理的发现者之上。在我看来，释迦牟尼、摩西和耶稣对人类所作的贡献远远超过那些聪明才智之士所取得的一切成就。”[38]</p>
<p>无限扩张的动机实际上是一种无根据的动机，是虚无主义的意志。怀特海说得好，“科学从来不为自己的信念找根据，或解释自身的意义”[39]，它的根据和意义必得从更一般的人类思想中找寻。科学主义不是科学的一个必然产物，而是技术理性支配下的某种特定的意识形态。伟大的科学家们都不是科学主义者，他们深知科学的限度。</p>
<p>我已经指出了弘扬科学精神的第一种思路的种种缺陷，但我还是赞成弘扬科学精神的提法，因为还有第二种思路。</p>
<p>第二种思路是科学与人文相统一的思路。这里弘扬的不是与人文相对立的意义上、更具优越性的科学的方法，而是本质上就是人文精神的科学精神，也就是我们在前面反复讨论过的“自由”精神。它与第一种思路的效用的精神、权力意志的精神、科学至上的精神相对立。在中国，有许多杰出的科学家充分意识到了，科学的“精神”之高出具体“科学”的地方就在于“追求真理”。竺可桢说：“提倡科学，不但要晓得科学的方法，而尤贵在乎认清近代科学的目标。近代科学的目标是什么？就是探求真理。科学方法可以随时随地而改变，这科学目标，蕲求真理也就是科学的精神，是永远不改变的。”[40]如何“追求真理”？竺可桢概括说：“只问是非，不计利害”。</p>
<p>“不计利害”对于一个崇尚实用理性的民族来说是很难理解和接受的，这正是我们缺乏科学精神的根本原因。“不计利害”包含着独立思考、怀疑批判的精神，包含着不畏强权、为真理而献身的精神，包含着为科学而科学的精神，所有这一切，实际上都是自由的精神。弘扬科学精神，首先是弘扬自由的精神。</p>
<p>为什么要弘扬科学精神？因为在这个科技昌明的时代，自由的精神反而面临着威胁和危险。“算计利害”而非“不计利害”成了压倒性的时代精神，令自由的心灵感到窒息，这是90年代的有识之士发起人文精神大讨论的真正动机。弘扬科学精神轻而易举地走上了第一种思路，更表明“不计利害、但求是非”的自由精神已处在遗忘的边缘。</p>
<hr>
<p>[1] 参见C. P. 斯诺《两种文化》，纪树立译，三联书店1994年版。</p>
<p>[2] 拙着“技术与人文”，《北京社会科学》2001年第2期</p>
<p>[3] 《雅典之夜》（Attic Nights）13.17.1，转引自D. Goicoechea, eds., The Question of Humanism, Prometheus Books,1991, p.42</p>
<p>[4] 参见樊洪业：“从格致到科学”，《自然辩证法通讯》1988年第3期</p>
<p>[5] 类似的学问由于服务于不同的人性理想，在西方为“科学”，在中国则为“礼”。以关于天象的学问为例，希腊人发展出了数理天文学（mathematical astronomy），中国人则发展出了以占星为主题的天学（astrology）。中国天学并非对天象本身而是对天象所象征的东西感兴趣，它的基本功能是为制定“礼”服务。参见江晓原《天学真原》（辽宁教育出版社1991年版）一书之论述。</p>
<p>[6] 近代以来很长时间，基础科学、理论科学还被称做自然哲学，比如牛顿的名著称为《自然哲学的数学原理》（1687），光之波动说的复兴者托马斯·杨的名著称为《自然哲学讲义》（1807），拉马克的进化论的代表作是《动物哲学》（1809）。</p>
<p>[7] 参见苗力田主编《亚里士多德全集》第七卷，中国人民大学出版社1993年版，第31页。</p>
<p>[8] 参见博伊德《西方教育史》，任宝祥等译，人民教育出版社1985年版，第69页。</p>
<p>[9] 参见罗素《西方哲学史》上卷，何兆武等译，商务印书馆1963年版，第519页。</p>
<p>[10] 怀特海：《科学与近代世界》，何钦译，商务印书馆1959年版，第13页。</p>
<p>[11] 同上，第12页。</p>
<p>[12] 德国人J. T. Miethammer于1808年在辩论古典文化的重要性时第一次使用这个词，1859年，George Voigt在《古典文化的复兴或人文主义的第一个世纪》（The Revival of Classical Antiquity or The First Century of Humanism）一书中，将这个词用于文艺复兴。参见The Question of Humanism, p.94-95。</p>
<p>[13] 按照克利斯特勒在《意大利文艺复兴时期八个哲学家》（姚鹏等译，上海译文出版社1987年版）中的说法，studia humanitatis是一个可以追溯到罗马作家的古老用词，而humanista可以追溯到15世纪后期，16世纪开始通用。（第182－183页）</p>
<p>[14] 参见克利斯特勒《意大利文艺复兴时期八个哲学家》，第27页。</p>
<p>[15] 转引自The Question of Humanism, p.27</p>
<p>[16] 《莎士比亚全集》，朱生豪译，人民文学出版社1994年版，第5卷第327页。</p>
<p>[17] 宋祖良在其《拯救地球和人类未来》（中国社会科学出版社1993年版）中把海德格尔的《关于人道主义的信》译成《论人类中心主义的信》。</p>
<p>[18] 王若水：《为人道主义辩护》，三联书店1986年版，第242页。</p>
<p>[19] 参见拙文“技术与人文”</p>
<p>[20] 科恩：《牛顿革命》，颜锋等译，江西教育出版社1999年版，第5页</p>
<p>[21] 培根：《新工具》第一卷第七十一节，许宝騤译，商务印书馆1984年版，第48、49页。</p>
<p>[22] 库恩就曾提出物理学发展中的数学传统与实验传统的对立，参见《必要的张力》，纪树立等译，福建人民出版社1981年版，第32页。</p>
<p>[23] 爱因斯坦：“自述”，载《爱因斯坦文集》第1卷，许良英等编译，商务印书馆1976年版，第10－11页。</p>
<p>[24] 爱因斯坦：“理论物理学的方法”，载《爱因斯坦文集》第1卷，第314页。</p>
<p>[25] 参见科恩：《牛顿革命》，第168页。</p>
<p>[26] 《新工具》第一卷第三节</p>
<p>[27] 值得注意的是，20世纪的大数学家大哲学家怀特海视为自己最终哲学观点的讲演正是《数学与善》。</p>
<p>[28] 参见拙着《希腊空间概念的发展》第八章，四川教育出版社1994年版。</p>
<p>[29] 著名科学史家亚历山大·柯瓦雷(Alexander Koyre)的名著《从封闭世界到无限宇宙》（From the closed world to the infinite universe）极好的描述了这场革命的实质。</p>
<p>[30] 罗伯特·容克：《比一千个太阳还亮》，何纬译，原子能出版社1966年版，第48页、第50页。</p>
<p>[31] Ronald Barnett, The Idea of Higher Education, Open University Press, 1990, p.165.</p>
<p>[32] 转引自汉伯里·布朗：《科学的智慧》，李醒民译，辽宁教育出版社1998年版，第136页。</p>
<p>[33] 这其实正是《韦伯斯特新世界词典》里关于“科学主义”（scientism）的定义：the principle that scientific methods can and should be applied in all fields of investigation。</p>
<p>[34] 拙着《追思自然》，辽海出版社1998年版，第398页。</p>
<p>[35] 《爱因斯坦文集》第3卷，许良英等编译，商务印书馆1979年版，第73页。</p>
<p>[36] 杜卡斯编：《爱因斯坦谈人生》，高志凯译，世界知识出版社1984年版，第23页。</p>
<p>[37] 《爱因斯坦谈人生》，第75页。</p>
<p>[38] 《爱因斯坦谈人生》，第61－62页。</p>
<p>[39] 怀特海：《科学与近代世界》，第16－17页。</p>
<p>[40] 竺可桢：《看风云舒卷》，百花文艺出版社1998年版，第140页。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
  </section>

  


          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel  sidebar-panel-active ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.gif"
               alt="starrysky" />
          <p class="site-author-name" itemprop="name">starrysky</p>
          <p class="site-description motion-element" itemprop="description"></p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/">
              <span class="site-state-item-count">2</span>
              <span class="site-state-item-name">Artikel</span>
            </a>
          </div>

          

          
            <div class="site-state-item site-state-tags">
              
                <span class="site-state-item-count">2</span>
                <span class="site-state-item-name">Tags</span>
              
            </div>
          

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/lixinyancici" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://www.zhihu.com/people/li-xin-yan-73" target="_blank" title="ZhiHu">
                  
                    <i class="fa fa-fw fa-globe"></i>
                  
                  ZhiHu
                </a>
              </span>
            
          
        </div>

        
        

        
        
          <div class="links-of-blogroll motion-element links-of-blogroll-inline">
            <div class="links-of-blogroll-title">
              <i class="fa  fa-fw fa-globe"></i>
              
            </div>
            <ul class="links-of-blogroll-list">
              
                <li class="links-of-blogroll-item">
                  <a href="http://blog.sciencenet.cn/u/zmpenguestc" title="老马迷图" target="_blank">老马迷图</a>
                </li>
              
            </ul>
          </div>
        

      </section>

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy;  2016 - 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">starrysky</span>
</div>

<div class="powered-by">
  Erstellt mit  <a class="theme-link" href="http://hexo.io">Hexo</a>
</div>

<div class="theme-info">
  Theme -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Pisces
  </a>
</div>

        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/vendors/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.0.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.0.1"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.0.1"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.0.1"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.0.1"></script>



  



  



  
  
  
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
  </script>

  <script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for (i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
      }
    });
  </script>
  <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


  

  

</body>
</html>
