<!doctype html>



  


<html class="theme-next pisces use-motion">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  
  
  <link href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/vendors/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.0.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Machine Learning," />





  <link rel="alternate" href="/atom.xml" title="StarrySky's Blog" type="application/atom+xml" />




  <link rel="shortcut icon" type="image/x-icon" href="/images/myfavicon.ico?v=5.0.1" />






<meta name="description" content="全文引自Free Mind博客,如果要引用请注明Free Mind的原文地址。  ##Maximum Margin Classifier    支持向量机即 Support Vector Machine，简称 SVM 。我最开始听说这头机器的名号的时候，一种神秘感就油然而生，似乎把 Support 这么一个具体的动作和 Vector 这么一个抽象的概念拼到一起，然后再做成一个 Machine ，">
<meta name="keywords" content="Machine Learning">
<meta property="og:type" content="article">
<meta property="og:title" content="(转)支持向量机">
<meta property="og:url" content="http://yoursite.com/2016/05/23/svm/index.html">
<meta property="og:site_name" content="StarrySky's Blog">
<meta property="og:description" content="全文引自Free Mind博客,如果要引用请注明Free Mind的原文地址。  ##Maximum Margin Classifier    支持向量机即 Support Vector Machine，简称 SVM 。我最开始听说这头机器的名号的时候，一种神秘感就油然而生，似乎把 Support 这么一个具体的动作和 Vector 这么一个抽象的概念拼到一起，然后再做成一个 Machine ，">
<meta property="og:image" content="http://blog.pluskid.org/wp-content/uploads/2010/09/svm.png">
<meta property="og:image" content="http://blog.pluskid.org/wp-content/uploads/2010/09/Hyper-Plane.png">
<meta property="og:image" content="http://blog.pluskid.org/wp-content/uploads/2010/09/geometric_margin.png">
<meta property="og:image" content="http://blog.pluskid.org/wp-content/uploads/2010/09/Optimal-Hyper-Plane.png">
<meta property="og:image" content="http://blog.pluskid.org/wp-content/uploads/2010/09/Optimal-Hyper-Plane.png">
<meta property="og:image" content="http://blog.pluskid.org/wp-content/uploads/2010/09/two_circles.png">
<meta property="og:image" content="http://blog.pluskid.org/wp-content/uploads/2010/09/rotate.gif">
<meta property="og:image" content="http://blog.pluskid.org/wp-content/uploads/2010/09/Optimal-Hyper-Plane-2.png">
<meta property="og:image" content="http://blog.pluskid.org/wp-content/uploads/2010/09/interval.png">
<meta property="og:image" content="http://blog.pluskid.org/wp-content/uploads/2011/01/infinity.png">
<meta property="og:updated_time" content="2016-05-23T12:17:39.084Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="(转)支持向量机">
<meta name="twitter:description" content="全文引自Free Mind博客,如果要引用请注明Free Mind的原文地址。  ##Maximum Margin Classifier    支持向量机即 Support Vector Machine，简称 SVM 。我最开始听说这头机器的名号的时候，一种神秘感就油然而生，似乎把 Support 这么一个具体的动作和 Vector 这么一个抽象的概念拼到一起，然后再做成一个 Machine ，">
<meta name="twitter:image" content="http://blog.pluskid.org/wp-content/uploads/2010/09/svm.png">



<script type="text/javascript" id="hexo.configuration">
  var NexT = window.NexT || {};
  var CONFIG = {
    scheme: 'Pisces',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: 0,
      author: 'Author'
    }
  };
</script>

  <title> (转)支持向量机 | StarrySky's Blog </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="">

  










  
  
    
  

  <div class="container one-collumn sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta custom-logo">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">StarrySky's Blog</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle"></p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-主页">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br />
            
            menu.主页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-存档">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br />
            
            menu.存档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-标签">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br />
            
            menu.标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-分类">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br />
            
            menu.分类
          </a>
        </li>
      

      
    </ul>
  

  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                (转)支持向量机
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Veröffentlicht am</span>
            <time itemprop="dateCreated" datetime="2016-05-23T19:01:31+08:00" content="2016-05-23">
              2016-05-23
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <blockquote>
<p>全文引自<a href="http://blog.pluskid.org/?p=702" target="_blank" rel="external">Free Mind</a>博客,如果要引用请注明Free Mind的原文地址。</p>
</blockquote>
<p>##<strong>Maximum Margin Classifier</strong> </p>
<center><img src="http://blog.pluskid.org/wp-content/uploads/2010/09/svm.png" alt=""></center>

<p>支持向量机即 Support Vector Machine，简称 SVM 。我最开始听说这头机器的名号的时候，一种神秘感就油然而生，似乎把 Support 这么一个具体的动作和 Vector 这么一个抽象的概念拼到一起，然后再做成一个 Machine ，一听就很玄了！</p>
<p>不过后来我才知道，原来 SVM 它并不是一头机器，而是一种算法，或者，确切地说，是一类算法，当然，这样抠字眼的话就没完没了了，比如，我说 SVM 实际上是一个分类器 (Classifier) ，但是其实也是有用 SVM 来做回归 (Regression) 的。所以，这种字眼就先不管了，还是从分类器说起吧。</p>
<p>SVM 一直被认为是效果最好的现成可用的分类算法之一（其实有很多人都相信，“之一”是可以去掉的）。这里“现成可用”其实是很重要的，因为一直以来学术界和工业界甚至只是学术界里做理论的和做应用的之间，都有一种“鸿沟”，有些很 fancy 或者很复杂的算法，在抽象出来的模型里很完美，然而在实际问题上却显得很脆弱，效果很差甚至完全 fail 。而 SVM 则正好是一个特例——在两边都混得开。</p>
<p>好了，由于 SVM 的故事本身就很长，所以废话就先只说这么多了，直接入题吧。当然，说是入贴，但是也不能一上来就是 SVM ，而是必须要从线性分类器开始讲。这里我们考虑的是一个两类的分类问题，数据点用 $x$ 来表示，这是一个 $n$ 维向量，而类别用 $y$ 来表示，可以取 1 或者 -1 ，分别代表两个不同的类（有些地方会选 0 和 1 ，当然其实分类问题选什么都无所谓，只要是两个不同的数字即可，不过这里选择 +1 和 -1 是为了方便 SVM 的推导，后面就会明了了）。一个线性分类器就是要在 $n$ 维的数据空间中找到一个超平面，其方程可以表示为</p>
<p>$$<br>w^Tx + b = 0<br>$$</p>
<p>一个超平面，在二维空间中的例子就是一条直线。我们希望的是，通过这个超平面可以把两类数据分隔开来，比如，在超平面一边的数据点所对应的 $y$ 全是 -1 ，而在另一边全是 1 。具体来说，我们令 $f(x)=w^Tx + b$ ，显然，如果 $f(x)=0$ ，那么 $x$ 是位于超平面上的点。我们不妨要求对于所有满足 $f(x)<0$ 的点，其对应的="" $y$="" 等于-1，而="" $f(x)="">0$ 则对应 $y=1$ 的数据点。当然，有些时候（或者说大部分时候）数据并不是线性可分的，这个时候满足这样条件的超平面就根本不存在，不过关于如何处理这样的问题我们后面会讲，这里先从最简单的情形开始推导，就假设数据都是线性可分的，亦即这样的超平面是存在的。</0$></p>
<center><img src="http://blog.pluskid.org/wp-content/uploads/2010/09/Hyper-Plane.png" alt=""></center>

<p>如上图所示，两种颜色的点分别代表两个类别，红颜色的线表示一个可行的超平面。在进行分类的时候，我们将数据点 $x$ 代入 $f(x)$ 中，如果得到的结果小于 0 ，则赋予其类别 -1 ，如果大于 0 则赋予类别 1 。如果 $f(x)=0$，则很难办了，分到哪一类都不是。事实上，对于 $f(x)$ 的绝对值很小的情况，我们都很难处理，因为细微的变动（比如超平面稍微转一个小角度）就有可能导致结果类别的改变。理想情况下，我们希望 $f(x)$ 的值都是很大的正数或者很小的负数，这样我们就能更加确信它是属于其中某一类别的。</p>
<p>从几何直观上来说，由于超平面是用于分隔两类数据的，越接近超平面的点越“难”分隔，因为如果超平面稍微转动一下，它们就有可能跑到另一边去。反之，如果是距离超平面很远的点，例如图中的右上角或者左下角的点，则很容易分辩出其类别。</p>
<center><img src="http://blog.pluskid.org/wp-content/uploads/2010/09/geometric_margin.png" alt=""></center>

<p>实际上这两个 Criteria 是互通的，我们定义 functional margin 为 $\hat{\gamma}=y(w^Tx+b)=yf(x)$，注意前面乘上类别 $y$ 之后可以保证这个 margin 的非负性（因为 $f(x)&lt;0$ 对应于 $y=-1$ 的那些点），而点到超平面的距离定义为 geometrical margin 。不妨来看看二者之间的关系。如图所示，对于一个点 $x$ ，令其垂直投影到超平面上的对应的为 $x_0$ ，由于 $w$ 是垂直于超平面的一个向量（请自行验证），我们有 $$ x=x_0+\gamma\frac{w}{|w|} $$ 又由于 $x_0$ 是超平面上的点，满足 $f(x_0)=0$ ，代入超平面的方程即可算出 $$ \gamma = \frac{w^Tx+b}{|w|}=\frac{f(x)}{|w|} $$ 不过，这里的 $\gamma$ 是带符号的，我们需要的只是它的绝对值，因此类似地，也乘上对应的类别 $y$ 即可，因此实际上我们定义 geometrical margin 为： $$ \tilde{\gamma} = y\gamma = \frac{\hat{\gamma}}{|w|} $$ 显然，functional margin 和 geometrical margin 相差一个 $|w|$ 的缩放因子。按照我们前面的分析，对一个数据点进行分类，当它的 margin 越大的时候，分类的 confidence 越大。对于一个包含 $n$ 个点的数据集，我们可以很自然地定义它的 margin 为所有这 $n$ 个点的 margin 值中最小的那个。于是，为了使得分类的 confidence 高，我们希望所选择的 hyper plane 能够最大化这个 margin 值。 不过这里我们有两个 margin 可以选，不过 functional margin 明显是不太适合用来最大化的一个量，因为在 hyper plane 固定以后，我们可以等比例地缩放 $w$ 的长度和 $b$ 的值，这样可以使得 $f(x)=w^Tx+b$ 的值任意大，亦即 functional margin $\hat{\gamma}$ 可以在 hyper plane 保持不变的情况下被取得任意大，而 geometrical margin 则没有这个问题，因为除上了 $|w|$ 这个分母，所以缩放 $w$ 和 $b$ 的时候 $\tilde{\gamma}$ 的值是不会改变的，它只随着 hyper plane 的变动而变动，因此，这是更加合适的一个 margin 。这样一来，我们的 maximum margin classifier 的目标函数即定义为 $$ \max \tilde{\gamma} $$ 当然，还需要满足一些条件，根据 margin 的定义，我们有 $$ y_i(w^Tx_i+b) = \hat{\gamma}_i \geq \hat{\gamma}, \quad i=1,\ldots,n $$ 其中 $\hat{\gamma}=\tilde{\gamma}|w|$ ，根据我们刚才的讨论，即使在超平面固定的情况下，$\hat{\gamma}$ 的值也可以随着 $|w|$ 的变化而变化。由于我们的目标就是要确定超平面，因此可以把这个无关的变量固定下来，固定的方式有两种：一是固定 $|w|$ ，当我们找到最优的 $\tilde{\gamma}$ 时 $\hat{\gamma}$ 也就可以随之而固定；二是反过来固定 $\hat{\gamma}$ ，此时 $|w|$ 也可以根据最优的 $\tilde{\gamma}$ 得到。处于方便推导和优化的目的，我们选择第二种，令 $\hat{\gamma}=1$ ，则我们的目标函数化为： $$ \max \frac{1}{|w|}, \quad s.t., y_i(w^Tx_i+b)\geq 1, i=1,\ldots,n $$ 通过求解这个问题，我们就可以找到一个 margin 最大的 classifier ，如下图所示，中间的红色线条是 Optimal Hyper Plane ，另外两条线到红线的距离都是等于 $\tilde{\gamma}$ 的： </p>
<center><img src="http://blog.pluskid.org/wp-content/uploads/2010/09/Optimal-Hyper-Plane.png" alt=""></center>

<p>到此为止，算是完成了 Maximum Margin Classifier 的介绍，通过最大化 margin ，我们使得该分类器对数据进行分类时具有了最大的 confidence （实际上，根据我们说给的一个数据集的 margin 的定义，准确的说，应该是“对最不 confidence 的数据具有了最大的 confidence”——虽然有点拗口）。不过，到现在似乎还没有一点点 Support Vector Machine 的影子。很遗憾的是，这个要等到下一次再说了，不过可以先小小地剧透一下，如上图所示，我们可以看到 hyper plane 两边的那个 gap 分别对应的两条平行的线（在高维空间中也应该是两个 hyper plane）上有一些点，显然两个 hyper plane 上都会有点存在，否则我们就可以进一步扩大 gap ，也就是增大 $\tilde{\gamma}$ 的值了。这些点呢，就叫做 support vector ，嗯，先说这么多了。</p>
<p>##<strong>Support Vector</strong><br>上一次介绍支持向量机，结果说到 Maximum Margin Classifier ，到最后都没有说“支持向量”到底是什么东西。不妨回忆一下上次最后一张图：</p>
<center><img src="http://blog.pluskid.org/wp-content/uploads/2010/09/Optimal-Hyper-Plane.png" alt=""></center>

<p>可以看到两个支撑着中间的 gap 的超平面，它们到中间的 separating hyper plane 的距离相等（想想看：为什么一定是相等的？），即我们所能得到的最大的 geometrical margin $\tilde{\gamma}$ 。而“支撑”这两个超平面的必定会有一些点，试想，如果某超平面没有碰到任意一个点的话，那么我就可以进一步地扩充中间的 gap ，于是这个就不是最大的 margin 了。由于在 $n$ 维向量空间里一个点实际上是和以原点为起点，该点为终点的一个向量是等价的，所以这些“支撑”的点便叫做支持向量。</p>
<p>很显然，由于这些 supporting vector 刚好在边界上，所以它们是满足 $y(w^Tx+b)=1$ （还记得我们把 functional margin 定为 1 了吗？），而对于所有不是支持向量的点，也就是在“阵地后方”的点，则显然有 $y(w^Tx+b) &gt; 1$ 。事实上，当最优的超平面确定下来之后，这些后方的点就完全成了路人甲了，它们可以在自己的边界后方随便飘来飘去都不会对超平面产生任何影响。这样的特性在实际中有一个最直接的好处就在于存储和计算上的优越性，例如，如果使用 100 万个点求出一个最优的超平面，其中是 supporting vector 的有 100 个，那么我只需要记住这 100 个点的信息即可，对于后续分类也只需要利用这 100 个点而不是全部 100 万个点来做计算。（当然，通常除了 K-Nearest Neighbor 之类的 Memory-based Learning 算法，通常算法也都不会直接把所有的点记忆下来，并全部用来做后续 inference 中的计算。不过，如果算法使用了 Kernel 方法进行非线性化推广的话，就会遇到这个问题了。Kernel 方法在下一次会介绍。）</p>
<p>当然，除了从几何直观上之外，支持向量的概念也会从其优化过程的推导中得到。其实上一次还偷偷卖了另一个关子就是虽然给出了目标函数，却没有讲怎么来求解。现在就让我们来处理这个问题。回忆一下之前得到的目标函数：</p>
<p>$$<br>\max \frac{1}{|w|}\quad s.t., y_i(w^Tx_i+b)\geq 1, i=1,\ldots,n<br>$$</p>
<p>这个问题等价于（为了方便求解，我在这里加上了平方，还有一个系数，显然这两个问题是等价的，因为我们关心的并不是最优情况下目标函数的具体数值）：</p>
<p>$$<br>\min \frac{1}{2}|w|^2\quad s.t., y_i(w^Tx_i+b)\geq 1, i=1,\ldots,n<br>$$</p>
<p>到这个形式以后，就可以很明显地看出来，它是一个凸优化问题，或者更具体地说，它是一个二次优化问题——目标函数是二次的，约束条件是线性的。这个问题可以用任何现成的 QP (Quadratic Programming) 的优化包进行求解。所以，我们的问题到此为止就算全部解决了，于是我睡午觉去了~ :)</p>
<p>啊？呃，有人说我偷懒不负责任了？好吧，嗯，其实呢，虽然这个问题确实是一个标准的 QP 问题，但是它也有它的特殊结构，通过 Lagrange Duality 变换到对偶变量 (dual variable) 的优化问题之后，可以找到一种更加有效的方法来进行求解——这也是 SVM 盛行的一大原因，通常情况下这种方法比直接使用通用的 QP 优化包进行优化要高效得多。此外，在推导过程中，许多有趣的特征也会被揭露出来，包括刚才提到的 supporting vector 的问题。</p>
<p>关于 Lagrange duality 我没有办法在这里细讲了，可以参考 Wikipedia 。简单地来说，通过给每一个约束条件加上一个 Lagrange multiplier，我们可以将它们融和到目标函数里去</p>
<p>$$<br>\mathcal{L}(w,b,\alpha)=\frac{1}{2}|w|^2-\sum_{i=1}^n\alpha_i \left(y_i(w^Tx_i+b)-1\right)<br>$$</p>
<p>然后我们令</p>
<p>$$<br>\theta(w) = \max_{\alpha_i\geq 0}\mathcal{L}(w,b,\alpha)<br>$$</p>
<p>容易验证，当某个约束条件不满足时，例如 $y_i(w^Tx_i+b) &lt; 1$，那么我们显然有 $\theta(w)=\infty$ （只要令 $\alpha_i=\infty$ 即可）。而当所有约束条件都满足时，则有 $\theta(w)=\frac{1}{2}|w|^2$ ，亦即我们最初要最小化的量。因此，在要求约束条件得到满足的情况下最小化 $\frac{1}{2}|w|^2$ 实际上等价于直接最小化 $\theta(w)$ （当然，这里也有约束条件，就是 $\alpha_i \geq 0,i=1,\cdots,n$），因为如果约束条件没有得到满足，$\theta(w)$ 会等于无穷大，自然不会是我们所要求的最小值。具体写出来，我们现在的目标函数变成了：</p>
<p>$$<br>\min<em>{w,b};\theta(w) = \min</em>{w,b};<br>\max_{\alpha_i\geq 0};<br>\mathcal{L}(w,b,\alpha) = p^*<br>$$ </p>
<p>这里用 $p^* $ 表示这个问题的最优值，这个问题和我们最初的问题是等价的。不过，现在我们来把最小和最大的位置交换一下： </p>
<p>$$<br>\max_{\alpha<em>i \geq 0};<br>\min</em>{w,b};<br>\mathcal{L}(w,b,\alpha) = d^*<br>$$ </p>
<p>当然，交换以后的问题不再等价于原问题，这个新问题的最优值用 $d^<em> $ 来表示。并，我们有 $d^</em> \leq p^<em> $ ，这在直观上也不难理解，最大值中最小的一个总也比最小值中最大的一个要大吧！ :) 总之，第二个问题的最优值 $d^</em> $ 在这里提供了一个第一个问题的最优值 $p^*  $ 的一个下界，在满足某些条件的情况下，这两者相等，这个时候我们就可以通过求解第二个问题来间接地求解第一个问题。具体来说，就是要满足 KKT 条件，这里暂且先略过不说，直接给结论：我们这里的问题是满足 KKT 条件的，因此现在我们便转化为求解第二个问题。</p>
<p>首先要让 $\mathcal{L}$ 关于 $w$ 和 $b$ 最小化，我们分别令 $\partial \mathcal{L}/\partial w$ 和 $\partial \mathcal{L}/\partial b$ 等于零：</p>
<p>$$<br>\begin{aligned}<br>\frac{\partial \mathcal{L}}{\partial w}=0 &amp;\Rightarrow w=\sum_{i=1}^n \alpha_i y_i x<em>i \<br>\frac{\partial \mathcal{L}}{\partial b} = 0 &amp;\Rightarrow \sum</em>{i=1}^n \alpha_i y_i = 0<br>\end{aligned}<br>$$</p>
<p>带回 $\mathcal{L}$ 得到：</p>
<p>$$<br>\begin{aligned}<br>\mathcal{L}(w,b,\alpha) &amp;= \frac{1}{2}\sum_{i,j=1}^n\alpha_i\alpha_jy_iy_jx_i^Tx<em>j-\sum</em>{i,j=1}^n\alpha_i\alpha_jy_iy_jx_i^Tx<em>j – b\sum</em>{i=1}^n\alpha_iy<em>i + \sum</em>{i=1}^n\alpha<em>i \<br>&amp;= \sum</em>{i=1}^n\alpha<em>i – \frac{1}{2}\sum</em>{i,j=1}^n\alpha_i\alpha_jy_iy_jx_i^Tx_j<br>\end{aligned}<br>$$</p>
<p>此时我们得到关于 dual variable $\alpha$ 的优化问题：</p>
<p>$$<br>\begin{aligned}<br>\max<em>\alpha &amp;\sum</em>{i=1}^n\alpha<em>i – \frac{1}{2}\sum</em>{i,j=1}^n\alpha_i\alpha_jy_iy_jx_i^Tx_j \<br>s.t., &amp;\alpha<em>i\geq 0, i=1,\ldots,n \<br>&amp;\sum</em>{i=1}^n\alpha_iy_i = 0<br>\end{aligned}<br>$$</p>
<p>如前面所说，这个问题有更加高效的优化算法，不过具体方法在这里先不介绍，让我们先来看看推导过程中得到的一些有趣的形式。首先就是关于我们的 hyper plane ，对于一个数据点 $x$ 进行分类，实际上是通过把 $x$ 带入到 $f(x)=w^Tx+b$ 算出结果然后根据其正负号来进行类别划分的。而前面的推导中我们得到 $w=\sum_{i=1}^n \alpha_i y_i x_i$ ，因此</p>
<p>$$<br>\begin{aligned}<br>f(x)&amp;=\left(\sum_{i=1}^n\alpha_i y_i x<em>i\right)^Tx+b \<br>&amp;= \sum</em>{i=1}^n\alpha_i y_i \langle x_i, x\rangle + b<br>\end{aligned}<br>$$</p>
<p>这里的形式的有趣之处在于，对于新点 $x$ 的预测，只需要计算它与训练数据点的内积即可（这里 $\langle \cdot, \cdot\rangle$ 表示向量内积），这一点至关重要，是之后使用 Kernel 进行非线性推广的基本前提。此外，所谓 Supporting Vector 也在这里显示出来——事实上，所有非 Supporting Vector 所对应的系数 $\alpha$ 都是等于零的，因此对于新点的内积计算实际上只要针对少量的“支持向量”而不是所有的训练数据即可。</p>
<p>为什么非支持向量对应的 $\alpha$ 等于零呢？直观上来理解的话，就是这些“后方”的点——正如我们之前分析过的一样，对超平面是没有影响的，由于分类完全有超平面决定，所以这些无关的点并不会参与分类问题的计算，因而也就不会产生任何影响了。这个结论也可由刚才的推导中得出，回忆一下我们刚才通过 Lagrange multiplier 得到的目标函数：</p>
<p>$$<br>\max_{\alpha<em>i\geq 0}\;\mathcal{L}(w,b,\alpha)=\max</em>{\alpha<em>i\geq 0}\;\frac{1}{2}|w|^2-\sum</em>{i=1}^n\alpha_i \color{red}{\left(y_i(w^Tx_i+b)-1\right)}<br>$$</p>
<p>注意到如果 $x_i$ 是支持向量的话，上式中红颜色的部分是等于 0 的（因为支持向量的 functional margin 等于 1 ），而对于非支持向量来说，functional margin 会大于 1 ，因此红颜色部分是大于零的，而 $\alpha_i$ 又是非负的，为了满足最大化，$\alpha_i$ 必须等于 0 。这也就是这些非 Supporting Vector 的点的悲惨命运了。 :p</p>
<p>嗯，于是呢，把所有的这些东西整合起来，得到的一个 maximum margin hyper plane classifier 就是支持向量机（Support Vector Machine），经过直观的感觉和数学上的推导，为什么叫“支持向量”，应该也就明了了吧？当然，到目前为止，我们的 SVM 还比较弱，只能处理线性的情况，不过，在得到了 dual 形式之后，通过 Kernel 推广到非线性的情况就变成了一件非常容易的事情了。不过，具体细节，还要留到下一次再细说了。 :)</p>
<p>##<strong>Kernel</strong> </p>
<p>前面我们介绍了线性情况下的支持向量机，它通过寻找一个线性的超平面来达到对数据进行分类的目的。不过，由于是线性方法，所以对非线性的数据就没有办法处理了。例如图中的两类数据，分别分布为两个圆圈的形状，不论是任何高级的分类器，只要它是线性的，就没法处理，SVM 也不行。因为这样的数据本身就是线性不可分的。</p>
<center><img src="http://blog.pluskid.org/wp-content/uploads/2010/09/two_circles.png" alt=""></center>

<p>对于这个数据集，我可以悄悄透露一下：我生成它的时候就是用两个半径不同的圆圈加上了少量的噪音得到的，所以，一个理想的分界应该是一个“圆圈”而不是一条线（超平面）。如果用 $X_1$ 和 $X_2$ 来表示这个二维平面的两个坐标的话，我们知道一条二次曲线（圆圈是二次曲线的一种特殊情况）的方程可以写作这样的形式：</p>
<p>$$<br>a_1X_1 + a_2X_1^2 + a_3 X_2 + a_4 X_2^2 + a_5 X_1X_2 + a_6 = 0<br>$$</p>
<p>注意上面的形式，如果我们构造另外一个五维的空间，其中五个坐标的值分别为 $Z_1 = X_1$, $Z_2=X_1^2$, $Z_3=X_2$, $Z_4=X_2^2$, $Z_5=X_1X_2$，那么显然，上面的方程在新的坐标系下可以写作：</p>
<p>$$<br>\sum_{i=1}^5a_iZ_i + a_6 = 0<br>$$</p>
<p>关于新的坐标 $Z$ ，这正是一个 hyper plane 的方程！也就是说，如果我们做一个映射 $\phi:\mathbb{R}^2\rightarrow\mathbb{R}^5$ ，将 $X$ 按照上面的规则映射为 $Z$ ，那么在新的空间中原来的数据将变成线性可分的，从而使用之前我们推导的线性分类算法就可以进行处理了。这正是 Kernel 方法处理非线性问题的基本思想。</p>
<p>再进一步描述 Kernel 的细节之前，不妨再来看看这个例子映射过后的直观例子。当然，我没有办法把 5 维空间画出来，不过由于我这里生成数据的时候就是用了特殊的情形，具体来说，我这里的超平面实际的方程是这个样子（圆心在 $X_2$ 轴上的一个正圆）：</p>
<p>$$<br>a_1X_1^2 + a_2(X_2-c)^2 + a_3 = 0<br>$$</p>
<p>因此我只需要把它映射到 $Z_1 = X_1^2$, $Z_2=X_2^2$, $Z_3=X_2$ 这样一个三维空间中即可，下图（这是一个 gif 动画）即是映射之后的结果，将坐标轴经过适当的旋转，就可以很明显地看出，数据是可以通过一个平面来分开的：</p>
<center><img src="http://blog.pluskid.org/wp-content/uploads/2010/09/rotate.gif" alt=""></center>

<p>现在让我们再回到 SVM 的情形，假设原始的数据时非线性的，我们通过一个映射 $\phi(\cdot)$ 将其映射到一个高维空间中，数据变得线性可分了，这个时候，我们就可以使用原来的推导来进行计算，只是所有的推导现在是在新的空间，而不是原始空间中进行。当然，推导过程也并不是可以简单地直接类比的，例如，原本我们要求超平面的法向量 $w$ ，但是如果映射之后得到的新空间的维度是无穷维的（确实会出现这样的情况，比如后面会提到的 Gaussian Kernel ），要表示一个无穷维的向量描述起来就比较麻烦。于是我们不妨先忽略过这些细节，直接从最终的结论来分析，回忆一下，我们上一次得到的最终的分类函数是这样的：</p>
<p>$$<br>f(x) = \sum_{i=1}^n\alpha_i y_i \langle x_i, x\rangle + b<br>$$</p>
<p>现在则是在映射过后的空间，即：</p>
<p>$$<br>f(x) = \sum_{i=1}^n\alpha_i y_i \langle \phi(x_i), \phi(x)\rangle + b<br>$$</p>
<p>而其中的 $\alpha$ 也是通过求解如下 dual 问题而得到的：</p>
<p>$$<br>\begin{aligned}<br>\max<em>\alpha &amp;\sum</em>{i=1}^n\alpha<em>i – \frac{1}{2}\sum</em>{i,j=1}^n\alpha_i\alpha_jy_iy_j\langle \phi(x_i),\phi(x_j)\rangle \<br>s.t., &amp;\alpha<em>i\geq 0, i=1,\ldots,n \<br>&amp;\sum</em>{i=1}^n\alpha_iy_i = 0<br>\end{aligned}<br>$$</p>
<p>这样一来问题就解决了吗？似乎是的：拿到非线性数据，就找一个映射 $\phi(\cdot)$ ，然后一股脑把原来的数据映射到新空间中，再做线性 SVM 即可。不过若真是这么简单，我这篇文章的标题也就白写了——说了这么多，其实还没到正题呐！其实刚才的方法稍想一下就会发现有问题：在最初的例子里，我们对一个二维空间做映射，选择的新空间是原始空间的所有一阶和二阶的组合，得到了五个维度；如果原始空间是三维，那么我们会得到 19 维的新空间（验算一下？），这个数目是呈爆炸性增长的，这给 $\phi(\cdot)$ 的计算带来了非常大的困难，而且如果遇到无穷维的情况，就根本无从计算了。所以就需要 Kernel 出马了。</p>
<p>不妨还是从最开始的简单例子出发，设两个向量 $x_1 = (\eta_1,\eta_2)^T$ 和 $x_2=(\xi_1,\xi_2)^T$ ，而 $\phi(\cdot)$ 即是到前面说的五维空间的映射，因此映射过后的内积为：</p>
<p>$$<br>\langle \phi(x_1),\phi(x_2)\rangle = \eta_1\xi_1 + \eta_1^2\xi_1^2 + \eta_2\xi_2 + \eta_2^2\xi_2^2+\eta_1\eta_2\xi_1\xi_2<br>$$</p>
<p>另外，我们又注意到：</p>
<p>$$<br>\left(\langle x_1, x_2\rangle + 1\right)^2 = 2\eta_1\xi_1 + \eta_1^2\xi_1^2 + 2\eta_2\xi_2 + \eta_2^2\xi_2^2 + 2\eta_1\eta_2\xi_1\xi_2 + 1<br>$$</p>
<p>二者有很多相似的地方，实际上，我们只要把某几个维度线性缩放一下，然后再加上一个常数维度，具体来说，上面这个式子的计算结果实际上和映射</p>
<p>$$<br>\varphi(X_1,X_2)=(\sqrt{2}X_1,X_1^2,\sqrt{2}X_2,X_2^2,\sqrt{2}X_1X_2,1)^T<br>$$</p>
<p>之后的内积 $\langle \varphi(x_1),\varphi(x_2)\rangle$ 的结果是相等的（自己验算一下）。区别在于什么地方呢？一个是映射到高维空间中，然后再根据内积的公式进行计算；而另一个则直接在原来的低维空间中进行计算，而不需要显式地写出映射后的结果。回忆刚才提到的映射的维度爆炸，在前一种方法已经无法计算的情况下，后一种方法却依旧能从容处理，甚至是无穷维度的情况也没有问题。</p>
<p>我们把这里的计算两个向量在映射过后的空间中的内积的函数叫做核函数 (Kernel Function) ，例如，在刚才的例子中，我们的核函数为：</p>
<p>$$<br>\kappa(x_1,x_2)=\left(\langle x_1, x_2\rangle + 1\right)^2<br>$$</p>
<p>核函数能简化映射空间中的内积运算——刚好“碰巧”的是，在我们的 SVM 里需要计算的地方数据向量总是以内积的形式出现的。对比刚才我们写出来的式子，现在我们的分类函数为：</p>
<p>$$<br>\sum_{i=1}^n\alpha_i y_i \color{red}{\kappa(x_i,x)} + b<br>$$</p>
<p>其中 $\alpha$ 由如下 dual 问题计算而得：</p>
<p>$$<br>\begin{aligned}<br>\max<em>\alpha &amp;\sum</em>{i=1}^n\alpha<em>i – \frac{1}{2}\sum</em>{i,j=1}^n\alpha_i\alpha_jy_iy_j \color{red}{\kappa(x_i, x_j)} \<br>s.t., &amp;\alpha<em>i\geq 0, i=1,\ldots,n \<br>&amp;\sum</em>{i=1}^n\alpha_iy_i = 0<br>\end{aligned}<br>$$</p>
<p>这样一来计算的问题就算解决了，避开了直接在高维空间中进行计算，而结果却是等价的，实在是一件非常美妙的事情！当然，因为我们这里的例子非常简单，所以我可以手工构造出对应于 $\varphi(\cdot)$ 的核函数出来，如果对于任意一个映射，想要构造出对应的核函数就很困难了。</p>
<p>最理想的情况下，我们希望知道数据的具体形状和分布，从而得到一个刚好可以将数据映射成线性可分的 $\phi(\cdot)$ ，然后通过这个 $\phi(\cdot)$ 得出对应的 $\kappa(\cdot,\cdot)$ 进行内积计算。然而，第二步通常是非常困难甚至完全没法做的。不过，由于第一步也是几乎无法做到，因为对于任意的数据分析其形状找到合适的映射本身就不是什么容易的事情，所以，人们通常都是“胡乱”选择映射的，所以，根本没有必要精确地找出对应于映射的那个核函数，而只需要“胡乱”选择一个核函数即可——我们知道它对应了某个映射，虽然我们不知道这个映射具体是什么。由于我们的计算只需要核函数即可，所以我们也并不关心也没有必要求出所对应的映射的具体形式。 :D</p>
<p>当然，说是“胡乱”选择，其实是夸张的说法，因为并不是任意的二元函数都可以作为核函数，所以除非某些特殊的应用中可能会构造一些特殊的核（例如用于文本分析的文本核，注意其实使用了 Kernel 进行计算之后，其实完全可以去掉原始空间是一个向量空间的假设了，只要核函数支持，原始数据可以是任意的“对象”——比如文本字符串），通常人们会从一些常用的核函数中选择（根据问题和数据的不同，选择不同的参数，实际上就是得到了不同的核函数），例如：</p>
<p>多项式核 $\kappa(x_1,x_2) = \left(\langle x_1,x_2\rangle + R\right)^d$ ，显然刚才我们举的例子是这里多项式核的一个特例（$R=1,d=2$）。虽然比较麻烦，而且没有必要，不过这个核所对应的映射实际上是可以写出来的，该空间的维度是 $\binom{m+d}{d}$ ，其中 $m$ 是原始空间的维度。<br>高斯核 $\kappa(x_1,x_2) = \exp\left(-\frac{|x_1-x_2|^2}{2\sigma^2}\right)$ ，这个核就是最开始提到过的会将原始空间映射为无穷维空间的那个家伙。不过，如果 $\sigma$ 选得很大的话，高次特征上的权重实际上衰减得非常快，所以实际上（数值上近似一下）相当于一个低维的子空间；反过来，如果 $\sigma$ 选得很小，则可以将任意的数据映射为线性可分——当然，这并不一定是好事，因为随之而来的可能是非常严重的过拟合问题。不过，总的来说，通过调控参数 $\sigma$ ，高斯核实际上具有相当高的灵活性，也是使用最广泛的核函数之一。<br>线性核 $\kappa(x_1,x_2) = \langle x_1,x_2\rangle$ ，这实际上就是原始空间中的内积。这个核存在的主要目的是使得“映射后空间中的问题”和“映射前空间中的问题”两者在形式上统一起来了。<br>最后，总结一下：对于非线性的情况，SVM 的处理方法是选择一个核函数 $\kappa(\cdot,\cdot)$ ，通过将数据映射到高维空间，来解决在原始空间中线性不可分的问题。由于核函数的优良品质，这样的非线性扩展在计算量上并没有比原来复杂多少，这一点是非常难得的。当然，这要归功于核方法——除了 SVM 之外，任何将计算表示为数据点的内积的方法，都可以使用核方法进行非线性扩展。</p>
<p>此外，略微提一下，也有不少工作试图自动构造专门针对特定数据的分布结构的核函数，感兴趣的同学可以参考，比如 NIPS 2003 的 Cluster Kernels for Semi-Supervised Learning 和 ICML 2005 的 Beyond the point cloud: from transductive to semi-supervised learning 等。</p>
<p>##<strong>Outliers</strong> </p>
<p>在最开始讨论支持向量机的时候，我们就假定，数据是线性可分的，亦即我们可以找到一个可行的超平面将数据完全分开。后来为了处理非线性数据，使用 Kernel 方法对原来的线性 SVM 进行了推广，使得非线性的的情况也能处理。虽然通过映射 $\phi(\cdot)$ 将原始数据映射到高维空间之后，能够线性分隔的概率大大增加，但是对于某些情况还是很难处理。例如可能并不是因为数据本身是非线性结构的，而只是因为数据有噪音。对于这种偏离正常位置很远的数据点，我们称之为 outlier ，在我们原来的 SVM 模型里，outlier 的存在有可能造成很大的影响，因为超平面本身就是只有少数几个 support vector 组成的，如果这些 support vector 里又存在 outlier 的话，其影响就很大了。例如下图：</p>
<center><img src="http://blog.pluskid.org/wp-content/uploads/2010/09/Optimal-Hyper-Plane-2.png" alt=""></center>

<p>用黑圈圈起来的那个蓝点是一个 outlier ，它偏离了自己原本所应该在的那个半空间，如果直接忽略掉它的话，原来的分隔超平面还是挺好的，但是由于这个 outlier 的出现，导致分隔超平面不得不被挤歪了，变成途中黑色虚线所示（这只是一个示意图，并没有严格计算精确坐标），同时 margin 也相应变小了。当然，更严重的情况是，如果这个 outlier 再往右上移动一些距离的话，我们将无法构造出能将数据分开的超平面来。</p>
<p>为了处理这种情况，SVM 允许数据点在一定程度上偏离一下超平面。例如上图中，黑色实线所对应的距离，就是该 outlier 偏离的距离，如果把它移动回来，就刚好落在原来的超平面上，而不会使得超平面发生变形了。具体来说，原来的约束条件</p>
<p>$$<br>y_i(w^Tx_i+b)\geq 1, \quad i=1,\ldots,n<br>$$</p>
<p>现在变成</p>
<p>$$<br>y_i(w^Tx_i+b)\geq 1\color{red}{-\xi_i}, \quad i=1,\ldots,n<br>$$</p>
<p>其中 $\xi_i\geq 0$ 称为松弛变量 (slack variable) ，对应数据点 $x_i$ 允许偏离的 functional margin 的量。当然，如果我们运行 $\xi_i$ 任意大的话，那任意的超平面都是符合条件的了。所以，我们在原来的目标函数后面加上一项，使得这些 $\xi_i$ 的总和也要最小：</p>
<p>$$<br>\min \frac{1}{2}|w|^2\color{red}{+C\sum_{i=1}^n \xi_i}<br>$$</p>
<p>其中 $C$ 是一个参数，用于控制目标函数中两项（“寻找 margin 最大的超平面”和“保证数据点偏差量最小”）之间的权重。注意，其中 $\xi$ 是需要优化的变量（之一），而 $C$ 是一个事先确定好的常量。完整地写出来是这个样子：</p>
<p>$$<br>\begin{aligned}<br>\min &amp; \frac{1}{2}|w|^2 + C\sum_{i=1}^n\xi_i \<br>s.t., &amp; y_i(w^Tx_i+b)\geq 1-\xi_i, i=1,\ldots,n \<br>&amp; \xi_i \geq 0, i=1,\ldots,n<br>\end{aligned}<br>$$</p>
<p>用之前的方法将限制加入到目标函数中，得到如下问题：</p>
<p>$$<br>\mathcal{L}(w,b,\xi,\alpha,r)=\frac{1}{2}|w|^2 + C\sum_{i=1}^n\xi<em>i – \sum</em>{i=1}^n\alpha_i \left(y_i(w^Tx_i+b)-1+\xi<em>i\right) – \sum</em>{i=1}^n r_i\xi_i<br>$$</p>
<p>分析方法和前面一样，转换为另一个问题之后，我们先让 $\mathcal{L}$ 针对 $w$、$b$ 和 $\xi$ 最小化：</p>
<p>$$<br>\begin{aligned}<br>\frac{\partial \mathcal{L}}{\partial w}=0 &amp;\Rightarrow w=\sum_{i=1}^n \alpha_i y_i x<em>i \<br>\frac{\partial \mathcal{L}}{\partial b} = 0 &amp;\Rightarrow \sum</em>{i=1}^n \alpha_i y_i = 0 \<br>\frac{\partial \mathcal{L}}{\partial \xi_i} = 0 &amp;\Rightarrow C-\alpha_i-r_i=0, \quad i=1,\ldots,n<br>\end{aligned}<br>$$</p>
<p>将 $w$ 带回 $\mathcal{L}$ 并化简，得到和原来一样的目标函数：</p>
<p>$$<br>\max<em>\alpha \sum</em>{i=1}^n\alpha<em>i – \frac{1}{2}\sum</em>{i,j=1}^n\alpha_i\alpha_jy_iy_j\langle x_i,x_j\rangle<br>$$</p>
<p>不过，由于我们得到 $C-\alpha_i-r_i=0$ ，而又有 $r_i\geq 0$ （作为 Lagrange multiplier 的条件），因此有 $\alpha_i\leq C$ ，所以整个 dual 问题现在写作：</p>
<p>$$<br>\begin{aligned}<br>\max<em>\alpha &amp;\sum</em>{i=1}^n\alpha<em>i – \frac{1}{2}\sum</em>{i,j=1}^n\alpha_i\alpha_jy_iy_j\langle x_i,x_j\rangle \<br>s.t., &amp;0\leq \alpha<em>i\leq C, i=1,\ldots,n \<br>&amp;\sum</em>{i=1}^n\alpha_iy_i = 0<br>\end{aligned}<br>$$</p>
<p>和之前的结果对比一下，可以看到唯一的区别就是现在 dual variable $\alpha$ 多了一个上限 $C$ 。而 Kernel 化的非线性形式也是一样的，只要把 $\langle x_i,x_j \rangle$ 换成 $\kappa(x_i,x_j)$ 即可。这样一来，一个完整的，可以处理线性和非线性并能容忍噪音和 outliers 的支持向量机才终于介绍完毕了。 :)</p>
<p>##<strong>Numerical Optimization</strong> </p>
<p>作为支持向量机系列的基本篇的最后一篇文章，我在这里打算简单地介绍一下用于优化 dual 问题的 Sequential Minimal Optimization (SMO) 方法。确确实实只是简单介绍一下，原因主要有两个：第一这类优化算法，特别是牵涉到实现细节的时候，干巴巴地讲算法不太好玩，有时候讲出来每个人实现得结果还不一样，提一下方法，再结合实际的实现代码的话，应该会更加明了，而且也能看出理论和实践之间的差别；另外（其实这个是主要原因）我自己对这一块也确实不太懂。 :p</p>
<p>先回忆一下我们之前得出的要求解的 dual 问题：</p>
<p>$$<br>\begin{aligned}<br>\max<em>\alpha &amp;\sum</em>{i=1}^n\alpha<em>i – \frac{1}{2}\sum</em>{i,j=1}^n\alpha_i\alpha_jy_iy_j\kappa( x_i,x_j) \<br>s.t., &amp;0\leq \alpha<em>i\leq C, i=1,\ldots,n \<br>&amp;\sum</em>{i=1}^n\alpha_iy_i = 0<br>\end{aligned}<br>$$</p>
<p>对于变量 $\alpha$ 来说，这是一个 quadratic 函数。通常对于优化问题，我们没有办法的时候就会想到最笨的办法——Gradient Descent ，也就是梯度下降。注意我们这里的问题是要求最大值，只要在前面加上一个负号就可以转化为求最小值，所以 Gradient Descent 和 Gradient Ascend 并没有什么本质的区别，其基本思想直观上来说就是：梯度是函数值增幅最大的方向，因此只要沿着梯度的反方向走，就能使得函数值减小得越大，从而期望迅速达到最小值。当然普通的 Gradient Descent 并不能保证达到最小值，因为很有可能陷入一个局部极小值。不过对于 quadratic 问题，极值只有一个，所以是没有局部极值的问题。</p>
<p>另外还有一种叫做 Coordinate Descend 的变种，它每次只选择一个维度，例如 $\alpha=(\alpha_1,\ldots,\alpha_n)$ ，它每次选取 $\alpha_i$ 为变量，而将 $\alpha<em>1,\ldots,\alpha</em>{i-1},\alpha_{i+1},\ldots,\alpha_n$ 都看成是常量，从而原始的问题在这一步变成一个一元函数，然后针对这个一元函数求最小值，如此反复轮换不同的维度进行迭代。Coordinate Descend 的主要用处在于那些原本很复杂，但是如果只限制在一维的情况下则变得很简单甚至可以直接求极值的情况，例如我们这里的问题，暂且不管约束条件，如果只看目标函数的话，当 $\alpha$ 只有一个分量是变量的时候，这就是一个普通的一元二次函数的极值问题，初中生也会做，带入公式即可。</p>
<p>然而这里还有一个问题就是约束条件的存在，其实如果没有约束条件的话，本身就是一个多元的 quadratic 问题，也是很好求解的。但是有了约束条件，结果让 Coordinate Descend 变得很尴尬了：比如我们假设 $\alpha_1$ 是变量，而 $\alpha_2,\ldots,\alpha<em>n$ 是固定值的话，那么其实没有什么好优化的了，直接根据第二个约束条件 $\sum</em>{i=1}^n\alpha_iy_i = 0$ ，$\alpha_1$ 的值立即就可以定下来——事实上，迭代每个坐标维度，最后发现优化根本进行不下去，因为迭代了一轮之后会发现根本没有任何进展，一切都停留在初始值。</p>
<p>所以 Sequential Minimal Optimization (SMO) 一次选取了两个坐标维度来进行优化。例如（不失一般性），我们假设现在选取 $\alpha_1$ 和 $\alpha_2$ 为变量，其余为常量，则根据约束条件我们有：</p>
<p>$$<br>\sum_{i=1}^n\alpha_iy_i = 0 \Rightarrow \alpha_2=\frac{1}{y<em>2}\left(\sum</em>{i=3}^n\alpha_iy_i-\alpha_1y_1\right) \triangleq y_2\left(K-\alpha_1y_1\right)<br>$$</p>
<p>其中那个从 3 到 n 的作和由于都是常量，我们统一记作 $K$ ，然后由于 $y\in{-1,+1}$ ，所以 $y_2$ 和 $1/y_2$ 是完全一样的，所以可以拿到分子上来。将这个式子带入原来的目标函数中，可以消去 $\alpha_2$ ，从而变成一个一元二次函数，具体展开的形式我就不写了，总之现在变成了一个非常简单的问题：带区间约束的一元二次函数极值问题——这个也是初中就学过求解方法的。唯一需要注意一点的就是这里的约束条件，一个就是 $\alpha_1$ 本身需要满足 $0\leq\alpha_1\leq C$ ，然后由于 $\alpha_2$ 也要满足同样的约束，即：</p>
<p>$$<br>0\leq y_2 (K-\alpha_1y_1) \leq C<br>$$</p>
<p>也可以得到 $\alpha_1$ 的一个可行区间，同 $[0,C]$ 交集即可得到最终的可行区间。这个问题可以从图中得到一个直观的感觉。原本关于 $\alpha_1$ 和 $\alpha_2$ 的区间限制构成途中绿色的的方块，而另一个约束条件 $y_1\alpha_1 + y_2\alpha_2 = K$ 实际上表示一条直线，两个集合的交集即是途中红颜色的线段，投影到 $\alpha_1$ 轴上所对应的区间即是 $\alpha_1$ 的取值范围，在这个区间内求二次函数的最大值即可完成 SMO 的一步迭代。</p>
<center><img src="http://blog.pluskid.org/wp-content/uploads/2010/09/interval.png" alt=""></center>

<p>同 Coordinate Descent 一样，SMO 也会选取不同的两个 coordinate 维度进行优化，可以看出由于每一个迭代步骤实际上是一个可以直接求解的一元二次函数极值问题，所以求解非常高效。此外，SMO 也并不是依次或者随机地选取两个坐标维度，而是有一些启发式的策略来选取最优的两个坐标维度，具体的选取方法（和其他的一些细节），可以参见 John C. Platt 的那篇论文 Fast Training of Support Vector Machines Using Sequential Minimal Optimization 。关于 SMO ，我就不再多说了。如果你对研究实际的代码比较感兴趣，可以去看 LibSVM 的实现，当然，它那个也许已经不是原来版本的 SMO 了，因为本来 SVM 的优化就是一个有许多研究工作的领域，在那些主要的优化方法之上，也有各种改进的办法或者全新的算法提出来。</p>
<p>除了 LibSVM 之外，另外一个流行的实现 SVMlight 似乎是用了另一种优化方法，具体可以参考一下它相关的论文 Making large-Scale SVM Learning Practical 。</p>
<p>此外，虽然我们从 dual 问题的推导中得出了许多 SVM 的优良性质，但是 SVM 的数值优化（即使是非线性的版本）其实并不一定需要转化为 dual 问题来完成的，具体做法我并不清楚，不过这方面的文章也不少，比如 2007 年 Neural Computation 的一篇 Training a support vector machine in the primal 。如果感兴趣可以参考一下。 :)</p>
<p>#<strong>Duality</strong><br>简单来说，对于任意一个带约束的优化都可以写成这样的形式：</p>
<p>$$<br>\begin{aligned}<br>\min&amp;f_0(x) \<br>s.t. &amp;f_i(x)\leq 0, \quad i=1,\ldots,m\<br>&amp;h_i(x)=0, \quad i=1,\ldots,p<br>\end{aligned}<br>$$</p>
<p>形式统一能够简化推导过程中不必要的复杂性。其他的形式都可以归约到这样的标准形式，例如一个 $\max f(x)$ 可以转化为 $\min -f(x)$ 等。假如 $f_0,f_1,\ldots,f_m$ 全都是凸函数，并且 $h_1,\ldots,h_p$ 全都是仿射函数（就是形如 $Ax+b$ 的形式），那么这个问题就叫做凸优化（Convex Optimization）问题。凸优化问题有许多优良的性质，例如它的极值是唯一的。不过，这里我们并没有假定需要处理的优化问题是一个凸优化问题。</p>
<p>虽然约束条件能够帮助我们减小搜索空间，但是如果约束条件本身就是比较复杂的形式的话，其实是一件很让人头痛的问题，为此我们希望把带约束的优化问题转化为无约束的优化问题。为此，我们定义 Lagrangian 如下：</p>
<p>$$<br>L(x,\lambda,\nu)=f<em>0(x)+\sum</em>{i=1}^m\lambda_if<em>i(x)+\sum</em>{i=1}^p\nu_ih_i(x)<br>$$</p>
<p>它通过一些系数把约束条件和目标函数结合在了一起。当然 Lagrangian 本身并不好玩，现在让我们来让他针对 $\lambda$ 和 $\nu$ 最大化，令：</p>
<p>$$<br>z(x)=\max_{\lambda\succeq 0, \nu}L(x,\lambda,\nu)<br>$$</p>
<p>这里 $\lambda\succeq 0$ 理解为向量 $\lambda$ 的每一个元素都非负即可。这个函数 $z(x)$ 对于满足原始问题约束条件的那些 $x$ 来说，其值等于 $f_0(x)$ ，这很容易验证，因为满足约束条件的 $x$ 会使得 $h_i(x)=0$ ，因此最后一项消掉了，而 $f_i(x)\leq 0$ ，并且我们要求了 $\lambda \succeq 0$ ，因此 $\lambda_if_i(x)\leq 0$ ，所以最大值只能在它们都取零的时候得到，这个时候就只剩下 $f_0(x)$ 了。因此，对于满足约束条件的那些 $x$ 来说，$f_0(x)=z(x)$ 。这样一来，原始的带约束的优化问题其实等价于如下的无约束优化问题：</p>
<p>$$<br>\min_x z(x)<br>$$</p>
<p>因为如果原始问题有最优值，那么肯定是在满足约束条件的某个 $x^* $ 取得，而对于所有满足约束条件的 $x$ ，$z(x)$ 和 $f_0(x)$ 都是相等的。至于那些不满足约束条件的 $x$ ，原始问题是无法取到的，否则极值问题无解。很容易验证对于这些不满足约束条件的 $x$ 有 $z(x)=\infty$，这也和原始问题是一致的，因为求最小值得到无穷大可以和“无解”看作是相容的。</p>
<p>到这里，我们成功把带约束问题转化为了无约束问题，不过这其实只是一个形式上的重写，并没有什么本质上的改变。我们只是把原来的问题通过 Lagrangian 写作了如下形式：</p>
<p>$$<br>\min<em>x\ \max</em>{\lambda\succeq 0, \nu} L(x, \lambda, \nu)<br>$$</p>
<p>这个问题（或者说原始的带约束的形式）称作 primal problem 。相对应的还有一个 dual problem ，其形式非常类似，只是把 $\min$ 和 $\max$ 交换了一下：</p>
<p>$$<br>\max_{\lambda\succeq 0, \nu}\ \min_x L(x, \lambda, \nu)<br>$$</p>
<p>交换之后的 dual problem 和原来的 primal problem 并不相等，直观地，我们可以这样来理解：胖子中最瘦的那个都比瘦骨精中最胖的那个要胖。当然这是很不严格的说法，而且扣字眼的话可以纠缠不休，所以我们还是来看严格数学描述。和刚才的 $z(x)$ 类似，我们也用一个记号来表示内层的这个函数，记：</p>
<p>$$<br>g(\lambda,\nu) = \min_x L(x, \lambda, \nu)<br>$$</p>
<p>并称 $g(\lambda,\nu)$ 为 Lagrange dual function （不要和 $L$ 的 Lagrangian 混淆了）。$g$ 有一个很好的性质就是它是 primal problem 的一个下界。换句话说，如果 primal problem 的最小值记为 $p^* $ ，那么对于所有的 $\lambda \succeq 0$ 和 $\nu$ ，我们有：</p>
<p>$$<br>g(\lambda,\nu)\leq p^*<br>$$</p>
<p>因为对于极值点（实际上包括所有满足约束条件的点）$x^* $，注意到 $\lambda\succeq 0$ ，我们总是有</p>
<p>$$<br>\sum_{i=1}^m\lambda_if<em>i(x^* )+\sum</em>{i=1}^p\nu_ih_i(x^* )\leq 0<br>$$</p>
<p>因此</p>
<p>$$<br>L(x^<em> ,\lambda,\nu)=f_0(x^</em> )+\sum_{i=1}^m\lambda_if<em>i(x^* )+\sum</em>{i=1}^p\nu_ih_i(x^<em> )\leq f_0(x^</em> )<br>$$</p>
<p>于是</p>
<p>$$<br>g(\lambda,\nu)=\min_x L(x,\lambda,\nu)\leq L(x^<em> ,\lambda,\nu)\leq f_0(x^</em> )=p^*<br>$$</p>
<p>这样一来就确定了 $g$ 的下界性质，于是</p>
<p>$$<br>\max_{\lambda\succeq 0,\nu}g(\lambda,\nu)<br>$$</p>
<p>实际上就是最大的下界。这是很自然的，因为得到下界之后，我们自然地就希望得到最好的下界，也就是最大的那一个——因为它离我们要逼近的值最近呀。记 dual problem 的最优值为 $d^* $ 的话，根据上面的推导，我们就得到了如下性质：</p>
<p>$$<br>d^<em> \leq p^</em><br>$$</p>
<p>这个性质叫做 weak duality ，对于所有的优化问题都成立。其中 $p^<em> -d^</em> $ 被称作 duality gap 。需要注意的是，无论 primal problem 是什么形式，dual problem 总是一个 convex optimization 的问题——它的极值是唯一的（如果存在的话），并且有现成的软件包可以对凸优化问题进行求解（虽然求解 general 的 convex optimization 实际上是很慢并且只能求解规模较小的问题的）。这样一来，对于那些难以求解的 primal problem （比如，甚至可以是 NP 问题），我们可以通过找出它的 dual problem ，通过优化这个 dual problem 来得到原始问题的一个下界估计。或者说我们甚至都不用去优化这个 dual problem ，而是（通过某些方法，例如随机）选取一些 $\lambda\succeq 0$ 和 $\nu$ ，带到 $g(\lambda,\nu)$ 中，这样也会得到一些下界（只不过不一定是最大的那个下界而已）。当然要选 $\lambda$ 和 $\nu$ 也并不是总是“随机选”那么容易，根据具体问题，有时候选出来的 $\lambda$ 和 $\nu$ 带入 $g$ 会得到 $-\infty$ ，这虽然是一个完全合法的下界，然而却并没有给我们带来任何有用的信息。</p>
<p>故事到这里还没有结束，既然有 weak duality ，显然就会有 strong duality 。所谓 strong duality ，就是</p>
<p>$$<br>d^<em> =p^</em><br>$$</p>
<p>这是一个很好的性质，strong duality 成立的情况下，我们可以通过求解 dual problem 来优化 primal problem ，在 SVM 中我们就是这样做的。当然并不是所有的问题都能满足 strong duality ，在讲 SVM 的时候我们直接假定了 strong duality 的成立，这里我们就来提一下 strong duality 成立的条件。不过，这个问题如果要讲清楚，估计写一本书都不够，应该也有不少专门做优化方面的人在研究这相关的问题吧，我没有兴趣（当然也没有精力和能力）来做一个完整的介绍，相信大家也没有兴趣来看这样的东西——否则你肯定是专门研究优化方面的问题的了，此时你肯定比我懂得更多，也就不用看我写的介绍啦。 :p</p>
<p>所以，这里我们就简要地介绍一下 Slater 条件和 KKT 条件。Slater 条件是指存在严格满足约束条件的点 $x$ ，这里的“严格”是指 $f_i(x)\leq 0$ 中的“小于或等于号”要严格取到“小于号”，亦即，存在 $x$ 满足</p>
<p>$$<br>\begin{aligned}<br>f_i(x)&lt;0&amp;\quad i=1,\ldots,m\ h_i(x)=0&amp;\quad i=1,\ldots,p \end{aligned} $$ 我们有：如果原始问题是 Convex 的并且满足 Slater 条件的话，那么 strong duality 成立。需要注意的是，这里只是指出了 strong duality 成立的一种情况，而并不是唯一情况。例如，对于某些非 convex optimization 的问题，strong duality 也成立。这里我们不妨回顾一下 SVM 的 primal problem ，那是一个 convex optimization 问题（QP 是凸优化问题的一种特殊情况），而 Slater 条件实际上在这里就等价于是存在这样的一个超平面将数据分隔开来，亦即是“数据是可分的”。当数据不可分是，strong duality 不能成立，不过，这个时候我们寻找分隔平面这个问题本身也就是没有意义的了，至于我们如何通过把数据映射到特征空间中来解决不可分的问题，这个当时已经介绍过了，这里就不多说了。</p>
<p>让我们回到 duality 的话题。来看看 strong duality 成立的时候的一些性质。假设 $x^<em>  $ 和 $(\lambda^</em>  ,\nu^<em>  )$ 分别是 primal problem 和 dual problem 的极值点，相应的极值为 $p^</em>  $ 和 $d^<em>  $ ，首先 $p^</em>  =d^*  $ ，此时我们可以得到</p>
<p>$$<br>\begin{aligned}<br>f_0(x^<em>  )&amp;=g(\lambda^</em>  ,\nu^<em> )\<br>&amp;=\min_x\left(f<em>0(x)+\sum</em>{i=1}^m\lambda_i^</em> f<em>i(x)+\sum</em>{i=1}^p\nu_i^<em> h_i(x)\right)\<br>&amp;\leq f_0(x^</em>  )+\sum_{i=1}^m\lambda_i^<em>  f_i(x^</em>  )+\sum_{i=1}^p\nu_i^<em>  h_i(x^</em>  ) \<br>&amp;\leq f_0(x^*  )<br>\end{aligned}<br>$$</p>
<p>由于两头是相等的，所以这一系列的式子里的不等号全部都可以换成等号。根据第一个不等号我们可以得到 $x^<em>  $ 是 $L(x,\lambda^</em>  ,\nu^<em>  )$ 的一个极值点，由此可以知道 $L(x,\lambda^</em>  ,\nu^<em>  )$ 在 $x^</em> $ 处的梯度应该等于 0 ，亦即：</p>
<p>$$<br>\nabla f<em>0(x^* )+\sum</em>{i=1}^m\lambda_i^<em>  \nabla f_i(x^</em> )+\sum_{i=1}^p\nu_i^<em>  \nabla h_i(x^</em> )=0<br>$$</p>
<p>此外，由第二个不等式，又显然 $\lambda_i^<em> f_i(x^</em> )$ 都是非正的，因此我们可以得到</p>
<p>$$<br>\lambda_i^<em> f_i(x^</em> )=0,\quad i=1,\ldots,m<br>$$</p>
<p>这个条件叫做 complementary slackness 。显然，如果 $\lambda_i^<em> &gt;0$，那么必定有 $f_i(x^</em> )=0$ ；反过来，如果 $f_i(x^<em> )&lt;0$ 那么可以得到 $\lambda_i^</em> =0$ 。这个条件正是我们在介绍支持向量的文章末尾时用来证明那些非支持向量（对应于 $f_i(x^* )&lt;0$）所对应的系数 $\alpha_i$ （在本文里对应 $\lambda_i$）是为零的。 :) 再将其他一些显而易见的条件写到一起，就是传说中的KKT(Karush-Kuhn-Tucker) 条件： </p>
<p>$$<br>\begin{aligned}<br>&amp;f_i(x^<em> )\leq 0,\quad i=1,\ldots,m\<br>&amp;h_i(x^</em> )=0,\quad i=1,\ldots,p\<br>&amp;\lambda_i^<em> \geq 0,\quad i=1,\ldots,m\<br>&amp;\lambda_i^</em> f_i(x^<em> )=0,\quad i=1,\ldots,m\<br>\end{aligned} \<br>\textstyle\nabla f_0(x^</em> )+\sum_{i=1}^m\lambda_i^<em> \nabla f_i(x^</em> )+\sum_{i=1}^p\nu_i^<em>  \nabla h_i(x^</em> )=0<br>$$</p>
<p>任何满足 strong duality （不一定要求是通过 Slater 条件得到，也不一定要求是凸优化问题）的问题都满足 KKT 条件，换句话说，这是 strong duality 的一个必要条件。不过，当原始问题是凸优化问题的时候（当然还要求一应函数是可微的，否则 KKT 条件的最后一个式子就没有意义了），KKT 就可以升级为充要条件。换句话说，如果 primal problem 是一个凸优化问题，且存在 $\tilde{x}$ 和 $(\tilde{\lambda},\tilde{\nu})$ 满足 KKT 条件，那么它们分别是 primal problem 和 dual problem 的极值点并且 strong duality 成立。 其证明也比较简单，首先 primal problem 是凸优化问题的话，$g(\lambda,\nu)=\min_x L(x,\lambda,\nu)$ 的求解对每一组固定的 $(\lambda,\nu)$ 来说也是一个凸优化问题，由 KKT 条件的最后一个式子，知道 $\tilde{x}$ 是 $\min_x L(x,\tilde{\lambda},\tilde{\nu})$ 的极值点（如果不是凸优化问题，则不一定能推出来），亦即： $$ \begin{aligned} g(\tilde{\lambda},\tilde{\nu}) &amp;= \min_x L(x,\tilde{\lambda},\tilde{\nu}) \ &amp;= L(\tilde{x},\tilde{\lambda},\tilde{\nu}) \ &amp; = f<em>0(\tilde{x})+\sum</em>{i=1}^m\tilde{\lambda}_i^<em> f<em>i(\tilde{x})+\sum</em>{i=1}^p\tilde{\nu_i}^</em> h_i(\tilde{x}) \ &amp;= f_0(\tilde{x}) \end{aligned} $$ 最后一个式子是根据 KKT 条件的第二和第四个条件得到。由于 $g$ 是 $f_0$ 的下界，这样一来，就证明了 duality gap 为零，也就是说，strong duality 成立。 到此为止，做一下总结。我们简要地介绍了 duality 的概念，基本上没有给什么具体的例子。不过由于内容比较多，为了避免文章超长，就挑了一些重点讲了一下。总的来说，一个优化问题，通过求出它的 dual problem ，在只有 weak duality 成立的情况下，我们至少可以得到原始问题的一个下界。而如果 strong duality 成立，则可以直接求解 dual problem 来解决原始问题，就如同经典的 SVM 的求解过程一样。有可能 dual problem 比 primal problem 更容易求解，或者 dual problem 有一些优良的结构（例如 SVM 中通过 dual problem 我们可以将问题表示成数据的内积形式从而使得 kernel trick 的应用成为可能）。此外，还有一些情况会同时求解 dual 和 primal problem ，比如在迭代求解的过程中，通过判断 duality gap 的大小，可以得出一个有效的迭代停止条件。</p>
<p>##<strong>Kernel II</strong><br>在之前我们介绍了如何用 Kernel 方法来将线性 SVM 进行推广以使其能够处理非线性的情况，那里用到的方法就是通过一个非线性映射 $\phi(\cdot)$ 将原始数据进行映射，使得原来的非线性问题在映射之后的空间中变成线性的问题。然后我们利用核函数来简化计算，使得这样的方法在实际中变得可行。不过，从线性到非线性的推广我们并没有把 SVM 的式子从头推导一遍，而只是直接把最终得到的分类函数</p>
<p>$$<br>f(x) = \sum_{i=1}^n\alpha_i y_i \langle x_i, x\rangle + b<br>$$</p>
<center><img src="http://blog.pluskid.org/wp-content/uploads/2011/01/infinity.png" alt=""></center>

<p>中的内积换成了映射后的空间中的内积，并进一步带入了核函数进行计算。如果映射过后的空间是有限维的，那么这样的做法是可行的，因为之前的推导过程会一模一样，只是特征空间的维度变化了而已，相当于做了一些预处理。但是如果映射后的空间是无限维的，还能不能这么做呢？答案当然是能，因为我们已经在这么做了嘛！ :) 但是理由却并不是理所当然的，从有限到无限的推广许多地方都可以“直观地”类比，但是这样的直观性仍然需要严格的数学背景来支持，否则就会在一些微妙的地方出现一些奇怪的“悖论”（例如比较经典的芝诺的那些悖论）。当然这是一个很大的坑，没法填，所以这次我们只是来浮光掠影地看一看核方法背后的故事。</p>
<p>回忆一下原来我们做的非线性映射 $\phi$ ，它将原始特征空间中的数据点映射到另一个高维空间中，之前我们没有提过，其实这个高维空间在这里有一个华丽的名字——“再生核希尔伯特空间 (Reproducing Kernel Hilbert Space, RKHS)”。“再生核”就是指的我们用于计算内积的核函数，再说“再生”之前，我们先来简单地介绍一下 Hilbert Space ，它其实是欧氏空间的一个推广。首先从基本的向量空间开始，空间中的点具有加法和数乘的操作，在这个向量空间上定义一个内积操作，于是空间将升级为内积空间。根据内积可以定义一个范数：</p>
<p>$$<br>|x|^2=\langle x, x\rangle<br>$$</p>
<p>从而成为一个赋范向量空间。范数可以用于定义一个度量</p>
<p>$$<br>d(x_1,x_2) = |x_1-x_2|<br>$$</p>
<p>从而成为一个度量空间。如果这样的空间在这个度量下是完备的，那么这个空间叫做 Hilbert Space 。简单地来说，Hilbert Space 就是完备的内积空间。最简单的例子就是欧氏空间 $\mathbb{R}^m$ ，这是一个 $m$ 维的 Hilbert Space ，无穷维的例子比如是区间 $[a,b]$ 上的连续函数所组成的空间，并使用如下的内积定义</p>
<p>$$<br>\langle f_1,f_2\rangle = \int_a^bf_1(t)f_2(t)dt<br>$$</p>
<p>我们这里的 RKHS 就是一个函数空间。实际上，在这里我们有一个很有用的性质，就是维度相同的 Hilbert Space 是互相同构的——也就是说空间的各种结构（包括内积、范数、度量和向量运算等）都可以在不同的空间之间转换的时候得到保持。有了这样的性质，就可以让我们不用去关心 RKHS 中的点到底是什么。</p>
<p>将映射记为 $\phi:\mathcal{X}\rightarrow \mathcal{H}$ ，这里 $\mathcal{H}$ 表示 RKHS，用 $f$ 表示里面的元素 ；而 $\mathcal{X}$ 是原始特征空间，这里我们甚至不需要要求原始空间必须要是一个欧氏空间或者向量空间（这也是核方法的优点之一），用 $x$ 表示里面的点。由于刚才说了 $\mathcal{H}$ 中点的本质是什么对于我们的计算不会产生影响，所以我们可以人为地认为这些点“是什么”——更确切地说，我们认为（或者说定义） $\mathcal{H}$ 中的点是定义在 $\mathcal{X}$ 上的函数，在一定的条件下（详见 N. Aronszajn, Theory of Reproducing Kernels），我们可以找到对应于这个 Hilbert Space 的一个（唯一的）再生核函数（Reproducing Kernel） $K: \mathcal{X}\times\mathcal{X}\rightarrow \mathbb{R}$ （这里只考虑实函数），满足如下两条性质：</p>
<p>对于任意固定的 $x_0\in\mathcal{X}$ ，$K(x,x_0)$ 作为 $x$ 的函数属于我们的函数空间 $\mathcal{H}$ 。<br>对于任意 $x\in\mathcal{X}$ 和 $f(\cdot)\in\mathcal{H}$ ，我们有 $f(x) = \langle f(\cdot),K(\cdot,x)\rangle$ 。<br>其中第二条性质就叫做 reproducing property ，也是“再生核”名字的来源。至于字面上为什么这么叫，我也不清楚。也许是说元素 $x$ 经过 kernel 映射之后，由内积一乘，又给冒出来了 -.-bb 。有了这个 kernel 之后，我们可以很自然地把映射 $\phi$ 定义为：</p>
<p>$$<br>\phi(x) = K(\cdot,x)<br>$$</p>
<p>由核的再生性质，我们之前的用于计算 $\mathcal{H}$ 中内积的 kernel trick 也自然成立了：</p>
<p>$$<br>\langle \phi(x_1),\phi(x_2)\rangle = \langle K(\cdot,x_1),K(\cdot,x_2)\rangle = K(x_1,x_2)<br>$$</p>
<p>再生核有很多很好的性质，比如正定性（在线性代数里这样的性质通常称为“半正定”），也就是说对任意 $x_1,\ldots,x_n\in\mathcal{X}$ 和 $\xi_1,\ldots,\xi_n\in\mathbb{R}$ ，都有</p>
<p>$$<br>\sum_{i,j=1}^nK(x_i,x_j)\xi_i\xi_j \geq 0<br>$$</p>
<p>这是很好证明的，按照核函数的再生性质写成刚才的内积形式，然后把系数拿到内积里面去，上面那个式子就等于 $|\sum_{i=1}^n\xi_iK(\cdot,x_i)|^2$ ，根据范数的性质，也就非负了。</p>
<p>到这里，铺垫已经够多了，于是让我们回到 SVM ，这次我们不是直接偷工减料在最终得到的分类函数上做手脚，而是回到线性 SVM 的最初推导。当然，第一步我们要用刚才定义的映射 $\phi$ 将数据从原始空间 $\mathcal{X}$ 映射到 RKHS $\mathcal{H}$ 中，简单起见，我们用 $f_i(\cdot)$ 来表示 $K(\cdot,x_i)$。</p>
<p>和以前一样，我们使用一个线性超平面来分隔两类不同的点，并且我们假设经过非线性映射到 $\mathcal{H}$ 中之后数据已经是线性可分的了。这个线性超平面由一个线性函数来表示。这里需要再明确一下线性函数的概念，简单的说，如果 $x_1$、$x_2$ 是向量，$\alpha_1$、$\alpha_2$ 是标量，那么线性函数应该满足</p>
<p>$$<br>\mathcal{F}(\alpha_1 x_1+\alpha_2 x_2) = \alpha_1 \mathcal{F}(x_1) + \alpha_2 \mathcal{F}(x_2)<br>$$</p>
<p>在这里，由于我们讨论的空间 $\mathcal{H}$ 中的元素本身就是函数，因此我们把 $\mathcal{H}$ 上的函数改称“泛函 (functional)”。根据 Riesz Representation 定理，Hilbert Space 中的任意一个线性泛函 $\mathcal{F}$ ，都有一个 $f_\mathcal{F}\in\mathcal{H}$ ，使得</p>
<p>$$<br>\mathcal{F}(f) = \langle f,f_\mathcal{F}\rangle,\quad \forall f\in\mathcal{H}<br>$$</p>
<p>换句话说，线性函数可以由向量内积表示，这和我们熟知的有限维欧氏空间中是一样的。只是要表示超平面还得再加上一个截距 $b$</p>
<p>$$<br>\mathcal{F}(f) = \langle f,g\rangle + b<br>$$</p>
<p>这个样子的函数（泛函）严格来说称作仿射函数（泛函）。同我们在第一篇中类似，我们可以定义 margin ，得到 geometrical margin 为</p>
<p>$$<br>\gamma = \frac{y(\langle f,g\rangle +b)}{|g|}<br>$$</p>
<p>类似于原来的推导，我们最终会得到一个如下的目标函数</p>
<p>$$<br>\min \frac{1}{2}|g|^2\quad s.t., y_i(\langle f_i,g\rangle+b)\geq 1, i=1,\ldots,n<br>$$</p>
<p>形式上和以前一样，只是把 $x_i$ 换成了 $f_i$ ，$w$ 换成了 $g$ ，但是现在我们要求的参数 $g$ 是在 Hilbert Space 中，特别当 $\mathcal{H}$ 是无穷维的时候，是没有办法直接使用数值方法来求解的。即使可以转到 dual 优化推导，但是里面涉及到对无穷维向量的求导之类的问题，我还不知道是不是能直接推广。不过幸运的是，我们在这里可以再把问题转化到有限维空间中。</p>
<p>这需要借助一个叫做 Representer Theorem 的定理，该定理说明，上面这个目标函数（还包括很大一类其他的目标函数）的最优解 $g^* $ 可以写成如下的形式：</p>
<p>$$<br>g^* =\sum_{i=1}^n a_i f_i<br>$$</p>
<p>换句话说，可以由这 $n$ 个训练数据（有限集）张成。定理的证明是很简单的，记 $\mathcal{H}_0$ 为 ${f_1,\ldots,f_n}$ 张成的子空间，其正交补记为 $\mathcal{H}_0^\bot$ ，则任意的 $f\in\mathcal{H}$ 都可以唯一地表示成 $f=f_0+f_0^\bot$ ，其中 $f_0\in\mathcal{H}_0$、$f_0^\bot \in\mathcal{H}_0^\bot$ ，因此</p>
<p>$$<br>g^<em>  = g^</em> _0 + g^{* \bot}_0<br>$$</p>
<p>由于 $g^{* \bot}_0$ 垂直于 $f_1,\ldots,f_n$ ，因此</p>
<p>$$<br>\langle f_i,g^<em> \rangle = \langle f_i,g_0^</em>  + g_0^{<em> \bot}\rangle = \langle f_i,g_0^</em> \rangle + 0<br>$$</p>
<p>因此，$g_0^{* \bot}$ 部分的取值对于目标函数中的约束条件并不产生影响，可以任意定。另一方面，考虑目标函数本身，我们有</p>
<p>$$<br>|g^<em> |^2 = |g_0^</em>  + g_0^{<em> \bot}|^2 = |g_0^</em> |^2 + |g_0^{* \bot}|^2<br>$$</p>
<p>最后一个等式是由于两者相互垂直而得到的（也就是勾股定理的推广啦），得到这个形式之后，再注意到我们是希望最小化 $|g^<em> |^2$ ，其中 $g_0^{</em> \bot}$ 是可以任意取值的，而范数 $|g_0^{<em> \bot}|^2$ 又是非负的，所以在最小值的时候我们必定有 $|g_0^{</em> \bot}|^2=0$ ，从而 $g_0^{<em> \bot}=0$ ，也就证明了 $g^</em> \in\mathcal{H}_0$ 。</p>
<p>这样一来，问题就从在一个无穷维的 Hilbert Space 中找一个最优的 $g^<em> $ 转化为了在一个 $n$ 维的欧氏空间中找一个最优的系数 $\mathbf{a}^</em> $ ，又回到了我们熟悉的问题，目标函数也变成了下面的样子：</p>
<p>$$<br>\begin{aligned}<br>\frac{1}{2}|g|^2 &amp;= \frac{1}{2}\left|\sum_{i=1}^na_if<em>i\right|^2 \<br>&amp;= \frac{1}{2}\sum</em>{i,j=1}^na_ia_jK(x_i,x_j) \<br>&amp;= \frac{1}{2}\mathbf{a}^TK\mathbf{a}<br>\end{aligned}<br>$$</p>
<p>这里矩阵 $(K)_{ij}=K(x_i,x_j)$ 就是 Kernel Gram Matrix 。而约束条件也可以相应地写成</p>
<p>$$<br>\begin{aligned}<br>1 &amp;\leq y_i(\langle f_i,g\rangle + b) \<br>&amp;= y_i(\langle f<em>i,\sum</em>{j=1}^na_jf_j\rangle +b) \<br>&amp;= y_i(\mathbf{a}^TK_i + b)<br>\end{aligned}<br>$$</p>
<p>这里 $i=1,\ldots,n$ ，而 $K_i$ 表示矩阵 $K$ 的第 $i$ 列。所以回到了最初的线性 SVM 的 Quadratic Programming 问题。当然，形式上有一些差别，另外，原来的线性 SVM 的问题的维度是原始数据空间 $\mathcal{X}$ 的维度，而这里的问题维度是等于数据点的个数 $n$ ，这是 RKHS $\mathcal{H}$ 的一个子空间 $\mathcal{H}_0$ 。此外，原来的线性 SVM 不能处理 $\mathcal{X}$ 中的非线性问题，但是现在经过非线性映射之后，（理想情况下）数据应该变得线性可分了。当然，即使不能完全线性可分，我们也可以使用之前说过的 slack variable 的方法来放松约束。而问题的数值求解，也和以前类似，一方面可以直接使用二次优化的包求解，另一方面则可以通过 dual 优化的方式来完成——得到的结果应该跟我们之前偷懒直接在最终结果上把内积进行替换所得的结果是一样的。</p>
<p>最后稍微补充一下：在刚才的介绍中我们看起来好像是先确定了 RKHS 之后再找出对应的再生核的，但是在实际中，通常是先设计出一个核函数（或者说通常都是直接使用几个常见的核函数），然后对应的 RKHS 就自然地确定下来了。关于 RKHS 还有许多的内容，但是没有办法全部讲了。在传统的 Kernel 方法应用中，通常只要注意到是否可以全部表示为内积运算就可以尝试使用 kernel 方法了，许多常见的算法（例如 Least-Square Regression 、PCA 等）都是可以用核方法来扩展的，在这里 Representer Theorem 将会是重要的一环。</p>
<p>除此之外，进来还有不少在 RKHS 里衡量统计独立性的工作，又不是只是像传统的 kernel trick 那么简单了，说明 RKHS 还是包含了不少有趣的话题的。 :)</p>

      
    </div>

    <div>
      
        
      
    </div>

    <div>
      
        

      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Machine-Learning/" rel="tag">#Machine Learning</a>
          
        </div>
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
          </div>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2016/04/22/sciencism-and-hunmanism/" rel="prev" title="科学与人文">
                科学与人文 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            Inhaltsverzeichnis
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            Übersicht
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.gif"
               alt="starrysky" />
          <p class="site-author-name" itemprop="name">starrysky</p>
          <p class="site-description motion-element" itemprop="description"></p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/">
              <span class="site-state-item-count">2</span>
              <span class="site-state-item-name">Artikel</span>
            </a>
          </div>

          

          
            <div class="site-state-item site-state-tags">
              
                <span class="site-state-item-count">2</span>
                <span class="site-state-item-name">Tags</span>
              
            </div>
          

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/lixinyancici" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://www.zhihu.com/people/li-xin-yan-73" target="_blank" title="ZhiHu">
                  
                    <i class="fa fa-fw fa-globe"></i>
                  
                  ZhiHu
                </a>
              </span>
            
          
        </div>

        
        

        
        
          <div class="links-of-blogroll motion-element links-of-blogroll-inline">
            <div class="links-of-blogroll-title">
              <i class="fa  fa-fw fa-globe"></i>
              
            </div>
            <ul class="links-of-blogroll-list">
              
                <li class="links-of-blogroll-item">
                  <a href="http://blog.sciencenet.cn/u/zmpenguestc" title="老马迷图" target="_blank">老马迷图</a>
                </li>
              
            </ul>
          </div>
        

      </section>

      
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">
            
              
            
            
              <p class="post-toc-empty">Dieser Artikel hat kein Inhaltsverzeichnis</p>
            
          </div>
        </section>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy;  2016 - 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">starrysky</span>
</div>

<div class="powered-by">
  Erstellt mit  <a class="theme-link" href="http://hexo.io">Hexo</a>
</div>

<div class="theme-info">
  Theme -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Pisces
  </a>
</div>

        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/vendors/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.0.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.0.1"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.0.1"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.0.1"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.0.1"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.0.1"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.0.1"></script>



  



  



  
  
  
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
  </script>

  <script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for (i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
      }
    });
  </script>
  <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


  

  

</body>
</html>
